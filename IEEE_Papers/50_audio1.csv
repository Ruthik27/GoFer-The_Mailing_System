"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"IEEE Standard for Advanced Audio Coding","",,"IEEE Std 1857.2-2013","8 Nov 2013","2013","","","1","343","This standard defines a set of tools to support specific audio coding functions including general audio coding and lossless audio coding. The tool set defined in this standard, in combination provides regular high quality and efficient coding tool sets for compression, decompression, processing, and representing of audio data to save bandwidth for transmission, to save space for storage, to speed up indexing and multimedia search, and to enhance performance when using mixed media for virtual reality and other applications that demand high bandwidth. The target applications and services include but not limited audio transmission, audio recording, internet streaming, and other video/audio enabled services and applications.;This standard defines a set of tools to support specific audio coding functions including general audio coding and lossless audio coding. The tool set defined in this standard, in combination provides regular high quality and efficient coding tool sets for compression, decompression, processing, and representing of audio data to save bandwidth for transmission, to save space for storage, to speed up indexing and multimedia search, and to enhance performance when using mixed media for virtual reality and other applications that demand high bandwidth. The target applications and services include but not limited audio transmission, audio recording, internet streaming, and other video/audio enabled services and applications.","","978-0-7381-8558-3","10.1109/IEEESTD.2013.6658828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6658828","audio coding;scalable coding;lossless coding;LPC;vector quantization;bit-plane coding;PQ-SPSC;entropy coding","IEEE standards;Audio coding;Loss measurement;Entropy;Encoding","audio coding;audio recording;data compression;IEEE standards;Internet;telecommunication standards;virtual reality","video/audio enabled services;Internet streaming;audio recording;audio transmission;virtual reality;audio data processing;audio data decompression;audio data compression;lossless audio coding;IEEE standard","","","","0","","8 Nov 2013","","","IEEE","IEEE Standards"
"Audio Encryption Algorithm Using Modified Elliptical Curve Cryptography and Arnold Transform for Audio Watermarking","R. Shelke; M. Nemade","Faculty of Engineering, Pacific Academy of Higher Education and Research University, Udaipur, India; Department of Electronics Engineering, K. J. Somaiya Institute of Engineering & Information Technology, Mumbai, India","2018 3rd International Conference for Convergence in Technology (I2CT)","11 Nov 2018","2018","","","1","4","Tremendous development in the digital technology has necessitated the copyright protection and authentication of the images, audios, video and documents using watermarking techniques. Audio watermarking is more challenging due to the fact that dynamic supremacy of hearing capacity over the visual field. Encryption of the watermark signal or image enhances its security under malicious attacks through high entropy. In this paper, performance of the proposed audio encryption algorithm using modified elliptical curve cryptography and Arnold transform is evaluated to determine its suitability for audio watermarking techniques. Elliptical curve cryptography benefits from smaller key size and large key space without sacrificing on level of security. Arnold transform is commonly employed in two dimensional domains for scattering of the pixels in an image. Single dimensional audio signal was converted into two dimensional signals before applying Arnold transform through elliptical curve cryptography. Experimental simulations were performed to evaluate the performance of the encryption algorithm through appropriate parameters and under general signal processing attacks. Audio encryption finds applications in the area of ownership protection, temper detection and authentication of music's, airline traffic monitoring and controlling, broadcast monitoring and voice activated commands for machine control and operations.","","978-1-5386-4273-3","10.1109/I2CT.2018.8529329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8529329","Audio encryption algorithms;Arnold transform;Elliptical curve cryptography;Signal processing attacks","Encryption;Transforms;Elliptic curve cryptography;Watermarking;Signal processing algorithms","audio signal processing;audio watermarking;copy protection;copyright;public key cryptography;transforms","audio encryption algorithm;modified elliptical curve cryptography;Arnold transform;copyright protection;authentication;watermark signal;audio watermarking techniques;single dimensional audio signal;ownership protection;temper detection","","2","","12","","11 Nov 2018","","","IEEE","IEEE Conferences"
"Audio processing with channel filtering using DSP techniques","J. Jiang","Department of Engineering Technology, Purdue University Northwest, Westville, IN, USA","2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC)","26 Feb 2018","2018","","","545","550","In this paper, a novel surround sound system operated by DSP boards is developed. Besides woofer(s) and tweeter(s), the audio system has been improved by adding additional speakers for handling mid-frequencies by designing a particular band-pass filter to process its mid-range. Meanwhile, three audio amplifiers are built to drive woofer (low frequency), mid-frequency speaker, and tweeter (high frequency), respectively. The practical audio amplifiers are also created with moderate gains in order to power speakers, in which the amplification circuits are modified using the data sheet of audio chip LM386 IC. Three different types of filters, which are low-pass, band-pass and high-pass, are also designed using the Remez exchange algorithm by MATLAB. Then, the obtained filter coefficients are loaded into two DSP boards (TMS320C6713) to process the sound signals in the full audio frequency range. Finally, the processed audio data are enhanced by audio amplifiers strong enough to power three sets of speakers: bass, mid-range, and tweeters. In addition to developing and testing the system, the research methodology and design is summarized and future improvements are discussed.","","978-1-5386-4649-6","10.1109/CCWC.2018.8301696","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8301696","Audio amplifiers;optimal design method;digital signal processing (DSP) techniques;Remez exchange function;Parks-McClellan algorithm;TMS320C6713 DSP board;bass;mid-range speaker;tweeter","Band-pass filters;Low pass filters;Finite impulse response filters;Audio systems;Matlab;Cutoff frequency","acoustic signal processing;audio signal processing;audio-frequency amplifiers;band-pass filters;digital signal processing chips;high-pass filters;low-pass filters;low-power electronics","channel filtering;DSP techniques;novel surround sound system;DSP boards;woofer;tweeter;audio system;bass;sound signal processing;Matlab;Remez exchange algorithm;high-pass filter;low-pass filter;amplification circuits;high frequency speaker;low frequency speaker;full audio frequency range;audio amplifiers;band-pass filter;audio signal processing;sound signals;filter coefficients;audio chip LM386 IC;data sheet;power speakers;mid-frequency speaker","","2","","7","","26 Feb 2018","","","IEEE","IEEE Conferences"
"Audio Compression Algorithm using Discrete Cosine Transform (DCT) and Lempel-Ziv-Welch (LZW) Encoding Method","S. Shukla; M. Ahirwar; R. Gupta; S. Jain; D. S. Rajput","ECE Department, IGEC, Sagar; ECE Department, IGEC, Sagar; ECE Department, IGEC, Sagar; ECE Department, IGEC, Sagar; ECE Department, IGEC, Sagar","2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)","11 Oct 2019","2019","","","476","480","This paper proposes a new approach to Audio Compression that incorporates lossless text compression algorithm. The purpose of Audio Compression is to reduce the amount of data required to represent the digital audio by removing redundant data. In the present work Discrete Cosine Transform and lossless text compression method (Lempel-Ziv-Welch method) based Audio Compression algorithm has been proposed which also includes audio normalization, scalar quantization and encoding. The LZW dictionary obtained after compression is used for transmission and storage. The performance of the proposed algorithm is analyzed using various audio specimens of distinct size with distinct audio signal parameters. The compression performance is assessed using Peak Signal to Noise Ratio (PSNR) and Compression Ratio (CR). The audio specimen outcomes indicate the propitious performance of proposed algorithm. The compression ratio can be enhanced by changing various parameters of the system.","","978-1-7281-0211-5","10.1109/COMITCon.2019.8862228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8862228","Audio Compression;Lempel-Ziv-Welch (LZW) Coding;Discrete Cosine Transform (DCT);Scalar Quantization;Audio Masking","Discrete cosine transforms;Encoding;Image coding;Dictionaries;Audio compression;Decoding;Quantization (signal)","audio coding;data compression;discrete cosine transforms;image coding","audio specimen outcomes;compression ratio;distinct audio signal parameters;audio specimens;audio normalization;Lempel-Ziv-Welch method;lossless text compression method;work Discrete Cosine Transform;digital audio;lossless text compression algorithm;Lempel-Ziv-Welch encoding method;Audio Compression algorithm","","","","9","","11 Oct 2019","","","IEEE","IEEE Conferences"
"Nonlinear Audio Systems Identification Through Audio Input Gaussianization","I. Mezghani-Marrakchi; G. Mahé; S. Djaziri-Larbi; M. Jaïdane; M. Turki-Hadj Alouane","Ecole Nationale d'Ingégnieurs de Sousse, Université de Sousse, Tunisia; UFR math-info, Université Paris Descartes, Paris Cedex, France; Ecole Nationale d'Ingénieurs de Tunis, Université Tunis El Manar, Tunisia; Ecole Nationale d'Ingénieurs de Tunis, Université Tunis El Manar, Tunisia; Ecole Nationale d'Ingénieurs de Tunis, Université Tunis El Manar, Tunisia","IEEE/ACM Transactions on Audio, Speech, and Language Processing","19 Nov 2013","2014","22","1","41","53","Nonlinear audio system identification generally relies on Gaussianity, whiteness and stationarity hypothesis on the input signal, although audio signals are non-Gaussian, highly correlated and non-stationary. However, since the physical behavior of nonlinear audio systems is input-dependent, they should be identified using natural audio signals (speech or music) as input, instead of artificial signals (sweeps or noise) as usually done. We propose an identification scheme that conditions audio signals to fit the desired properties for an efficient identification. The identification system consists in (1) a Gaussianization step that makes the signal near-Gaussian under a perceptual constraint; (2) a predictor filterbank that whitens the signal; (3) an orthonormalization step that enhances the statistical properties of the input vector of the last step, under a Gaussianity hypothesis; (4) an adaptive nonlinear model. The proposed scheme enhances the convergence rate of the identification and reduces the steady state identification error, compared to other schemes, for example the classical adaptive nonlinear identification.","2329-9304","","10.1109/TASL.2013.2282214","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6600774","Audio gaussianization;nonlinear system identification;orthonormalization","Speech;Correlation;Laplace equations;Polynomials;Speech processing;Manganese;Nonlinear systems","audio signal processing;channel bank filters;Gaussian processes","nonlinear audio system identification;audio input Gaussianization;artificial signals;audio signals;Gaussianization step;perceptual constraint;predictor filterbank;orthonormalization step;statistical properties;Gaussianity hypothesis;adaptive nonlinear model;steady state identification error","","10","","32","","16 Sep 2013","","","IEEE","IEEE Journals"
"Immersive Audio Coding for Virtual Reality Using a Metadata-assisted Extension of the 3GPP EVS Codec","D. McGrath; S. Bruhn; H. Purnhagen; M. Eckert; J. Torres; S. Brown; D. Darcy","Dolby Australia Pty. Ltd., Sydney, Australia; Dolby Sweden AB, Stockholm, Sweden; Dolby Sweden AB, Stockholm, Sweden; Dolby Australia Pty. Ltd., Sydney, Australia; Dolby Australia Pty. Ltd., Sydney, Australia; Dolby Australia Pty. Ltd., Sydney, Australia; Dolby Laboratories, Inc., San Francisco, USA","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","16 Apr 2019","2019","","","730","734","Virtual Reality (VR) audio scenes may be composed of a very large number of audio elements, including dynamic audio objects, fixed audio channels and scene-based audio elements such as Higher Order Ambisonics (HOA). Potentially, the subjective listening experience may be replicated using a compact spatial format with a set number of dynamic objects and scene-based elements, retaining only the perceptual essence of the audio scene. The compact format would further enable a reduction in the complexity of subsequent compression and rendering. This paper investigates these hypotheses by exploring the use of a compact format that consists of up to four dynamic objects and nine HOA channels, with the Enhanced Voice Services (EVS) codec being applied to a 4-channel down-mix of the compact format.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8683712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8683712","Audio Coding;Virtual Reality;Spatial Audio;Immersive Audio;Ambisonics","Metadata;Decoding;Bit rate;Codecs;Encoding;3GPP;Virtual reality","3G mobile communication;audio coding;codecs;meta data;virtual reality","Enhanced Voice Services codec;3GPP EVS codec;Virtual Reality audio scenes;dynamic audio objects;Higher Order Ambisonics;audio scene;audio coding;audio channels;metadata-assisted","","","1","11","","16 Apr 2019","","","IEEE","IEEE Conferences"
"A 40 nm CMOS analog front end with enhanced audio for HSPA/EDGE multimedia applications","X. Jiang; M. G. Kim; F. Cheung; F. Lin; H. Zheng; J. Chen; A. Chen; D. Cheung; K. Abdelfattah; S. -H. Lee; H. Huang; K. Kasichainula; Y. Cong; J. Wu; C. -H. Lee; G. Chih; Y. Tu; T. L. Brooks; E. Jiang; H. Kong; C. Zhao; M. Keskin","Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA; Broadcom Corporation, Irvine, CA 92617 USA","2012 Proceedings of the ESSCIRC (ESSCIRC)","10 Nov 2012","2012","","","414","417","A 40 nm CMOS analog front end (AFE) supporting HSPA/EDGE multimedia and enhanced audio applications is reported. The AFE consists of hi-fi audio and high-performance peripheral and auxiliary subsystems. Circuit techniques that enable a 200 μA audio RX path and a Class-AB driver with -80 dB THD are discussed. Audio playback and capture paths achieve 105 dB and 85 dB SNR, respectively.","1930-8833","978-1-4673-2213-3","10.1109/ESSCIRC.2012.6341343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341343","","Multimedia communication;Universal Serial Bus;CMOS integrated circuits;System-on-a-chip;Headphones;Baseband;Monitoring","3G mobile communication;audio signal processing;CMOS analogue integrated circuits;Hi-Fi equipment;multimedia communication","CMOS analog front end;enhanced audio;HSPA multimedia application;EDGE multimedia application;AFE;hi-fi audio;high-performance peripheral;auxiliary subsystems;circuit techniques;audio RX path;class-AB driver;audio playback;capture paths;size 40 nm;current 200 muA;noise figure 105 dB;noise figure 85 dB","","2","1","6","","10 Nov 2012","","","IEEE","IEEE Conferences"
"Enhanced Principal Component Using Polar Coordinate PCA for Stereo Audio Coding","S. Dong; R. Hu; W. Tu; X. Zheng; J. Jiang; S. Wang","Nat. Eng. Res. Center for Multimedia Software, Wuhan Univ., Wuhan, China; Nat. Eng. Res. Center for Multimedia Software, Wuhan Univ., Wuhan, China; Nat. Eng. Res. Center for Multimedia Software, Wuhan Univ., Wuhan, China; Nat. Eng. Res. Center for Multimedia Software, Wuhan Univ., Wuhan, China; Nat. Eng. Res. Center for Multimedia Software, Wuhan Univ., Wuhan, China; Nat. Eng. Res. Center for Multimedia Software, Wuhan Univ., Wuhan, China","2012 IEEE International Conference on Multimedia and Expo","13 Sep 2012","2012","","","628","633","High efficiency audio compression is the basic technology in audio involved multimedia application. Down mixing and parametric coding are efficient coding scheme with widely applications in some up to date audio codecs such as PS in EAAC+ and MPEG-Surround, and PCA stereo coding followed this idea to map two channels to one channel with maximum energy and parameterize the secondary channel. This paper investigates the conventional PCA method performance under general stereo model with multiple sound sources and different directions, and then proposes a Polar Coordinate based PCA (PC-PCA) stereo coding method. It has been proved that when multiple sound sources exist with different directions, proposed method is better than the conventional PCA method in certain conditions. A stereo codec based on PC-PCA has also been proposed to validate the performance improvement of proposed method.","1945-788X","978-1-4673-1659-0","10.1109/ICME.2012.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298472","PCA;Stereo audio;Audio compression","Encoding;Principal component analysis;Signal to noise ratio;Codecs;Decoding;Bit rate","audio coding;data compression;principal component analysis","enhanced principal component analysis;polar coordinate PCA method;stereo audio coding;high efficiency audio compression;parametric coding;down mixing coding;audio codecs;MPEG-surround;EAAC+;PCA stereo coding;general stereo model;multiple sound sources;PC-PCA stereo coding method","","3","","7","","13 Sep 2012","","","IEEE","IEEE Conferences"
"Evaluation of Spatial/3D Audio: Basic Audio Quality Versus Quality of Experience","M. Schoeffler; A. Silzle; J. Herre","International Audio Laboratories Erlangen (a joint institution of Friedrich-Alexander University of Erlangen-Nürnberg and Fraunhofer IIS), Erlangen, Germany; International Audio Laboratories Erlangen (a joint institution of Friedrich-Alexander University of Erlangen-Nürnberg and Fraunhofer IIS), Erlangen, Germany; International Audio Laboratories Erlangen (a joint institution of Friedrich-Alexander University of Erlangen-Nürnberg and Fraunhofer IIS), Erlangen, Germany","IEEE Journal of Selected Topics in Signal Processing","13 Feb 2017","2017","11","1","75","88","During the past decades, spatial reproduction of audio signals has evolved from simple two-channel stereo to surround sound (e.g., 5.1 or 7.1) and, more recently, to three-dimensional (3D) sound including height speakers, such as 9.1 or 22.2. With increasing number of speakers, increasing spatial fidelity and listener envelopment are expected. This paper reviews popular methods for subjective assessment of audio. Moreover, it provides an experimental evaluation of the subjective quality provided by these formats, contrasting the well-known basic audio quality (BAQ) type of evaluation with the more recent evaluation of overall listening experience (OLE). Commonalities and differences in findings between both assessment approaches are discussed. The results of the evaluation indicate that 3D audio enhances BAQ as well as OLE over both stereo and surround sound. Furthermore, the BAQ- and OLE-based assessments turned out to deliver consistent and reliable results.","1941-0484","","10.1109/JSTSP.2016.2639325","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7782315","Subjective assessment;3D audio;basic audio quality;overall listening experience","Three-dimensional displays;Loudspeakers;Quality assessment;Reliability;Taxonomy;Speech;Quality of service","audio signal processing","3D audio;basic audio quality;quality of experience;audio signals;two-channel stereo;three-dimensional sound;BAQ;OLE;overall listening experience","","8","","71","","13 Dec 2016","","","IEEE","IEEE Journals"
"Enhancing Audio and Video Steganography Technique Using Hybrid Algorithm","S. Teotia; P. Srivastava","Dept. of Computer Science & Engineering, Amity University, Noida, India; Dept. of Computer Science & Engineering, Amity University, Noida, India","2018 International Conference on Communication and Signal Processing (ICCSP)","8 Nov 2018","2018","","","1059","1063","As the technology is developing, people have tend to find out methods which are not only capable in hiding a information but also capable of even hiding the existence of a message or information. Steganography was introduced as a result of such different research works, but despite of so many researches still we have problems of minimizing the error and obtaining better PSNR values. Audio Steganography is a technique or technology which is used to transfer secret information or message by changing an audio signal into an imperceptible way. It's the ability of thrashing confidential message or audio data in a host or another message, Video Steganography refers to hiding a confidential data or message, it can be a text message or an image inside a larger or another one in a style that by only looking or seeing at it an unknown person cannot notice the existence of hidden message. Our paper presents a hybrid algorithm which reduces the error in audio and video steganography and gives better values of PSNR and MSE.","","978-1-5386-3521-6","10.1109/ICCSP.2018.8524182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8524182","Steganography;Stego message;Audio;Video","Image coding;Encoding;Encryption;Reliability;Data models","audio signal processing;steganography;video signal processing","audio data;video steganography;hybrid algorithm;audio signal;audio steganography;PSNR values;confidential message thrashing;confidential data hiding;MSE","","","","15","","8 Nov 2018","","","IEEE","IEEE Conferences"
"Enhanced Digital Audio Watermarking Using Genetic Algorithm","R. R. Ginanjar; D. S. Kim; C. B. Moon","Department of IT Convergence Engineering, Kumoh National Institute of Technology, Gyeongbuk-do, South Korea; Department of IT Convergence Engineering, Kumoh National Institute of Technology, Gyeongbuk-do, South Korea; Department of IT Convergence Engineering, Kumoh National Institute of Technology, Gyeongbuk-do, South Korea","2018 IEEE International Conference on Consumer Electronics - Asia (ICCE-Asia)","29 Nov 2018","2018","","","206","212","In this paper, a novel of high imperceptibility and capacity blind audio watermarking system based on Reduced Arc M-Ary Phase Shift Keying (MPSK) which is optimized by the Genetic Algorithm is proposed. The audio files samples will be divided into several frames and then transformed into frequency domain using FFT. The watermark data will be embedded into the phase element of the selected samples that have been framed and transformed. We use two modulation schemes for the embedding process, 256-PSK and BPSK to compare the effects of the number of M-ary used in the system. To improve the system, we used the Genetic Algorithm to find the best embedding parameter to yield the most optimal output based on the Perceptual quality of watermarked audio and the robustness of the watermark. Experimental results show that the proposed audio watermarking system produces high quality of watermarked audio while loading huge amount of watermark data.","","978-1-5386-5807-9","10.1109/ICCE-ASIA.2018.8552160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8552160","Digital Audio Watermarking;FFT;Reduced-Arc;MPSK;Genetic Algorithm;multimedia Application","Watermarking;Genetic algorithms;Robustness;Binary phase shift keying;Conferences","audio watermarking;fast Fourier transforms;genetic algorithms;phase shift keying","Genetic Algorithm;capacity blind audio watermarking system;Reduced Arc M-Ary Phase Shift Keying;audio files samples;frequency domain;watermark data;phase element;embedding process;embedding parameter;optimal output;watermarked audio;enhanced digital audio watermarking;BPSK;FFT","","1","","9","","29 Nov 2018","","","IEEE","IEEE Conferences"
"Cascaded Long Term Prediction for Enhanced Compression of Polyphonic Audio Signals","T. Nanjundaswamy; K. Rose","Department of Electrical and Computer Engineering, University of California, Santa Barbara, CA, USA; Department of Electrical and Computer Engineering, University of California, Santa Barbara, CA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 May 2017","2014","22","3","697","710","Audio compression systems exploit periodicity in signals to remove inter-frame redundancies via the long term prediction (LTP) tool. This simple tool capitalizes on the periodic component of the waveform by selecting a past segment as the basis for prediction of the current frame. However, most audio signals are polyphonic in nature, containing a mixture of several periodic components. While such polyphonic signals may themselves be periodic with overall period equaling the least common multiple of the individual component periods, the signal rarely remains sufficiently stationary over the extended period, rendering the LTP tool suboptimal. Instead of seeking a past segment that represents a “compromise” for incompatible component periods, we propose a more complex filter that predicts every periodic component of the signal from its immediate history, and this is achieved by cascading LTP filters, each corresponding to individual periodic component. We also propose a recursive “divide and conquer” technique to estimate parameters of all the LTP filters. We then demonstrate the effectiveness of cascaded LTP in two distinct settings of the ultra low delay Bluetooth Subband Codec and the MPEG Advanced Audio Coding (AAC) standard. In MPEG AAC, we specifically adapt the cascaded LTP parameter estimation to take into account the perceptual distortion criteria, and also propose a low decoder complexity variant. Objective and subjective results for all the settings validate the effectiveness of the proposal on a variety of polyphonic signals.","2329-9304","","10.1109/TASLP.2014.2303292","NSF(grant numbers:CCF-0917230,CCF-1320599); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727408","Audio compression;long term prediction;perceptual optimization;polyphonic signals","Transform coding;Nickel;Speech;Standards;Speech processing;Decoding;Speech coding","audio coding;computational complexity;distortion;divide and conquer methods;filtering theory;linear predictive coding;recursive estimation","long term prediction;polyphonic audio signal compression enhancement;signal periodicity;periodic component mixture;periodic polyphonic audio signal;complex filter;periodic component prediction;recursive divide and conquer technique;cascaded LTP filter;Bluetooth subband codec;MPEG AAC;advanced audio coding;cascaded LTP parameter estimation;perceptual distortion criteria;decoder complexity variant","","1","1","36","","28 Jan 2014","","","IEEE","IEEE Journals"
"Enhanced Signal-Dependent Rank-Ordered Mean-Based Audio Noise Removal","G. T. Stouraitis","Psychiko College, Hellenic American Educational Foundation,Athens,Greece","2019 Panhellenic Conference on Electronics & Telecommunications (PACET)","13 Jan 2020","2019","","","1","4","This study focuses on restoring audio signals corrupted by impulse noise added to them in a random manner, either when generated or when recorded, transferred or processed. More specifically, we investigate the application of a variant of the well-known Signal-Dependent Rank-Ordered Mean (SDROM) method using simulated addition of impulse noise to clean signals. Our study describes an automatic exploration of the threshold space used in this method and selection of optimal thresholds based on global minima for the mean energy of the error in noise identification. Experimental results showed that the method performs well for noise levels up to 10% of the signal.","","978-1-7281-4360-6","10.1109/PACET48583.2019.8956267","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8956267","SDROM;audio noise removal;threshold selection","","audio signal processing;impulse noise;interference suppression","noise identification;audio noise removal;audio signals;impulse noise;Signal-Dependent Rank-Ordered Mean method;threshold space","","","","11","","13 Jan 2020","","","IEEE","IEEE Conferences"
"An Efficient Visual-Based Method for Classifying Instrumental Audio using Deep Learning","J. Hall; W. O’Quinn; R. J. Haddad","Georgia Southern University,Department of Electrical and Computer Engineering,Statesboro,United States; Georgia Southern University,Department of Electrical and Computer Engineering,Statesboro,United States; Georgia Southern University,Department of Electrical and Computer Engineering,Statesboro,United States","2019 SoutheastCon","5 Mar 2020","2019","","","1","4","In this paper, an efficient method for classifying and identifying instrumental audio is proposed via utilizing a deep learning image classification algorithm. The method of classification will involve analyzing the visual equivalent of an audio sample with a neural network to identify the generating musical instrument. Audio samples are converted into a logarithmic spectrogram format, which allows visual classifiers to attempt the identification of the audio source. The primary focus is on developing an efficient method for analyzing audio spectrograms using various forms of neural networks and analysis techniques. The use of deep learning convolutional neural networks in analyzing visually formatted audio data provides an enhanced classification method over traditional schemes. A classification accuracy of 73.7% was achieved with a limited data set and minimal manipulation of network architecture.","1558-058X","978-1-7281-0137-8","10.1109/SoutheastCon42311.2019.9020571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020571","Audio Classification;Deep Learning;Neural Networks;Transfer Learning;Audio Visual Transform;Music Instrument;Spectrograms","Instruments;Spectrogram;Visualization;Neural networks;Machine learning;Training;Time-frequency analysis","audio signal processing;convolutional neural nets;image classification;learning (artificial intelligence);musical instruments","instrumental audio;deep learning image classification algorithm;logarithmic spectrogram format;visual classifiers;audio source;audio spectrograms;deep learning convolutional neural networks;visually formatted audio data;enhanced classification method;classification accuracy;musical instrument generation","","","","8","","5 Mar 2020","","","IEEE","IEEE Conferences"
"Automatic Audio Mastering System","E. Najduchowski; M. Lewandowski; P. Bobinski","Faculty of Electronics and Information Technology, Warsaw University of Technology, Institute of Radioelectronics and Multimedia Technology, Warsaw, Poland; Faculty of Electronics and Information Technology, Warsaw University of Technology, Institute of Radioelectronics and Multimedia Technology, Warsaw, Poland; Faculty of Electronics and Information Technology, Warsaw University of Technology, Institute of Radioelectronics and Multimedia Technology, Warsaw, Poland","2018 Joint Conference - Acoustics","25 Oct 2018","2018","","","1","6","The modern music production process requires multiple steps of digital signal processing such as audio frequency response equalization, or dynamic range compression. In order to process the original audio material the mastering engineer controls the parameters of these processing algorithms with respect to genre and style of audio content. The main purpose of this processing is to aesthetically enhance perceived acoustic characteristics of the signals. The selection and the adj ustment of these parameters relies on the continuous interaction between the audio mastering engineer and the apparatus that handles the audio signals. Modelling such dynamic operations becomes very important in automated applications. In this work we present a system which automatically enhances unprocessed audio signal with respect to the specific parameters of the reference audio material. These parameters are obtained through analysis of magnitude spectrum (spectral roll-off point, and energy calculated for specified frequency bands), amplitude histogram, audio content tempo, signal envelope's timing features and LUFS parameter. Results from conducted online listening tests are presented and discussed, along with objective measurements of unprocessed and reference audio signal.","","978-1-5386-7114-6","10.1109/ACOUSTICS.2018.8502427","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502427","audio mastering;sound engineering;multimedia technology;electroacoustics;signal processing;recording studio","Dynamic range;Histograms;Signal processing algorithms;Gain;Frequency response;Band-pass filters","audio coding;audio signal processing;data compression;frequency response;music","automatic audio mastering system;modern music production process;digital signal processing;audio frequency response equalization;dynamic range compression;processing algorithms;acoustic characteristics;continuous interaction;audio mastering engineer;dynamic operations;reference audio material;specified frequency bands;audio content tempo;unprocessed reference audio signal enhancement;magnitude spectrum analysis;amplitude histogram;signal envelope timing features;LUFS parameter;online listening tests;objective measurements","","1","","8","","25 Oct 2018","","","IEEE","IEEE Conferences"
"Enhanced long-term predictor for Unified Speech and Audio Coding","J. Song; H. -O. Oh; H. -G. Kang","Digital Signal Processing Lab, Yonsei University, Seoul, Korea; Digital Signal Processing Lab, Yonsei University, Seoul, Korea; Digital Signal Processing Lab, Yonsei University, Seoul, Korea","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","11 Jul 2011","2011","","","505","508","Unified Speech and Audio Coding (USAC) is an emerging MPEG audio standard striving for efficiently representing both speech and music signals even in very low bitrate ranges. The reference codec takes an approach of unifying two state-of-the-art speech and audio coding structures in a single platform. This paper proposes an enhanced long term predictor (eLTP) that effectively utilizes periodic redundancies of inter- and intra- time frames. Experimental results with various types of input signals confirm the superiority of the proposed algorithm compared to the reference codec.","2379-190X","978-1-4577-0539-7","10.1109/ICASSP.2011.5946451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5946451","Unified Speech and Audio Coding (USAC);Enhanced Long Term predictor (eLTP)","Speech;Codecs;Transform coding;Audio coding;Speech coding;Decoding","audio coding;code standards;music;prediction theory;speech codecs;speech coding","unified speech and audio coding;USAC;MPEG;audio standard;music signals;codec;enhanced long term predictor","","1","2","16","","11 Jul 2011","","","IEEE","IEEE Conferences"
"Self-Supervised Audio Spatialization with Correspondence Classifier","Y. Lu; H. Lee; H. Tseng; M. Yang",University of California at Merced; University of California at Merced; University of California at Merced; University of California at Merced,"2019 IEEE International Conference on Image Processing (ICIP)","26 Aug 2019","2019","","","3347","3351","Spatial audio is an essential medium to audiences for 3D visual and auditory experience. However, the recording devices and techniques are expensive or inaccessible to the general public. In this work, we propose a self-supervised audio spatialization network that can generate spatial audio given the corresponding video and monaural audio. To enhance spatialization performance, we use an auxiliary classifier to classify ground-truth videos and those with audio where the left and right channels are swapped. We collect a large-scale video dataset with spatial audio to validate the proposed method. Experimental results demonstrate the effectiveness of the proposed model on the audio spatialization task.","2381-8549","978-1-5386-6249-6","10.1109/ICIP.2019.8803494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803494","Audio-visual;Spatial audio;Self-supervised","Feature extraction;MONOS devices;Synthesizers;Training;Testing;Task analysis;Visualization","audio signal processing;learning (artificial intelligence);pattern classification;video signal processing","self-supervised audio spatialization network;recording devices;auditory experience;essential medium;correspondence classifier;audio spatialization task;spatial audio;ground-truth videos;auxiliary classifier;spatialization performance;monaural audio","","3","","23","","26 Aug 2019","","","IEEE","IEEE Conferences"
"Enhanced Audio Source Separation and Musical Component Analysis","T. Bhagwat; S. Deolalkar; J. Lokhande; L. Ragha","Ramrao Adik Institute of Technology,Department of Computer Engineering,Mumbai,India; Ramrao Adik Institute of Technology,Department of Computer Engineering,Mumbai,India; Ramrao Adik Institute of Technology,Department of Computer Engineering,Mumbai,India; Ramrao Adik Institute of Technology,Department of Computer Engineering,Mumbai,India","2020 IEEE International Symposium on Sustainable Energy, Signal Processing and Cyber Security (iSSSC)","25 Feb 2021","2020","","","1","6","Audio source separation is a cornerstone problem for researchers engaged in Digital Signal Processing and Artificial Intelligence. Music unmixing is the task of decomposing music into its constitutive components, like yielding separated stems for the vocals, bass, drums, accompaniment, jazz, and others from a mastered song track. Due to recent progress in the field of Deep Learning, researchers have been able to devise Neural Networks that can perform this task with considerable precision. However, these models lack performance when dealing with generic musical audio despite having decent utility over specific music genres. The proposed system aims to develop a universal platform-independent software for accurate domain-specific implementation of music source separation for acute subsets of stereo audio using the Bidirectional Long Short Term Memory (BLSTM) architecture of Recurrent Neural Networks. The Deep Neural Network helps demix audio mixtures into the jazz solo and its accompaniment. In cohesion, these two models extract five independent audio stems from the original audio with reasonable accuracy. Further, the extracted accompaniment stem is processed using ConvNet Model to estimate the instrumental components. In synchronization, these three models can break down audio to its fundamental elements.","","978-1-7281-8880-5","10.1109/iSSSC50941.2020.9358850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9358850","Audio Source Separation;Recurrent Neural Networks;LSTM;Audio Classification;Convolutional Neural Networks","Source separation;Recurrent neural networks;Memory architecture;Music;Software;Synchronization;Task analysis","audio signal processing;deep learning (artificial intelligence);music;recurrent neural nets;source separation","enhanced audio source separation;musical component analysis;digital signal processing;artificial intelligence;music unmixing;mastered song track;deep learning;generic musical audio;music genres;universal platform-independent software;music source separation;stereo audio;bidirectional long short term memory architecture;recurrent neural networks;deep neural network;demix audio mixtures;jazz solo;independent audio stems;original audio;ConvNet model;domain-specific implementation;accompaniment stem extraction;BLSTM architecture","","","","30","","25 Feb 2021","","","IEEE","IEEE Conferences"
"A QoE and Visual Attention Evaluation on the Influence of Audio in 360° Videos","A. Hirway; Y. Qiao; N. Murray","Athlone Institute of Technology, Ireland; Athlone Institute of Technology, Ireland; Athlone Institute of Technology, Ireland","2020 IEEE 21st International Symposium on "A World of Wireless, Mobile and Multimedia Networks" (WoWMoM)","9 Oct 2020","2020","","","191","193","360° video, also known as immersive video, is the recording of video content which simultaneously captures scene information in every direction, using an omnidirectional camera. Due to their immersive nature, the popularity of 360° videos has grown significantly. Understanding user Visual Attention when watching 360° videos is very important. This knowledge can help develop effective techniques for processing, encoding, distributing, and rendering 360° content. Whilst major efforts have concentrated on the visual element of immersive experiences, recently there has been growing interest in different forms of audio and in particular high-quality spatial audio. Spatial audio allows listeners to experience sound in all directions. Ambisonics or 3D audio is one such technique which offers a complete 360° soundscape. Although several models of visual and audio-visual attention have been proposed, very few have investigated the role of spatial audio in guiding attention in 360° videos. This demo shows our dataset and our methodological approach to understanding the user's audio-visual attention and QoE when experiencing 360° videos enhanced with spatial sound (first and third order ambisonic). Our research focus is to understand how audio affects Visual Attention in 360° videos and to evaluate its impact on the user's Quality of Experience (QoE).","","978-1-7281-7374-0","10.1109/WoWMoM49955.2020.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9217677","360° Video;Spatial Audio;Ambisonics;QoE;Audio-Visual Attention","Videos;Visualization;Quality of experience;Virtual reality;Streaming media;Media;Resists","audio signal processing;audio-visual systems;quality of experience;video recording;video signal processing","visual element;immersive experiences;high-quality spatial audio;audio-visual attention;QoE;spatial sound;visual attention evaluation;360° videos;immersive video;360° soundscape;video content recording;omnidirectional camera;ambisonics;3D audio;quality of experience","","","","19","","9 Oct 2020","","","IEEE","IEEE Conferences"
"Object-Based Audio for Sports TV Production","S. A. Silva",NA,"SMPTE Motion Imaging Journal","4 Jan 2017","2016","125","9","1","5","Object-based audio (OBA) is the next exciting breakthrough in television production. It will provide personalization and an enhanced listening experience as revolutionary as sound was to motion pictures. The age of personalization has arrived, and TV consumers can view any program, at any time, and on virtually any media device. The next generation of audio encoders will have the ability to create OBA in television production and post-production. The beneficial features of OBA include audio personalization for language selection, dialogue enhancements, and options for the hearing impaired. OBA provides the viewer with the ability to customize their viewing for any type of program in any viewing setting. The future audio codec technologies will enable audio production mixers, producers, and broadcasters to produce customized audio for the viewer. This process begins at the original mix location and continues through the broadcast chain to delivery on any consumer device. These encoders will have the ability to emit surround sound and immersive sound with more than 100 channels with objects either separately or in combination with each other. Scene-based audio will also be a feature of the next generation codecs, enabling the mixer to represent the sound image instead of channels. This paper describes the evolution from current channel-based TV production to the next generation of multi-featured audio encoders with OBA and the potential benefits they will offer to all types of TV viewers.","2160-2492","","10.5594/JMI.2016.2615430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7805134","Audio Objects;Personalization;Media Devices;Dialogue enhancement;Audio mixers;Directors;Producers;Broadcasters;HDTV;4K-UHDTV;Uve Sporting Events;TV shows;Hearing Impaired;Metadata;Next Generation Audio Codecs","","audio coding;codecs;sport;television production","audio codec technologies;current channel-based TV production;sound image;next generation codecs;scene-based audio;surround sound;audio production mixers;dialogue enhancements;language selection;audio personalization;audio encoders;media device;TV consumers;motion pictures;television production;OBA;sports TV production;object-based audio","","3","","8","","4 Jan 2017","","","SMPTE","SMPTE Journals"
"A 5.2mW, 0.0016% THD up to 20kHz, ground-referenced audio decoder with PSRR-enhanced class-AB 16Ω headphone amplifiers","S. Wen; C. Yang","MeditTek Inc. Hsin-Chu, Taiwan; MeditTek Inc. Hsin-Chu, Taiwan","2012 Symposium on VLSI Circuits (VLSIC)","19 Jul 2012","2012","","","20","21","A low-power ground-referenced audio decoder with PSRR-enhanced class-AB headphone amplifiers presents <;0.0016% THD in the whole audio band against the supply ripple by a negative charge-pump. Realized in the 40nm CMOS, the fully-integrated stereo decoder achieves 91dB SNDR and 100dB dynamic range while driving a 16Ω headphone load and consumes 5.2mW from a 1.8V power supply. The core area is 0.093mm<sup>2</sup>/channel only.","2158-5636","978-1-4673-0849-6","10.1109/VLSIC.2012.6243769","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6243769","Ground-referenced;PSRR-enhanced;Class-AB","Decoding;Noise;Headphones;CMOS integrated circuits;Phasor measurement units;Manganese;Charge pumps","audio coding;audio-frequency amplifiers;charge pump circuits;CMOS integrated circuits;decoding;harmonic distortion;headphones","ground-referenced audio decoder;PSRR-enhanced class-AB 16Ω headphone amplifiers;audio band;negative charge-pump;CMOS;fully-integrated stereo decoder;headphone load;power 5.2 mW;size 40 nm;voltage 1.8 V","","1","","6","","19 Jul 2012","","","IEEE","IEEE Conferences"
"Learning Dynamic Stream Weights For Coupled-HMM-Based Audio-Visual Speech Recognition","A. H. Abdelaziz; S. Zeiler; D. Kolossa","Faculty of Electrical Engineering and Information Technology, Cognitive Signal Processing Group, Institute of Communication Acoustics at Ruhr-Universität Bochum, Bochum, Germany; Faculty of Electrical Engineering and Information Technology, Cognitive Signal Processing Group, Institute of Communication Acoustics at Ruhr-Universität Bochum, Bochum, Germany; Faculty of Electrical Engineering and Information Technology, Cognitive Signal Processing Group, Institute of Communication Acoustics at Ruhr-Universität Bochum, Bochum, Germany","IEEE/ACM Transactions on Audio, Speech, and Language Processing","27 Mar 2015","2015","23","5","863","876","With the increasing use of multimedia data in communication technologies, the idea of employing visual information in automatic speech recognition (ASR) has recently gathered momentum. In conjunction with the acoustical information, the visual data enhances the recognition performance and improves the robustness of ASR systems in noisy and reverberant environments. In audio-visual systems, dynamic weighting of audio and video streams according to their instantaneous confidence is essential for reliably and systematically achieving high performance. In this paper, we present a complete framework that allows blind estimation of dynamic stream weights for audio-visual speech recognition based on coupled hidden Markov models (CHMMs). As a stream weight estimator, we consider using multilayer perceptrons and logistic functions to map multidimensional reliability measure features to audiovisual stream weights. Training the parameters of the stream weight estimator requires numerous input-output tuples of reliability measure features and their corresponding stream weights. We estimate these stream weights based on oracle knowledge using an expectation maximization algorithm. We define 31-dimensional feature vectors that combine model-based and signal-based reliability measures as inputs to the stream weight estimator. During decoding, the trained stream weight estimator is used to blindly estimate stream weights. The entire framework is evaluated using the Grid audio-visual corpus and compared to state-of-the-art stream weight estimation strategies. The proposed framework significantly enhances the performance of the audio-visual ASR system in all examined test conditions.","2329-9304","","10.1109/TASLP.2015.2409785","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056468","Audio-visual speech recognition;coupled hidden Markov model;logistic regression;multilayer perceptron;reliability measure;stream weight","Hidden Markov models;Reliability;Speech;Vectors;Heuristic algorithms;Signal processing algorithms;Weight measurement","audio streaming;audio-visual systems;decoding;expectation-maximisation algorithm;feature extraction;hidden Markov models;learning (artificial intelligence);multilayer perceptrons;speech coding;speech recognition;video streaming","coupled-HMM-based audio-visual automatic speech recognition;communication technology;multimedia data;ASR;acoustical information;audio streaming;video streaming;learning dynamic stream weight blind estimation;coupled hidden Markov model;CHMM;multilayer perceptron;logistic function;multidimensional reliability;oracle knowledge;expectation maximization algorithm;31-dimensional feature vector;signal-based reliability;decoding;grid audio-visual corpus","","23","","53","","9 Mar 2015","","","IEEE","IEEE Journals"
"Measuring Audio Processing Latency for Lip-Sync purposes in DSP-based Home Theatre Systems","N. Pekez; R. Čelić; R. Pečkai-Kovač; J. Kovačević","University of Novi Sad, RT-RK R&D Institute for Computer Based Systems,Computer Engineering Department Home Audio DepartmentRT-RK,Novi Sad,Serbia,21000; RT-RK R&D Institute for Computer Based Systems,Home Audio Department,Novi Sad,Serbia,21000; RT-RK R&D Institute for Computer Based Systems,Home Audio Department,Novi Sad,Serbia,21000; University of Novi Sad, RT-RK R&D Institute for Computer Based Systems,Computer Engineering Department Home Audio DepartmentRT-RK,Novi Sad,Serbia,21000","2019 27th Telecommunications Forum (TELFOR)","30 Jan 2020","2019","","","1","4","Higher communication requirements introduced with advances in digital audio have also led to many connectivity and compatibility challenges in consumer electronics (CE). Systems composed of several audio-video, pre-and post-processing components such as Home Theatres (HT) often open up some major issues due to incompatibile processing times between included components. The main such issue is lip-sync error - one of the most disturbing flaws of HT and similar systems from consumers perspective. With announcement of HDMI feature known as Enhanced Audio Return Channel (eARC) utilization of full audio bandwidth and thus processing of HQ object-based audio technologies such as Dolby Atmos and DTS:X is finally possible. However, audio bandwidth magnified by several milions of bits has also increased audio processing latencies and has made even wider gap between audio-video processing times. In order to optmize high memory demands and compensate for delays in the HT system, audio engineers need to precisely measure digital audio processing latencies. This paper proposes a method for the most accurate measurement of digital processing times in DSP-based audio systems.","","978-1-7281-4790-1","10.1109/TELFOR48224.2019.8971274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8971274","dsp;dsp latency;processing delay;home theatre;lip-sync error","","audio signal processing;audio systems;consumer electronics;video signal processing","included components;main such issue;lip-sync error;consumers perspective;Enhanced Audio Return Channel utilization;audio bandwidth;HQ object-based audio technologies;audio-video processing times;HT system;audio engineers;digital audio processing latencies;digital processing times;DSP-based audio systems;lip-sync purposes;higher communication requirements;compatibility challenges;consumer electronics;post-processing components;incompatibile processing times;DSP-based Home theatre systems;DSP-baseD Home Theatre Systems;Enhanced Audio Return Channel utilization","","","","7","","30 Jan 2020","","","IEEE","IEEE Conferences"
"Frequency domain singular value decomposition for efficient spatial audio coding","S. Zamani; T. Nanjundaswamy; K. Rose","Department of Electrical and Computer Engineering, University of California Santa Barbara, CA 93106; Department of Electrical and Computer Engineering, University of California Santa Barbara, CA 93106; Department of Electrical and Computer Engineering, University of California Santa Barbara, CA 93106","2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)","11 Dec 2017","2017","","","126","130","Advances in virtual reality have generated substantial interest in accurately reproducing and storing spatial audio in the higher order ambisonics (HOA) representation, given its rendering flexibility. Recent standardization for HOA compression adopted a framework wherein HOA data are decomposed into principal components that are then encoded by standard audio coding, i.e., frequency domain quantization and entropy coding to exploit psychoacoustic redundancy. A noted shortcoming of this approach is the occasional mismatch in principal components across blocks, and the resulting suboptimal transitions in the data fed to the audio coder. Instead, we propose a framework where singular value decomposition (SVD) is performed after transformation to the frequency domain via the modified discrete cosine transform (MDCT). This framework not only ensures smooth transition across blocks, but also enables frequency dependent SVD for better energy compaction. Moreover, we introduce a novel noise substitution technique to compensate for suppressed ambient energy in discarded higher order ambisonics channels, which significantly enhances the perceptual quality of the reconstructed HOA signal. Objective and subjective evaluation results provide evidence for the effectiveness of the proposed framework in terms of both higher compression gains and better perceptual quality, compared to existing methods.","1947-1629","978-1-5386-1632-1","10.1109/WASPAA.2017.8170008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170008","Higher Order Ambisonics;spatial audio coding;audio compression;3D audio","Transform coding;Codecs;Frequency-domain analysis;Transforms;Quantization (signal);Entropy coding;Decoding","audio coding;data compression;discrete cosine transforms;frequency-domain analysis;singular value decomposition;virtual reality","frequency domain singular value decomposition;efficient spatial audio coding;virtual reality;higher order ambisonics representation;rendering flexibility;HOA compression;HOA data;principal components;standard audio coding;frequency domain quantization;entropy coding;psychoacoustic redundancy;occasional mismatch;audio coder;smooth transition;frequency dependent SVD;suppressed ambient energy;discarded higher order ambisonics channels;reconstructed HOA signal;objective evaluation results;subjective evaluation results;higher compression gains;standardization;modified discrete cosine transform;suboptimal transitions;MDCT;energy compaction;noise substitution technique;perceptual quality","","1","","13","","11 Dec 2017","","","IEEE","IEEE Conferences"
"A 130dB PSRR, 108dB DR and 95dB SNDR, ground-referenced audio decoder with PSRR-enhanced load-adaptive Class-G 16Ohm headphone amplifiers","S. Wen; C. Chen; C. Yang; C. Chen; J. Jiang; K. Hsiao; C. Chien","MediaTek Inc. Hsin-Chu, Taiwan; MediaTek Inc. Hsin-Chu, Taiwan; MediaTek Inc. Hsin-Chu, Taiwan; MediaTek Inc. Hsin-Chu, Taiwan; MediaTek Inc. Hsin-Chu, Taiwan; MediaTek Inc. Hsin-Chu, Taiwan; MediaTek Inc. Hsin-Chu, Taiwan","2015 IEEE Asian Solid-State Circuits Conference (A-SSCC)","21 Jan 2016","2015","","","1","4","A Class-G headphone amplifier for Hi-Fi audio playback with high immunity to supply disturbance and Class-G switching noise is presented that achieves 130dB PSRR at 217Hz and higher than 100dB PSRR up to 20kHz. In addition, a Class-G amplifier with load-adaptive voltage supply rails selection and Class-G switching scheme improves the power efficiency for different impedances. Realized in 0.18μm CMOS, the decoder and amplifier together achieve 108dB DR and 95dB SNDR with a 16D headphone load.","","978-1-4673-7191-9","10.1109/ASSCC.2015.7387442","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7387442","Class-G;Audio amplifier;PSRR-enhancement;Power efficiency;Load-adaptive;Charge-pump","Impedance;Headphones;Charge pumps;Switches;Decoding;Rails;Detectors","audio coding;audio-frequency amplifiers;CMOS analogue integrated circuits;decoding;headphones;Hi-Fi equipment","PSRR;SNDR;ground-referenced audio decoder;PSRR-enhanced load-adaptive class-G headphone amplifiers;Hi-Fi audio playback;class-G switching noise;load-adaptive voltage supply rails selection;power efficiency;CMOS process;resistance 16 ohm;frequency 217 Hz;size 0.18 mum","","1","1","3","","21 Jan 2016","","","IEEE","IEEE Conferences"
"Audio Watermarking Based on Amplitude Variation to Enhance Imperceptibility and Robustness of an Audio Signal","M. K. Pandey; G. Parmar; R. Gupta","Deptt. of Electron. Eng., Rajasthan Tech. Univ., Kota, India; Deptt. of Electron. Eng., Rajasthan Tech. Univ., Kota, India; Deptt. of Electron. Eng., Rajasthan Tech. Univ., Kota, India","2015 Fifth International Conference on Communication Systems and Network Technologies","1 Oct 2015","2015","","","657","661","In recent years, proliferation of digital multimedia including images, audio, video and documents become popular due to on line distribution of digital multimedia. In such an open environment, it is convenient to get the access to various information sources. This increases the possibility of large-scale unauthorized copying of digital data. These concerns over protecting copyright have triggered significant research to find ways to hide copyright messages and serial number into digital media. In this paper, to enhance imperceptibility and robustness of an audio signal, we propose a robust audio watermarking technique based on pseudo-random gray sequences that embed multi bits within an audio signal. Proposed audio watermarking algorithm aims to satisfy and maximize both imperceptibility and robustness of the watermark by embedding 1023 bits within a sample of an audio signal. Proposed technique relies on basic principle that on varying the amplitude of the host signal by a few percent produces inaudible variations resulting in a watermarked signal which sounds identical to the original host file. The imperceptibility and the robustness of watermarking based on amplitude variation of host signal according to PN gray sequence is tested using various attacks.","","978-1-4799-1797-6","10.1109/CSNT.2015.196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280000","Audio Watermarking;PN Gray Sequene;Amplitude Variation","Watermarking;Robustness;Signal to noise ratio;Conferences;Multimedia communication;Decoding;Noise measurement","audio watermarking;copy protection;random sequences","audio watermarking;amplitude variation;imperceptibility;audio signal;digital multimedia;online distribution;open environment;information sources;large-scale unauthorized copying;digital data;copyright protection;copyright messages;serial number;digital media;pseudorandom gray sequences;host signal;watermarked signal;PN gray sequence","","","","26","","1 Oct 2015","","","IEEE","IEEE Conferences"
"High-performance audio matching with features learned by convolutional deep belief network","W. Feng; N. Guan; Z. Luo","Institute of Software, College of Computer National University of Defense Technology, Changsha, Hunan, P.R. China, 410073; Institute of Software, College of Computer National University of Defense Technology, Changsha, Hunan, P.R. China, 410073; Institute of Software, College of Computer National University of Defense Technology, Changsha, Hunan, P.R. China, 410073","2016 IEEE 13th International Conference on Signal Processing (ICSP)","16 Mar 2017","2016","","","1724","1728","Audio matching automatically retrieves all excerpts that have the same content as the query audio clip from given audio recordings. The extracted feature is critical for audio matching and the Chroma Energy Normalized Statistics (CENS) feature is the state-of-the-arts. However, CENS might behave unsatisfactorily on some audio because it is a handcraft feature. In this paper, we propose to utilize the features learned by Convolutional Deep Belief Network (CDBN) to enhance the performance of audio matching. Benefit from the strong generalization ability of CDBN, our method works better than CENS based methods on most audio datasets. Since the features learned by CDBN are binary-valued, we can develop a more efficient audio matching algorithm by taking the advantage of this property. Experimental results on both TIMIT dataset and a simulated music dataset confirm effectiveness of the proposed CDBN based method comparing with the traditional CENS feature based algorithm.","2164-5221","978-1-5090-1345-6","10.1109/ICSP.2016.7878122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7878122","audio matching;convolutional deep belief network;content-based audio retrieval","Feature extraction;Convolution;Detectors;Audio recording;Audio databases;Spectrogram;Signal processing algorithms","audio recording;audio signal processing;belief networks;statistical analysis","high-performance audio matching algorithm;convolutional deep belief network;query audio clip;audio recordings;chroma energy normalized statistics;CENS;handcraft feature;CDBN;audio dataset;TIMIT dataset;simulated music dataset","","1","","13","","16 Mar 2017","","","IEEE","IEEE Conferences"
"Audio Word2vec: Sequence-to-Sequence Autoencoding for Unsupervised Learning of Audio Segmentation and Representation","Y. Chen; S. Huang; H. Lee; Y. Wang; C. Shen","Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Carnegie Mellon University, Language Technologies Institute, Pittsburgh, PA, USA; CompStak, New York, NY, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","28 Jun 2019","2019","27","9","1481","1493","In text, word2vec transforms each word into a fixed-size vector used as the basic component in applications of natural language processing. Given a large collection of unannotated audio, audio word2vec can also be trained in an unsupervised way using a sequence-to-sequence autoencoder (SA). These vector representations are shown to effectively describe the sequential phonetic structures of the audio segments. In this paper, we further extend this research in the following two directions. First, we disentangle phonetic information and speaker information from the SA vector representations. Second, we extend audio word2vec from the word level to the utterance level by proposing a new segmental audio word2vec in which unsupervised spoken word boundary segmentation and audio word2vec are jointly learned and mutually enhanced, and utterances are directly represented as sequences of vectors carrying phonetic information. This is achieved by means of a segmental sequence-to-sequence autoencoder, in which a segmentation gate trained with reinforcement learning is inserted in the encoder.","2329-9304","","10.1109/TASLP.2019.2922832","Ministry of Science and Technology, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736337","Audio word2Vec;sequence-to-sequence autoencoder","Phonetics;Speech recognition;Acoustics;Decoding;Training;Task analysis;Recurrent neural networks","audio signal processing;natural language processing;signal representation;speech processing;unsupervised learning","reinforcement learning;natural language processing;SA vector representations;phonetic information;speaker information;unsupervised spoken word boundary segmentation;segmental audio word2vec;word level;unannotated audio word2vec;audio segmentation;sequence-to-sequence autoencoding","","7","","80","IEEE","13 Jun 2019","","","IEEE","IEEE Journals"
"IEEE Draft Standard for Advanced Audio Coding","",,"IEEE P1857.2/D3, May 2013","15 May 2013","2012","","","1","356","This standard defines a set of tools to support specific audio coding functions including general audio coding and lossless audio coding. The tool set defined in this standard, in combination provides regular high quality and efficient coding tool sets for compression, decompression, processing, and representing of audio data to save bandwidth for transmission, to save space for storage, to speed up indexing and multimedia search, and to enhance performance when using mixed media for virtual reality and other applications that demand high bandwidth. The target applications and services include but not limited audio transmission, audio recording, internet streaming, and other video/audio enabled services and applications.","","978-0-7381-8443-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6516510","audio coding;scalable coding;lossless coding;LPC;vector quantization;bit-plane coding;PQ-SPSC;entropy coding","IEEE standards;Audio coding;Loss measurement;Vector quantization;Entropy coding","","","","","","","","15 May 2013","","","IEEE","IEEE Standards"
"Fast Conversion Algorithm for the Dolby Digital (Plus) AC-3 Audio Coding Standards","V. Britanak","Institute of Informatics, Slovak Academy of Sciences, Bratislava, Slovak republic","IEEE Signal Processing Letters","15 Nov 2012","2012","19","12","910","913","The Dolby Digital (AC-3) and the Dolby Digital Plus or Enhanced AC-3 (E-AC-3) systems are currently the key enabling technologies for high-quality compression of digital audio signals. For the time-to-frequency transformation of an audio data block and vice versa, both systems have adopted a long transform being the modified discrete cosine transform (MDCT). The AC-3 additionally defines two variants of cosine-modulated filter banks called the first and second short transforms. A fast conversion algorithm is presented to convert the frequency coefficients of the long (MDCT) transform to those of two short transforms and vice versa, directly in the frequency domain. It is based on a block sparse matrix factorization of a conversion matrix. The fast conversion algorithm is efficient in terms of the structural simplicity, arithmetic complexity and memory requirements compared to the obvious conversion methods. Moreover, the existing AC-3 fast computational modules may be simply re-used in the conversion procedures. Consequently, the E-AC-3 to AC-3 bit stream conversion and the AC-3 to E-AC-3 bit stream transcoding can be realized in a simplified and efficient way, thus minimizing the amount of partial decoding/encoding and memory requirements during the conversion and transcoding processes.","1558-2361","","10.1109/LSP.2012.2226028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6336787","AC-3 analysis/synthesis filter banks;AC-3 to E-AC-3 transcoder;block sparse matrix factorization;dolby digital (AC-3);dolby digital plus or enhanced AC-3 (E-AC-3);E-AC-3 to AC-3 conversion;fast conversion algorithm","Transforms;Sparse matrices;Matrix converters;Standards;Frequency conversion;Transcoding;Filter banks;Audio coding","audio coding;decoding;discrete cosine transforms;matrix decomposition;sparse matrices;time-frequency analysis","Dolby digital plus AC-3 audio coding standards;enhanced AC-3 systems;high-quality compression;digital audio signals;time-to-frequency transformation;audio data block;modified discrete cosine transform;MDCT;block sparse matrix factorization;conversion matrix;structural simplicity;arithmetic complexity;memory requirements;fast computational modules;bit stream conversion;E-AC-3 bit stream transcoding;partial decoding-encoding","","1","","10","","22 Oct 2012","","","IEEE","IEEE Journals"
"An improved audio fingerprinting algorithm with robust and efficient","Wei Xiong; Xiaoqing Yu; Jianhua Shi","School of Communication and Information Engineering, Shanghai University, China; School of Communication and Information Engineering, Shanghai University, China; School of Communication and Information Engineering, Shanghai University, China","IET International Conference on Smart and Sustainable City 2013 (ICSSC 2013)","13 Feb 2014","2013","","","377","380","An audio fingerprint is a content-based compact feature that summarizes an audio clip. As same as we using human fingerprint to recognize people, audio fingerprint can be used to retrieval unknown audio clips. In this paper, we propose an algorithm which extracts audio fingerprint based on spectral bark-band energy and PCA (Principle Components Analysis). By using this algorithm, search a huge audio database efficient and highly robust in the presence of noise be possible. Especially, the algorithm enhances the efficiency to a higher level. The preliminary result of our experiments shows prominent performance.","","978-1-84919-707-6","10.1049/cp.2013.1960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737837","Audio Fingerprint;Robust;Efficient;PCA;Content-Based Retrieval","","audio signal processing;principal component analysis","audio fingerprinting algorithm;content-based compact feature;human fingerprint;people recognition;unknown audio clip retrieval;spectral bark-band energy;principle components analysis;PCA;huge audio database","","","","","","13 Feb 2014","","","IET","IET Conferences"
"An enhanced least significant bit modification technique for audio steganography","M. Asad; J. Gilani; A. Khalid","Telecommunication Engineering Department, University of Engineering and Technology Taxila, UET, Taxila-47050, Rawalpindi, Pakistan; Telecommunication Engineering Department, University of Engineering and Technology Taxila, UET, Taxila-47050, Rawalpindi, Pakistan; Telecommunication Engineering Department, University of Engineering and Technology Taxila, UET, Taxila-47050, Rawalpindi, Pakistan","International Conference on Computer Networks and Information Technology","15 Sep 2011","2011","","","143","147","Increased use of electronic communication has given birth to new ways of transmitting information securely. Audio steganography is the science of hiding some secret text or audio information in a host message. The host message before steganography and stego message after steganography have the same characteristics. Least Significant Bit (LSB) modification technique is the most simple and efficient technique used for audio steganography. The conventional LSB modification technique is vulnerable to steganalysis. This paper proposes two ways to improve the conventional LSB modification technique. The first way is to randomize bit number of host message used for embedding secret message while the second way is to randomize sample number containing next secret message bit. The improvised proposed technique works against steganalysis and decreases the probability of secret message being extracted by an intruder. Advanced Encryption Standard (AES) with 256 bits key length is used to secure secret message in case the steganography technique breaks. Proposed technique has been tested successfully on a. wav file at a sampling frequency of 8000 samples/second with each sample containing 8 bits.","2223-6317","978-1-61284-941-6","10.1109/ICCNIT.2011.6020921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6020921","Steganography;Audio Steganography;Steganalysis;LSB Modification Steganography;Information Security;Secret Information Transmission;AES-256","Encryption;Communications technology","audio coding;steganography","enhanced least significant bit modification technique;audio steganography;electronic communication;secret text;secret audio information;stego message;secret message bit;advanced encryption standard","","32","","5","","15 Sep 2011","","","IEEE","IEEE Conferences"
"Representation of spectral envelope with warped frequency resolution for audio coder","R. Sugiura; Y. Kamamoto; N. Harada; H. Kameoka; T. Moriya","Graduate School of Information Science and Technology, The University of Tokyo, Japan; NTT Communication Science Labs., Nippon Telegraph and Telephone Corp., Japan; NTT Communication Science Labs., Nippon Telegraph and Telephone Corp., Japan; NTT Communication Science Labs., Nippon Telegraph and Telephone Corp., Japan; NTT Communication Science Labs., Nippon Telegraph and Telephone Corp., Japan","2014 22nd European Signal Processing Conference (EUSIPCO)","13 Nov 2014","2014","","","51","55","We have devised a method for representing frequency spectral envelopes with warped frequency resolution based on sparse non-negative matrices aiming at its use for frequency domain audio coding. With optimally prepared matrices, we can selectively control the resolution of spectral envelopes and enhance the coding efficiency. We show that the devised method can enhance the subjective quality of the state-of-the-art wide-band coder at 16 kbit/s at a cost of minor additional complexity. The method is therefore, expected to be useful for low-bit-rate and low-delay audio coder for mobile communications.","2076-1465","978-0-9928-6261-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6951969","audio coding;signal processing;frequency warping;non-negative matrix;TCX","Speech;Frequency-domain analysis;Speech coding;Quantization (signal);Audio coding;Sparse matrices","audio coding;frequency-domain analysis;sparse matrices","spectral envelope representation;warped frequency resolution;sparse nonnegative matrices;frequency domain audio coding;coding efficiency enhancement;wideband coder;low-bit-rate audio coder;low-delay audio coder;mobile communications","","","","11","","13 Nov 2014","","","IEEE","IEEE Conferences"
"Summarizing Long-Length Videos with GAN-Enhanced Audio/Visual Features","H. Lee; G. Lee","Seoul National University of Science and Technology, Republic of Korea; Seoul National University of Science and Technology, Republic of Korea","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","3727","3731","In this paper, we propose a novel supervised method for summarizing long-length videos. Many recent works presented successful results in video summarization. However, most videos in those works are short in duration (~5 minutes), and the methods often break down on very long videos (~30 minutes). Moreover, most works only use visual features, while audios provide useful features for the task. Based on these observations, we present a model that exploits both visual and audio features. To handle long videos, our model also refines the extracted features using adversarial networks. To demonstrate our model, we have collected a new dataset of 63 e-sports (~30 minutes) videos, each accompanied by an editorial summary video that is about 10% in length of the original video. Evaluation on this dataset suggests that our method produces quality summaries for very long videos.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022347","Summarization;GAN;Audio;Video;Multimodal","Videos;Feature extraction;Visualization;Task analysis;Mel frequency cepstral coefficient;Benchmark testing;Gallium nitride","audio signal processing;feature extraction;sport;video signal processing","long-length videos;supervised method;video summarization;audio features;editorial summary video;visual features;feature extraction;adversarial networks;e-sports","","","","16","","5 Mar 2020","","","IEEE","IEEE Conferences"
"Research and Implementation of ASOC Audio Driver Model on the Blackfin Platform","Q. Wang; L. Zhao; X. Han","Shenyang Univ. of Chem. Technol., Shenyang, China; Shenyang Univ. of Chem. Technol., Shenyang, China; Shenyang Univ. of Chem. Technol., Shenyang, China","2012 Second International Conference on Intelligent System Design and Engineering Application","3 Apr 2012","2012","","","720","723","The development of Audio driver is very important in embed audio system, which decides the performance of audio system directly. WM8731 audio driver is implemented on uclinux embedded operating system by using ASOC driver model in this paper. And in the design, combining the characteristics of Blackfin561 and uclinux sufficiently. To achieve the demand of real time characteristic by using the techniques of interrupt, DMA and Circle buffer storage, the performance of system is enhanced. Audio driver procedure in this paper is proved steady and reliable after many tests.","","978-1-4577-2120-5","10.1109/ISdea.2012.672","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6173307","ASOC audio driver;Interrupt;DMA;Circle buffer storage","Phase change materials;Buffer storage;Codecs;Audio systems;Kernel;Linux;Hardware","audio signal processing;system-on-chip","ASOC audio driver model;blackfin platform;embedded audio system;WM8731 audio driver;uclinux embedded operating system;Blackfin561 characteristics","","","","6","","3 Apr 2012","","","IEEE","IEEE Conferences"
"Normalization of LPC residue for random access frame in audio coding","H. Shu; R. Yu; H. Huang; S. Rahardja","Institute for Infocomm Research, Agency for Science, Technology & Research, Singapore 138632; Institute for Infocomm Research, Agency for Science, Technology & Research, Singapore 138632; Institute for Infocomm Research, Agency for Science, Technology & Research, Singapore 138632; Institute for Infocomm Research, Agency for Science, Technology & Research, Singapore 138632","2011 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)","17 Nov 2011","2011","","","265","268","In a time-domain audio codec system, linear prediction is typically used as a tool to predict the value of future samples based on the value of previous samples. Such tool is useful to remove redundancy in the signal so that the signal can be more efficiently coded by an entropy coder. Modern digital audio system requires random access capability and because of this requirement, the residue produced by the predictor exhibits a high dynamic range. In this paper, an adaptive normalization scheme is proposed to reduce the energy level of first few prediction residues in random access blocks. This enhances the coding efficiency of the entropy coding. The proposed scheme can be used in speech/audio analysis and synthesis.","1947-1629","978-1-4577-0693-6","10.1109/ASPAA.2011.6082272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6082272","Linear Predictive Coding (LPC);Audio Coding;Random Access;Normalization","Entropy;Audio coding;Acoustics;Energy states;Indexes;Conferences","audio coding;entropy codes;linear predictive coding;speech codecs;speech coding;speech synthesis;time-domain analysis","time-domain analysis;audio codec system;entropy code;digital audio system;adaptive normalization scheme;random access blocks;audio coding;speech synthesis;LPC residue;linear predictive coding","","1","","7","","17 Nov 2011","","","IEEE","IEEE Conferences"
"Optimal frequency characteristics of a car audio system under the traveling noise","M. Tsuruta-Hamamura; T. Hidaka; H. Hasegawa","Utsunomiya University, Graduate school of engineering, Utsunomiya, Japan; Utsunomiya University, Graduate school of engineering, Utsunomiya, Japan; Utsunomiya University, Graduate school of engineering, Utsunomiya, Japan","2018 International Workshop on Advanced Image Technology (IWAIT)","31 May 2018","2018","","","1","3","To improve the car audio system that affects the evaluation of comfortability of a vehicle, the subjective evaluation of music reproduced from the car audio system was conducted with or without car interior noise. As a result, the sound stimuli enhanced low frequency by -6 dB/oct. was most suitable for car interior comfortability with and without noise conditions.","","978-1-5386-2615-3","10.1109/IWAIT.2018.8369673","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8369673","car audio system;subjective evaluation;frequency characteristics;noise;music","Automobiles;Audio systems;Standards;Safety;Reliability;Engines;Roads","acoustic signal processing;audio systems","optimal frequency characteristics;car audio system;car interior noise;car interior comfortability","","","","2","","31 May 2018","","","IEEE","IEEE Conferences"
"Attentive Fusion Enhanced Audio-Visual Encoding for Transformer Based Robust Speech Recognition","L. Wei; J. Zhang; J. Hou; L. Dai","University of Science and Technology of China (USTC),National Engineering Laboratory for Speech and Language Information Processing,Hefei,Anhui,P. R. China; University of Science and Technology of China (USTC),National Engineering Laboratory for Speech and Language Information Processing,Hefei,Anhui,P. R. China; University of Science and Technology of China (USTC),National Engineering Laboratory for Speech and Language Information Processing,Hefei,Anhui,P. R. China; University of Science and Technology of China (USTC),National Engineering Laboratory for Speech and Language Information Processing,Hefei,Anhui,P. R. China","2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","31 Dec 2020","2020","","","638","643","Audio-visual information fusion enables a performance improvement in speech recognition performed in complex acoustic scenarios, e.g., noisy environments. It is required to explore an effective audio-visual fusion strategy for audiovisual alignment and modality reliability. Different from the previous end-to-end approaches where the audio-visual fusion is performed after encoding each modality, in this paper we propose to integrate an attentive fusion block into the encoding process. It is shown that the proposed audio-visual fusion method in the encoder module can enrich audio-visual representations, as the relevance between the two modalities is leveraged. In line with the transformer-based architecture, we implement the embedded fusion block using a multi-head attention based audiovisual fusion with one-way or two-way interactions. The proposed method can sufficiently combine the two streams and weaken the over-reliance on the audio modality. Experiments on the LRS3-TED dataset demonstrate that the proposed method can increase the recognition rate by 0.55%, 4.51% and 4.61% on average under the clean, seen and unseen noise conditions, respectively, compared to the state-of-the-art approach.","2640-0103","978-988-14768-8-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9306348","","Encoding;Visualization;Speech recognition;Decoding;Training;Speech enhancement;Feature extraction","audio signal processing;audio-visual systems;feature extraction;sensor fusion;speech processing;speech recognition;video signal processing","performance improvement;audio-visual fusion strategy;audio-visual information fusion;transformer based robust speech recognition;fusion enhanced audio-visual encoding;recognition rate;audio modality;audiovisual fusion;multihead attention;embedded fusion block;transformer-based architecture;audio-visual representations;encoder module;audio-visual fusion method;encoding process;attentive fusion block;previous end-to-end approaches;audiovisual alignment","","","","26","","31 Dec 2020","","","IEEE","IEEE Conferences"
"Audio-Visual Speech Enhancement Using Multimodal Deep Convolutional Neural Networks","J. Hou; S. Wang; Y. Lai; Y. Tsao; H. Chang; H. Wang","Research Center for Information Technology Innovation, Taipei, Taiwan; Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Department of Biomedical Engineering, National Yang-Ming University, Taipei, Taiwan; Research Center for Information Technology Innovation, Taipei, Taiwan; Department of Audiology and Speech language pathology, Mackay Medical College, New Taipei City, Taiwan; Institute of Information Science, Academia Sinica, Taipei, Taiwan","IEEE Transactions on Emerging Topics in Computational Intelligence","23 Mar 2018","2018","2","2","117","128","Speech enhancement (SE) aims to reduce noise in speech signals. Most SE techniques focus only on addressing audio information. In this paper, inspired by multimodal learning, which utilizes data from different modalities, and the recent success of convolutional neural networks (CNNs) in SE, we propose an audio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual streams into a unified network model. We also propose a multitask learning framework for reconstructing audio and visual signals at the output layer. Precisely speaking, the proposed AVDCNN model is structured as an audio-visual encoder-decoder network, in which audio and visual data are first processed using individual CNNs, and then fused into a joint network to generate enhanced speech (the primary task) and reconstructed images (the secondary task) at the output layer. The model is trained in an endto-end manner, and parameters are jointly learned through back propagation. We evaluate enhanced speech using five instrumental criteria. Results show that the AVDCNN model yields a notably superior performance compared with an audio-only CNN-based SE model and two conventional SE approaches, confirming the effectiveness of integrating visual information into the SE process. In addition, the AVDCNN model also outperforms an existing audio- visual SE model, confirming its capability of effectively combining audio and visual information in SE.","2471-285X","","10.1109/TETCI.2017.2784878","Academia Sinica Thematic Research(grant numbers:AS-105-TP-C02-1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8323326","Audio-visual systems;deep convolutional neural networks;multimodal learning;speech enhancement","Visualization;Speech;Speech enhancement;Noise measurement;Mouth;Training;Convolutional neural networks","audio coding;audio signal processing;backpropagation;feedforward neural nets;image coding;speech enhancement;speech recognition","audio-visual speech enhancement;multimodal deep convolutional neural networks;speech signals;audio information;multimodal learning;visual streams;unified network model;multitask learning framework;visual signals;output layer;audio-visual encoder-decoder network;visual data;enhanced speech;SE process;audio-visual deep CNN-SE model;AVDCNN model","","40","","87","","23 Mar 2018","","","IEEE","IEEE Journals"
"Audio-watermarking based ownership verification system using enhanced DWT-SVD technique","M. S. Al-Yaman; M. A. Al-Taee; H. A. Alshammas","Computer Engineering Department, The University of Jordan, Amman, Jordan; School of Computing & Information Technology, Kingston University London, London, UK; Computer Engineering Department, The University of Jordan, Amman, Jordan","International Multi-Conference on Systems, Signals & Devices","10 May 2012","2012","","","1","5","Audio watermarking has been proposed as an effective solution to solve or mitigate the new challenges of audio ownership verification. These challenges arose from easiness of copying and distributing of audio applications. This paper proposes important performance enhancements to a previously-reported audio ownership verification system based upon utilizing a combination of Discrete Wavelet Transform (DWT) and Singular Value Decomposition (SVD) techniques. A new audio signal framing, DWT matrix formation and embedding methods are proposed and successfully implemented to improve the minimum audio-cover period, quality of the watermarked audio and its robustness against various attacks. Performance evaluation of the developed system demonstrated improvements of 69% in the required minimum audio-cover period, 25% in the Signal-to-Noise Ratio (SNR), and improved robustness against various watermarking benchmark attacks.","","978-1-4673-1591-3","10.1109/SSD.2012.6198022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6198022","Audio watermarking;ownership verification;audio copyright protection;discrete wavelet transform;singular-value decomposition","Watermarking;Discrete wavelet transforms;Robustness;Signal to noise ratio;Matrix decomposition;Media","audio watermarking;discrete wavelet transforms;singular value decomposition","audio-watermarking based ownership verification system;enhanced DWT-SVD technique;discrete wavelet transform;singular value decomposition;audio signal framing;DWT matrix formation","","13","","13","","10 May 2012","","","IEEE","IEEE Conferences"
"Enhancing Sampling and Counting Method for Audio Retrieval with Time-Stretch Resistance","S. Yao; B. Niu; J. Liu","Taiyuan University of Technology, School of Information and Computer, Taiyuan, China; Taiyuan University of Technology, School of Information and Computer, Taiyuan, China; NEC Corporation, System Platform Research Laboratories, Tokyo, Japan","2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)","21 Oct 2018","2018","","","1","5","An ideal audio retrieval method should be not only highly efficient in identifying an audio track from a massive audio dataset, but also robust to any distortion. Unfortunately, none of the audio retrieval methods is robust to all types of distortions. An audio retrieval method has to do with both the audio fingerprint and the strategy, especially how they are combined. We argue that the Sampling and Counting Method (SC), a state-of-the-art audio retrieval method, would be promising towards an ideal audio retrieval method, if we could make it robust to time-stretch and pitch-stretch. Towards this objective, this paper proposes a turning point alignment method to enhance SC with resistance to time-stretch, which makes Philips and Philips-like fingerprints resist to time-stretch. Experimental results show that our approach can resist to time-stretch from 70% to 130%, which is on a par to the state-of-the-art methods. It also marginally improves the retrieval performance with various noise distortions.","","978-1-5386-5321-0","10.1109/BigMM.2018.8499068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8499068","audio retrieval;audio fingerprint;fingerprint matching;LSH;time-stretch","Fingerprint recognition;Turning;Distortion;Resistance;Resists;Indexes;Filtering","acoustic signal processing;audio signal processing;information retrieval","time-stretch resistance;ideal audio retrieval method;audio track;massive audio dataset;audio fingerprint;state-of-the-art audio retrieval method;turning point alignment method;state-of-the-art methods;retrieval performance;Sampling and Counting Method;Philips-like fingerprints;noise distortions","","2","","11","","21 Oct 2018","","","IEEE","IEEE Conferences"
"Audio Captioning Based on Combined Audio and Semantic Embeddings","A. Ö. Eren; M. Sert","Başkent University,Department of Computer Engineering,Ankara,Turkey; Başkent University,Department of Computer Engineering,Ankara,Turkey","2020 IEEE International Symposium on Multimedia (ISM)","22 Jan 2021","2020","","","41","48","Audio captioning is a recently proposed task for automatically generating a textual description of a given audio clip. Most existing approaches use the encoder-decoder model without using semantic information. In this study, we propose a bi-directional Gated Recurrent Unit (BiGRU) model based on encoder-decoder architecture using audio and semantic embeddings. To obtain semantic embeddings, we extract subject-verb embeddings using the subjects and verbs from the audio captions. We use a Multilayer Perceptron classifier to predict subject-verb embeddings of test audio clips for the testing stage. Within the aim of extracting audio features, in addition to log Mel energies, we use a pretrained audio neural network (PANN) as a feature extractor which is used for the first time in the audio captioning task to explore the usability of audio embeddings in the audio captioning task. We combine audio embeddings and semantic embeddings to feed the BiGRU-based encoder-decoder model. Following this, we evaluate our model on two audio captioning datasets: Clotho and AudioCaps. Experimental results show that the proposed BiGRU-based deep model significantly outperforms the state of the art results across different evaluation metrics and inclusion of semantic information enhance the captioning performance.","","978-1-7281-8697-9","10.1109/ISM.2020.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327916","audio captioning;PANNs;GRU;BiGRU","Feature extraction;Task analysis;Semantics;Decoding;Birds;Training;Logic gates","audio signal processing;feature extraction;multilayer perceptrons;signal classification;speech recognition;video signal processing","audio clip;semantic information;bi-directional gated recurrent unit model;encoder-decoder architecture;semantic embeddings;subject-verb embeddings;audio features;pretrained audio neural network;BiGRU-based encoder-decoder model;audio captioning datasets;BiGRU-based deep model;combined audio embeddings;textual description;multilayer perceptron classifier;log Mel energies;PANN;feature extractor","","","","33","","22 Jan 2021","","","IEEE","IEEE Conferences"
"Design and Performance Evaluation of Advanced Digital Audio Broadcasting System","M. Baek; B. Lee; K. Yang; S. Beack; M. Lee; G. Shin; K. Kwon; H. Lim","Electron. Telecommun. Res. Inst. (ETRI), Daejeon, South Korea; Electron. Telecommun. Res. Inst. (ETRI), Daejeon, South Korea; Electron. Telecommun. Res. Inst. (ETRI), Daejeon, South Korea; Electron. Telecommun. Res. Inst. (ETRI), Daejeon, South Korea; Kai-media, Daejeon, South Korea; Marueng, Daejeon, South Korea; Korea Electron. Technol. Inst., Seoul, South Korea; Electron. Telecommun. Res. Inst. (ETRI), Daejeon, South Korea","2015 7th International Conference on Multimedia, Computer Graphics and Broadcasting (MulGraB)","21 Mar 2016","2015","","","9","12","DAB is a traditional digital radio broadcasting technology. To enhance the performance and quality of DAB system, DAB+ is standardized with outer coding and superior audio codec (HE-AACv2). Recently, as audio codec technology develops, state of the art audio codec, USAC has been standardized. This paper designs the DAB+ system with USAC to enhance the audio quality and efficiency. USAC can improve the audio quality and efficiency of the digital radio system better than HE-AACv2. This paper describes the design and implementation of digital audio broadcasting system with USAC audio codec based on DAB+ system.","","978-1-4673-9831-2","10.1109/MulGraB.2015.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7434189","DAB+;USAC","Digital audio broadcasting;Receivers;Codecs;Multiplexing;Encoding","audio coding;digital audio broadcasting;performance evaluation","performance evaluation;advanced digital audio broadcasting system;DAB system;HE-AACv2;DAB+ system;audio quality enhancement;USAC audio codec","","","","6","","21 Mar 2016","","","IEEE","IEEE Conferences"
"A robust audio identification for enhancing audio-based indoor localization","Hye-Seung Cho; Sang-Sun Ko; Hyoung-Gook Kim","Kwangwoon University, Seoul, Rep. of Korea; Kwangwoon University, Seoul, Rep. of Korea; Kwangwoon University, Seoul, Rep. of Korea","2016 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","26 Sep 2016","2016","","","1","6","Nowadays, user localization in indoor environments is more necessary to build many location-based services. This paper presents a robust audio identification method for enhancing a real-time indoor localization system on a mobile device using the audio signals emitted by nearby loudspeakers. The proposed audio identification method deals with various noise distortions due to different noisy indoor locations by using foreground/background audio separation, prominent spectral pitch-based binary audio fingerprinting, and spectral peak-triplet-based audio fingerprinting. Experimental results confirm that the proposed audio identification method is quite robust in different noise conditions and achieves preliminary promising results for discriminating the location and orientation of a user in large indoor locations.","","978-1-5090-1552-8","10.1109/ICMEW.2016.7574701","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574701","Indoor localization system;Audio fingerprinting;Foreground/background audio separation","Fingerprint recognition;Watermarking;Databases;Robustness;Loudspeakers;Spectrogram;Noise measurement","audio signal processing;loudspeakers;mobile handsets","audio-based indoor localization enhancement;robust audio identification method;user localization;indoor environment;location-based services;mobile device;audio signal;loudspeaker;noise distortion;audio separation;spectral pitch-based binary audio fingerprinting;spectral peak-triplet-based audio fingerprinting","","1","","15","","26 Sep 2016","","","IEEE","IEEE Conferences"
"Peak-Based Philips Fingerprint Robust To Pitch-Shift For Massive Audio Retrieval","R. Chu; B. Niu; S. Yao; J. Liu","School of Information and Computer, Taiyuan University of Technology, Taiyuan, China; School of Information and Computer, Taiyuan University of Technology, Taiyuan, China; Institute of Big Data Science and Industry, Shanxi University, Taiyuan, China; Biometrics Research Laboratories, NEC Corporation, Tokyo, Japan","2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM)","5 Dec 2019","2019","","","314","320","An ideal audio retrieval system identifies a short query snippet from a massive audio database with both robustness and efficiency. Unfortunately, none of the existing systems could robustly handle all distortions while keeping efficient. An efficient audio retrieval method of the systems must match the features of the fingerprint. Enhanced Sampling and Counting method (eSC), the state-of-the-art audio retrieval method, proposed for Philips-like fingerprints, has achieved both high efficiency and strong robustness, featuring time-stretch resistance. We argue that Philips fingerprint, robust to many types of distortions except speed-change which includes time-stretch and pitch-shift, combined with eSC is promising towards an ideal audio retrieval system, if we could make it robust to pitch-shift. To achieve the goal, this paper proposes a peak-point based energy bands computation method (PPEB) to enhance Philips fingerprint (PF) with resistance to pitch-shift, and the resulting fingerprint is called Peak-point based Philips fingerprint (PPF). Experimental results show that PPF can resist pitch-shift ranging from 70% to 130%, while retaining the robustness of PF to various noise distortions.","","978-1-7281-5527-2","10.1109/BigMM.2019.000-3","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919347","audio fingerprint;audio retrieval;pitch shift","Fingerprint recognition;Distortion;Robustness;Resists;Spectrogram;Filtering;GSM","audio databases;audio signal processing;query processing","short query snippet;audio database;time-stretch resistance;peak-point based energy bands computation method;pitch-shift;audio retrieval method;enhanced sampling and counting method;peak-based Philips fingerprint","","1","","14","","5 Dec 2019","","","IEEE","IEEE Conferences"
"Audio fingerprinting based on local energy centroid","Xueqian Pan; Xiaoqing Yu; Jijun Deng; Wei Yang; Hongxue Wang","School of Communication and Information Engineering, Shanghai University, 200072, China; School of Communication and Information Engineering, Shanghai University, 200072, China; School of Communication and Information Engineering, Shanghai University, 200072, China; School of Communication and Information Engineering, Shanghai University, 200072, China; School of Communication and Information Engineering, Shanghai University, 200072, China","IET International Communication Conference on Wireless Mobile and Computing (CCWMC 2011)","7 May 2012","2011","","","351","354","Audio fingerprint is an effective representation of an audio signal using low-level features and can be used to identify unlabeled audio based on its content. In this paper, we introduce a robust audio feature, local energy centroid (LEC), which can represent the energy conglomeration degree of the relative small region in the spectrum. Our audio fingerprint is generated based on the LEC feature which is conducive to enhance the robustness of system. In audio retrieval processing, an improved scoring strategy is proposed to resist the linear speed change. Experimental results show that the new fingerprinting system is quite robust in the present of noise and the proposed method can achieve satisfying recognition accuracy.","","978-1-84919-505-8","10.1049/cp.2011.0907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6194776","Audio fingerprint;local energy centroid;hash;noise resistant","","audio signal processing;information retrieval;signal representation","audio fingerprinting;local energy centroid;audio signal representation;low-level features;unlabeled audio identification;robust audio feature;LEC;energy conglomeration degree;audio retrieval processing;linear speed change;signal recognition","","","","","","7 May 2012","","","IET","IET Conferences"
"An audio-haptic feedbacks for enhancing user experience in mobile devices","J. Lim; J. Lee; K. Kyung; J. Ryou","Electronics and Telecommunications Research Institute, Daejeon, Korea; Electronics and Telecommunications Research Institute, Daejeon, Korea; Electronics and Telecommunications Research Institute, Daejeon, Korea; Chungnam National University, Daejeon, Korea","2013 IEEE International Conference on Consumer Electronics (ICCE)","28 Mar 2013","2013","","","49","50","We introduce a haptic library that creates tactile feedback by analyzing audio data. Because the proposed haptic library uses audio signal of application to make tactile feedback, it doesn't need to modify application. Also, user can select a particular audio frequency band from multiple audio sources with haptic profiles; then only selected audio can be converted to the tactile feedback. We designed 4 haptic profiles for specific tactile effects. Finally, application examples of applying the haptic library were introduced.","2158-4001","978-1-4673-1363-6","10.1109/ICCE.2013.6486790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6486790","","Haptic interfaces;Tactile sensors;Libraries;Games;Music;Frequency conversion;Engines","audio signal processing;data analysis;haptic interfaces;mobile computing","audio-haptic feedback;user experience;mobile device;haptic library;audio data analysis;audio signal;tactile feedback;audio frequency band;audio source","","","","6","","28 Mar 2013","","","IEEE","IEEE Conferences"
"Enhanced Modelling of Guitar Distortion","E. Zeki; T. Çiloğlu","Elektrik ve Elektronik Mühendisliği Bölümü, Orta Doğu Teknik Üniversitesi, Ankara, Türkiye; Elektrik ve Elektronik Mühendisliği Bölümü, Orta Doğu Teknik Üniversitesi, Ankara, Türkiye","2015 23nd Signal Processing and Communications Applications Conference (SIU)","22 Jun 2015","2015","","","1473","1476","Digital audio effects are used by many electric guitar players. These effects help players to find their desired tones and sounds. For the modelling of main nonlinear guitar effects, distortion and overdrive, this paper investigates current methods of static modelling and dynamic nonlinear state space solutions. After discussion of previous models, this paper introduces a new method of distortion modelling with system identification called Enhanced Modelling of Guitar Distortion. Enhanced Modelling of Guitar Distortion algorithm will use adaptive network based system identification method ANFIS (Adaptive-Network-Based Fuzzy Inference System). ANFIS is used as a system identification tool in Enhanced Modelling of Guitar Distortion algorithm. This algorithm takes the guitar output signal and pre-amplifies the input with 12AX7 vacuum tube amplifier simulation model to obtain clean channel. ANFIS System Identification block is trained using desired distortion effect input output pair. This training and learning results into a ANFIS structure that can be used for processing future inputs. Using clean channel output as an input to the ANFIS structure, lead channel output is obtained. Real-time implementations of distortion and overdrive will be done using C/C++/C# software languages. Using different distortion effect input output pair, different models of distortion effects can be obtained.","2165-0608","978-1-4673-7386-9","10.1109/SIU.2015.7130122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7130122","Guitar;Audio;Guitar Audio Effects;Audio Signal Processing;Nonlinear Signal Processing;ANFIS;Distortion;Overdrive;System Identification","Adaptation models;Mathematical model;Nonlinear distortion;Electron tubes;Blogs;Real-time systems","audio signal processing;C++ language;fuzzy reasoning;identification;real-time systems","digital audio effects;electric guitar players;nonlinear guitar effects;static modelling;dynamic nonlinear state space solutions;enhanced modelling of guitar distortion algorithm;system identification method;adaptive-network-based fuzzy inference system;12AX7 vacuum tube amplifier simulation model;ANFIS structure;real-time implementations;C++ software languages;C# software languages;C software languages","","","","","","22 Jun 2015","","","IEEE","IEEE Conferences"
"Lossless and secure watermarking scheme in MP3 audio by modifying redundant bit in the frames","B. Yang; P. Wu; Y. Jing; J. Mao","Xi'an Research Inst. of Hi-Tech Hongqing Town, Xi'an, 710025, China; Xi'an Research Inst. of Hi-Tech Hongqing Town, Xi'an, 710025, China; Xi'an Research Inst. of Hi-Tech Hongqing Town, Xi'an, 710025, China; Xi'an Research Inst. of Hi-Tech Hongqing Town, Xi'an, 710025, China","2013 6th International Conference on Information Management, Innovation Management and Industrial Engineering","9 Jan 2014","2013","1","","154","157","This paper expounds the encoding method and the frame structure of MP3 audio in detail and presents a digital watermarking scheme without changing the host audio data. The private bit in the bitstream of the host MP3 frame header is used as an indicator to show the consistency between watermark and maindata. Since the maindata are unchanged, the watermark does not introduce any distortion to the host audio. Secret information is encrypted by Arnold transform before embedding to improve the system security. Furthermore, under the circumstance of packet loss during web transmission, a synchronic symbol is inserted to the watermark to enhance the reconstruction quality of the watermark. Experimental results prove the imperceptibility of the proposed method. Additionally, low processing time makes it a real-time method for streaming media applications.","2155-1472","978-1-4799-0245-3","10.1109/ICIII.2013.6702898","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702898","audio watermarking;MP3;bitstream format;Arnold transform;synchronic symbol","Watermarking;Digital audio players;Synchronization;Encoding;Transforms;Robustness;Data mining","audio watermarking;cryptography;encoding;real-time systems","watermarking scheme;MP3 audio;redundant bit;encoding;digital watermarking;MP3 frame header;encryption;Arnold transform;system security;packet loss;Web transmission;synchronic symbol;real-time method;media streaming","","1","","11","","9 Jan 2014","","","IEEE","IEEE Conferences"
"An approach to enhance the robustness of audio watermark based on Turbo code","R. Hu; T. Zhang; C. Yi","Chongqing Key Laboratory of Signal and Information Processing, Chongqing University of Posts and Telecommunication, 400065, China; Chongqing Key Laboratory of Signal and Information Processing, Chongqing University of Posts and Telecommunication, 400065, China; Chongqing Key Laboratory of Signal and Information Processing, Chongqing University of Posts and Telecommunication, 400065, China","2012 5th International Congress on Image and Signal Processing","25 Feb 2013","2012","","","178","182","We present an approach to enhance robustness of audio watermark based on Turbo code. Firstly, we encode the decreased dimensional watermark sequence using Turbo code, then embed the watermark sequence into the diagonal matrix which has been decomposed by the singular value using low frequency coefficients after the wavelet transform. Finally, we can get the watermark sequence during the watermark extraction, decoded it with the Turbo iterative decoding algorithm. In this article, a pseudo random interleaver is added to the Turbo encoder to enhance the robustness of anti-burst-error. Simulations results show that the algorithm is with strong robustness on anti-general random errors attack and good robustness on anti-burst errors attack.","","978-1-4673-0964-6","10.1109/CISP.2012.6469694","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6469694","Audio watermarking;Turbo code;Interleaver;Burst error;Robustness","Watermarking;Turbo codes;Robustness;Matrix decomposition;Wavelet transforms;Arrays","audio watermarking;interleaved codes;iterative decoding;matrix algebra;turbo codes;wavelet transforms","audio watermark;Turbo code;dimensional watermark sequence;diagonal matrix;singular value;low frequency coefficients;wavelet transform;watermark extraction;Turbo iterative decoding algorithm;pseudo random interleaver;Turbo encoder;antigeneral random error attack;antiburst error attack","","1","","10","","25 Feb 2013","","","IEEE","IEEE Conferences"
