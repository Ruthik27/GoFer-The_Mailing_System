"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Same frame rate IR to enhance visible video conference lighting","C. Wu; R. Samadani; P. Gunawardane",Stanford University EE; HP Labs: Mobile & Immersive Experiences; UC Santa Cruz CS,"2011 18th IEEE International Conference on Image Processing","29 Dec 2011","2011","","","1521","1524","Professional studios produce good lighting by using well-positioned, bright studio lights to illuminate the subjects, while casual desktop video conferencing often suffers from uneven and unreliable lighting. Using bright lights is not an option - they annoy and distract non-professional subjects. In this work, we illuminate the subjects using invisible near infrared (IR) lights and capture the subject simultaneously in IR and visible light. The IR video is used as a reference to enhance the visible video. The advantages of IR illumination for video conferencing have previously been shown, but the prior works require 4 to 8 times higher IR frame rate, with concomitant complexity and bandwidth increases. The enhancement developed here performs well with low IR frame rates equal to the visible frame rate. The technique works well even when uneven lighting causes bright visible light pixels to clip. After describing the technique, comparisons with prior methods are provided.","2381-8549","978-1-4577-1303-3","10.1109/ICIP.2011.6115734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6115734","video relighting;video enhancement;video conferencing","Skin;Lighting;Image color analysis;Face;Light sources;Light emitting diodes;Image segmentation","image enhancement;infrared imaging;studios;teleconferencing;video communication","visible video conference lighting;studio lights;invisible near infrared lights;IR illumination;video enhancement;visible light pixels","","7","","10","","29 Dec 2011","","","IEEE","IEEE Conferences"
"BOLA: Near-Optimal Bitrate Adaptation for Online Videos","K. Spiteri; R. Urgaonkar; R. K. Sitaraman","College of Information and Computer Sciences, University of Massachusetts at Amherst, Amherst, MA, USA; Amazon Prime Video, Seattle, WA, USA; College of Information and Computer Sciences, University of Massachusetts at Amherst, Amherst, MA, USA","IEEE/ACM Transactions on Networking","17 Aug 2020","2020","28","4","1698","1711","Modern video players employ complex algorithms to adapt the bitrate of the video that is shown to the user. Bitrate adaptation requires a tradeoff between reducing the probability that the video freezes (rebuffers) and enhancing the quality of the video. A bitrate that is too high leads to frequent rebuffering, while a bitrate that is too low leads to poor video quality. Video providers segment videos into short segments and encode each segment at multiple bitrates. The video player adaptively chooses the bitrate of each segment to download, possibly choosing different bitrates for successive segments. We formulate bitrate adaptation as a utility-maximization problem and devise an online control algorithm called BOLA that uses Lyapunov optimization to minimize rebuffering and maximize video quality. We prove that BOLA achieves a time-average utility that is within an additive term O(1/V) of the optimal value, for a control parameter V related to the video buffer size. Further, unlike prior work, BOLA does not require prediction of available network bandwidth. We empirically validate BOLA in a simulated network environment using a collection of network traces. We show that BOLA achieves near-optimal utility and in many cases significantly higher utility than current state-of-the-art algorithms. Our work has immediate impact on real-world video players and for the evolving DASH standard for video transmission. We also implemented an updated version of BOLA that is now part of the standard reference player dash.js and is used in production by several video providers such as Akamai, BBC, CBS, and Orange.","1558-2566","","10.1109/TNET.2020.2996964","NSF(grant numbers:CNS-1413998,CNS-1901137); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9110784","Internet video;video quality;adaptive bitrate streaming;Lyapunov optimization;optimal control","Videos;Bit rate;Bandwidth;Internet;Standards;Servers;Heuristic algorithms","image segmentation;optimisation;video coding;video streaming","standard reference player dash;video transmission;real-world video players;near-optimal utility;video buffer size;optimal value;time-average utility;Lyapunov optimization;online control algorithm;utility-maximization problem;video player;multiple bitrates;short segments;segmented videos;video quality;rebuffers;video freezes;complex algorithms;video players;online videos;near-optimal bitrate adaptation;BOLA","","5","","25","IEEE","8 Jun 2020","","","IEEE","IEEE Journals"
"Interactive Video Enhanced Learning-Teaching Process for Digital Native Students","A. L. Franzoni; C. P. Ceballos; E. Rubio","Comput. Sci. Dept., Inst. Tecnol. Autonomo de Mexico (ITAM), Mexico City, Mexico; Comput. Sci. Dept., Inst. Tecnol. Autonomo de Mexico (ITAM), Mexico City, Mexico; Comput. Sci. Dept., Inst. Tecnol. Autonomo de Mexico (ITAM), Mexico City, Mexico","2013 IEEE 13th International Conference on Advanced Learning Technologies","19 Sep 2013","2013","","","270","271","Nowadays the use of video is a natural process for digital natives' students. Several aspects of instructional video in e-learning or in a traditional learning have not yet been well investigated. A major problem with the use of instructional video has been lack of interactivity [1]. It's difficult manage video, students cannot directly jump to a particular part of a video or add some explanations to a specific part by the teacher or the student. Browsing a not interactive video is more difficult and time consuming, because people have to view and listen to the video sequentially this remains a linear process. We defined and developed an interactive platform video online system to allow proactive and random access to video content based on questions or search targets, use of an interactive word glossary, dictionary, an online books, educational video resources, extra explanations for the teachers and comments for students in real time. If learners can determine what to construct or create, they are more likely to engage in learning. Interactive video increases learner-content interactivity, thus potentially motivating students and improving learning effectiveness [1]. We are evaluating this system at some universities of Mexico.","2161-377X","978-0-7695-5009-1","10.1109/ICALT.2013.84","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601927","Interactive video;collaboration;learning process;video","Educational institutions;Streaming media;Collaboration;Computers;Computer science;Cities and towns","computer aided instruction;dictionaries;glossaries;interactive systems","interactive video enhanced learning-teaching process;digital native students;instructional video;e-learning;interactive platform video online system;video content;online books;interactive word glossary;educational video resources;learner-content interactivity;Mexico;learning effectiveness","","2","","8","","19 Sep 2013","","","IEEE","IEEE Conferences"
"A realtime fusion algorithm of visible and infrared videos based on spectrum characteristics","J. Wu; H. Hu; Y. Gao","Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing, 100191; Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing, 100191; Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing, 100191","2016 IEEE International Conference on Image Processing (ICIP)","8 Dec 2016","2016","","","3369","3373","The fusion of visible and infrared videos can improve the perceptual quality of videos under severe environments, which is important for video surveillance. Since the infrared and visible videos have different spectrum characteristics, it is difficult to retain the hot targets in the fused videos and enhance the textures at the same time. Moreover, a real-time fusion algorithm is highly required in video surveillance. Therefore, this paper proposes a real-time fusion algorithm of visible and infrared videos by fully utilizing their different spectrum characteristics. A hot-target-oriented fusion based on the gray distribution is firstly proposed to retain the hot targets in the fused videos. Moreover, a texture-enhanced fusion is proposed to enhance the texture information of the fused videos based on the guided filter. Furthermore, the inter-frame correlations are employed to speed up fusion process. The experimental results demonstrate that the proposed algorithm can achieve real-time video fusion and improve both subjective and objective qualities compared with the state-of-the-art.","2381-8549","978-1-4673-9961-6","10.1109/ICIP.2016.7532984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532984","visible video;infrared video;spectrum characteristics;video fusion","Decision support systems;Indexes","correlation methods;image filtering;image fusion;image texture;video surveillance","real-time video fusion algorithm;visible videos;infrared videos;spectrum characteristics;videos perceptual quality;video surveillance;hot-target-oriented fusion;gray distribution;texture-enhanced fusion;guided filter;interframe correlations","","1","","22","","8 Dec 2016","","","IEEE","IEEE Conferences"
"A resolution-adaptive interpolation filter for video codec","H. Lv; R. Wang; Y. Li; C. Zhu; H. Jia; X. Xie; W. Gao","School of Electronic and Computer Engineering, Peking University Shenzhen Graduate School, China; School of Electronic and Computer Engineering, Peking University Shenzhen Graduate School, China; National Engineering Laboratory of Video Technology, Peking University, Beijing, China; National Engineering Laboratory of Video Technology, Peking University, Beijing, China; National Engineering Laboratory of Video Technology, Peking University, Beijing, China; National Engineering Laboratory of Video Technology, Peking University, Beijing, China; National Engineering Laboratory of Video Technology, Peking University, Beijing, China","2014 IEEE International Symposium on Circuits and Systems (ISCAS)","26 Jul 2014","2014","","","542","545","The fraction-pel interpolation filter varies in the video coding standards such as H.264/AVC, AVS and HEVC. Since fractional-pel motion compensation plays an important role in the video encoder, the interpolation of fractional-pel pixels can be refined and designed better to enhance the coding efficiency. In this paper, we firstly propose the generation algorithm of interpolation filter coefficients, and four different tap filters, namely 4tap, 6 tap, 8 tap and 10tap, are tested. A resolution-adaptive interpolation filter for different resolution videos is then introduced based on this algorithm to achieve the maximum bitrate saving. In the proposed scheme, 4 tap filter is applied for the UHD (2560×1600 and above) videos, 6 tap filter and 10 tap filter are performed in the videos whose resolution ranging from 720P (1280×720) to 1080P (1920×1080) and the videos with the resolution below 720P, respectively. When 4 tap filter and 6 tap filter are used in high-definition video, the coding efficiency can increase and the computational complexity will reduce greatly, which is actually beneficial to make hardware optimization more effectively especially SIMD (Single Instruction Multiple Data) and VLSI design. Experiments show that the average BD-rate gains on luma Y, chroma U and V are 1.4%, 0.7% and 0.7% for LP-Main configuration, when conducted in HEVC reference software HM11.0. The coding efficiency gains are significant for some video sequences and can reach up to 6.1%.","2158-1525","978-1-4799-3432-4","10.1109/ISCAS.2014.6865192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6865192","Resolution-adaptive;HEVC;interpolation","Interpolation;Filtering theory;Finite impulse response filters;Complexity theory;Encoding;Video coding;Filtering algorithms","computational complexity;data compression;filtering theory;image resolution;integrated circuit design;interpolation;motion compensation;video codecs;video coding;VLSI","resolution-adaptive interpolation filter;video codec;fraction-pel interpolation filter;video coding standards;H.264-AVC standard;AVS standard;HEVC standard;fractional-pel motion compensation;video encoder;fractional-pel pixel interpolation;tap filters;4tap filter;6 tap filter;8 tap filter;10tap filter;maximum bitrate saving;coding efficiency gains;computational complexity reduction;VLSI design;SIMD;single instruction multiple data;video sequences;video compression","","2","","12","","26 Jul 2014","","","IEEE","IEEE Conferences"
"Object detection, tracking and counting using enhanced BMA on static background videos","P. S. Khude; S. S. Pawar","Department of Computer Engineering, D.Y Patil College of Engineering, Pune University, Pune, India; Department of Computer Engineering, D.Y Patil College of Engineering, Pune University, Pune, India","2013 IEEE International Conference on Computational Intelligence and Computing Research","27 Jan 2014","2013","","","1","4","The availability of high quality and inexpensive video cameras, and the demand for automated video analysis has generated a great deal of interest in object tracking, counting algorithms. This paper presents an approach to count the moving objects or vehicles of traffic scenes recorded by static cameras. Different algorithms and sensors are used for object detection tracking and counting which increases the overall cost and gives less accurate result due to intensity of camera used for recording the video traffic. This paper proposes enhanced BMA(Block matching algorithm) with counting by using kernel tracking (template matching). Background subtraction technique is used to extract moving object from videos subsequently, then BMA and dilution technique is used to isolate and identify image blocks as single vehicle or object. Choice is given to user for selecting one or two different region to count the object. If the object passes from the region then only it will be counted. Finally the number of count of objects is done and sum of all objects (vehicles) is calculated in case of multiple lanes.","","978-1-4799-1597-2","10.1109/ICCIC.2013.6724236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724236","Object counting;ROI;BMA;Template matching","Videos;Object tracking;Object detection;Vehicles;Cameras;Kernel","feature extraction;image matching;image motion analysis;object detection;object tracking;traffic engineering computing;video signal processing","object detection;object tracking;object counting;enhanced BMA;static background video;video camera;automated video analysis;static camera;video traffic;block matching algorithm;kernel tracking;template matching;background subtraction technique;dilution technique","","4","","11","","27 Jan 2014","","","IEEE","IEEE Conferences"
"Artifacts Reduction GAN For Enhancing Quality Of Compressed Panoramic Video","X. Wang; X. Jing; H. Huang; Y. Cui; M. Kadoch; M. Cheriet","Beijing University of Posts and Telecommunications,Beijing,China; Beijing University of Posts and Telecommunications,Beijing,China; Beijing University of Posts and Telecommunications,Beijing,China; Beijing University of Posts and Telecommunications,Beijing,China; University of Quebec,ETS,Montreal,Canada; University of Quebec,ETS,Montreal,Canada","2020 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)","19 Mar 2021","2020","","","1","5","Panoramic video has the characteristics of high resolution, massive information and high sense of immersion, bringing unprecedented visual sensory enjoyment to us. However, considering the limited capacity of current cellular network, videos transmitted to users are often encoded, e.g. by High Efficiency Video Coding (HEVC), resulting in block artifact problems that significantly affects the visual quality. Thus, it is necessary to enhance the quality of compressed panoramic videos. Inspired by the convolutional networks (CNN) and generative adversarial networks (GAN), the paper proposes a deep GAN model -Artifacts Reduction GAN (ARGAN) which is able to enhance the quality of compressed panoramic videos. ARGAN has the ability of reducing artifacts caused by HEVC. Meanwhile, it can increase the visual realistic of the enhanced videos. We tested the performance of our model under PSNR, SSIM and Perception Index. Qualitative results are also provided to display the visual effects of ARGAN. Experimental results show that our method is superior to other quality enhancement methods in both qualitative and quantitative aspects.","2155-5052","978-1-7281-5784-9","10.1109/BMSB49480.2020.9379855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9379855","artificial intelligence in media processing;generative adversarial network;video coding and processing;video quality enhancement","Media;Generative adversarial networks;Visual effects;Encoding;Multimedia communication;Indexes;High efficiency video coding","data compression;image enhancement;video coding;video signal processing","enhancing quality;compressed panoramic Video;unprecedented visual sensory enjoyment;current cellular network;High Efficiency Video Coding;block artifact problems;visual quality;compressed panoramic videos;deep GAN model -Artifacts Reduction GAN;enhanced videos;quality enhancement methods","","","","20","","19 Mar 2021","","","IEEE","IEEE Conferences"
"Enhanced Surveillance Video Compression with Dual Reference Frames Generation","L. Zhao; S. Wang; S. Wang; Y. Ye; S. Ma; W. Gao","School of Electronic Engineering and Computer Science, Institute of Digital Media, Peking University, Beijing 100871, China, and National Engineering Laboratory for Video Technology, Peking University, Beijing 100871, China.; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong.; School of Electronic Engineering and Computer Science, Institute of Digital Media, Peking University, Beijing 100871, China, and National Engineering Laboratory for Video Technology, Peking University, Beijing 100871, China.; Alibaba Group US, Sunnyvale, CA 94085, USA.; School of Electronic Engineering and Computer Science, Institute of Digital Media, Peking University, Beijing 100871, China, and National Engineering Laboratory for Video Technology, Peking University, Beijing 100871, China.; School of Electronic Engineering and Computer Science, Institute of Digital Media, Peking University, Beijing 100871, China, and National Engineering Laboratory for Video Technology, Peking University, Beijing 100871, China.","IEEE Transactions on Circuits and Systems for Video Technology","","2021","PP","99","1","1","In this paper, we improve the inter coding performance of surveillance videos by simultaneously investigating the distinct characteristics of background and foreground redundancy, and introduce two novel reference frames in a complementary manner. On one hand, a block level background reference frame (BRF) is proposed to reduce the background redundancy. The proposed scheme incorporates semantic information into the compression process, and makes use of instance segmentation to facilitate the background block decision, making the generated BRF free from foreground pollution. On the other hand, in order to handle foreground redundancy, a foreground reference frame (FRF) is generated based on Surveillance Prediction Generative Adversarial Network (SP-GAN), which utilizes previous reconstructed frames, optical flow based prediction, as well as BRF to infer the foreground objects of the to-be-coded frame. We integrate the proposed scheme into HM-16.6 software and append BRF and FRF to the reference pictures list (RPS). Simulation results demonstrate considerable superiority of the proposed scheme. In particular, by adding the proposed BRF to RPS, 3% coding gains are observed compared with the state-of-the-art BRF method. When both BRF and FRF are incorporated into RPS, 5.8% gains are achieved for surveillance video coding.","1558-2205","","10.1109/TCSVT.2021.3073114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9404218","Surveillance video;video compression;background reference frame;foreground reference frame","Surveillance;Encoding;Redundancy;Video coding;Streaming media;Task analysis;Generative adversarial networks","","","","","","","IEEE","14 Apr 2021","","","IEEE","IEEE Early Access Articles"
"An Enhanced fractional Motion Estimation algorithm for HD video","P. Arnaudov; T. Ogunfunmi","Department of Electrical Engineering, Santa Clara University, CA USA; Department of Electrical Engineering, Santa Clara University, CA USA","2017 IEEE International Conference on Consumer Electronics (ICCE)","30 Mar 2017","2017","","","221","222","Motion estimation (ME) consumes the major part of time and power in both video compression standards - HEVC and H.264. This paper evaluates the impact of fractional motion estimation (FME) for HD videos if applied along with Signature Based FME algorithm, which targets Full Search quality. The paper compares the quality of results of Fractional FME (FFME) vs Enhanced FME (EFME) assuming they have similar computation complexity, hence power consumption. The main purpose is improving the image quality for the same power consumption or equivalently reducing cost and power for the same quality in handheld devices performing Motion Estimation. The algorithms used are an extension to one of the most efficient algorithms - HMDS.","2158-4001","978-1-5090-5544-9","10.1109/ICCE.2017.7889291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889291","Motion estimation;Video Compression;Video coding;HEVC;H.264;HMDS;Hash Table","Motion estimation;Resists;Algorithm design and analysis;High definition video;Diamond;Standards;High efficiency video coding","computational complexity;data compression;motion estimation;video coding","enhanced fractional motion estimation algorithm;HD video;video compression standards;HEVC;H.264;signature Based FME algorithm;full search quality;computation complexity;power consumption;image quality","","","","9","","30 Mar 2017","","","IEEE","IEEE Conferences"
"Design and implementation of high-performance video processor for head-mounted displays","Hou Zuoxun; Ge Chenyang; Zhao Wenzhe; Liu Longjun; Zheng Nanning","School of Electronics and Information Engineering, Xi'an Jiaotong University, China; School of Electronics and Information Engineering, Xi'an Jiaotong University, China; School of Electronics and Information Engineering, Xi'an Jiaotong University, China; School of Electronics and Information Engineering, Xi'an Jiaotong University, China; School of Electronics and Information Engineering, Xi'an Jiaotong University, China","2011 3DTV Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)","16 Jun 2011","2011","","","1","4","This paper presents the design of a high-performance processor used for head-mounted display (HMD) applications targeting stereo video processing. The proposed hardware architecture of the video processor consists of three major parts: an adaptive 3-dimensional (3D) video decoder to accurately decoding the stereo composite video base band signal (CVBS) source, a video source separation module to generate the 3D display effects while maintaining the original field frequencies on both output channels, and a image post-processing module to enhance the display quality. Furthermore, the paper discusses the key design issue on compact hardware structure for SDRAM access, which is ultimately achieved in a single general SDRAM by data clustering and integration. Both FPGA and ASIC implementations are carried out and the results carefully compared showing that the designed video processor for 3D display could produce well immersing feeling with limited costs in effective decreasing the noise, flicker and crosstalk.","2161-203X","978-1-61284-162-5","10.1109/3DTV.2011.5877154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5877154","Head-mounted display (HMD);video processor;SDRAM access structure","Three dimensional displays;Decoding;SDRAM;Hardware;Pixel;Streaming media;Source separation","application specific integrated circuits;DRAM chips;field programmable gate arrays;helmet mounted displays;pattern clustering;stereo image processing;three-dimensional displays;video coding","high-performance video processor;head-mounted displays;HMD;stereo video processing;three-dimensional video decoder;3D video decoder;composite video base band signal source;CVBS source;video source separation module;3D display effects;image post-processing module;SDRAM access;FPGA;ASIC;DESIGN data clustering;data integration","","","","11","","16 Jun 2011","","","IEEE","IEEE Conferences"
"A real-time, DSP-based JPWL implementation for wireless High Definition video transmission","F. Fiorucci; G. Baruffa; P. Micanti; F. Frescura","Dept. of Electronic and Information Engineering, University of Perugia, Italy; Dept. of Electronic and Information Engineering, University of Perugia, Italy; Dept. of Electronic and Information Engineering, University of Perugia, Italy; Dept. of Electronic and Information Engineering, University of Perugia, Italy","2011 IEEE International Conference on Multimedia and Expo","5 Sep 2011","2011","","","1","4","In this paper we present a novel implementation of the JPWL standard on a DSP device, targeted to the protection of HD video streams at 50 Mbit/s. Several investigations on computational cost have been carried out in order to find the parts that had to be optimized for DSP. The result is an implementation that uses tailored instructions for operation on Galois Field and Enhanced Direct Memory Access (EDMA) for interleaving.","1945-788X","978-1-61284-350-6","10.1109/ICME.2011.6012054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012054","JPWL;DSP;Real-time video processing","High definition video;Real time systems;Indexes;Digital signal processing;IEC standards;IEC","digital signal processing chips;Galois fields;high definition video;video coding;video streaming;wireless channels","wireless high definition video transmission;JPWL standard;real-time DSP device;HD video stream protection;Galois field;enhanced direct memory access;wireless channels;bit rate 50 Mbit/s","","","","9","","5 Sep 2011","","","IEEE","IEEE Conferences"
"On Enhancing Error-Tolerability of Videos via Re-Encoding with Adaptive I-Frame Insertion","T. -Y. Hsieh; C. -C. Chung; J. -T. Wu","National Sun Yat-sen University,Department of Electrical Engineering,Kaohsiung,Taiwan; National Sun Yat-sen University,Department of Electrical Engineering,Kaohsiung,Taiwan; National Sun Yat-sen University,Department of Electrical Engineering,Kaohsiung,Taiwan","2020 IEEE International Test Conference in Asia (ITC-Asia)","16 Oct 2020","2020","","","136","141","Videos are expected to be widely used in IoT or AI applications. The quality of videos are thus crucial for the success of these technologies. However, noises during transmission of videos, or aging of video processing or storage related circuits may significantly degrade the quality of videos, making the video become unacceptable. In this work we investigate the issue of video quality (error-tolerability) enhancement. Different from the previous work that mainly focuses on noisy videos, this work considers erroneous videos generated by faulty circuits. Single stuck-at faults are injected to an H.264 decoding circuit for generating erroneous videos. We find that adaptive insertion of I-frames is an attractive solution. This method first examines the quality of the current videos and accordingly suggests re-encoding of future videos with a proper number of additional I-frames. An error-tolerability enhancement flow for videos is proposed, which integrates video quality grading and determination of the suggested number of I-frames to be inserted. Limitations and implementation of this flow are also discussed. Experimental results on a total of 81,412 erroneous videos show that more than 90% of unacceptable erroneous videos whose quality is within a specified range become acceptable by applying the proposed flow.","","978-1-7281-8944-4","10.1109/ITC-Asia51099.2020.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9226542","error-tolerability;erroneous video;I-frame insertion;video re-encoding;video quality assessment;video quality enhancement","Videos;Circuit faults;Quality assessment;Decoding;Maintenance engineering;Image coding;Benchmark testing","fault tolerant computing;logic testing;video coding;video signal processing","video quality grading;video processing;video quality enhancement;noisy videos;error-tolerability enhancement flow;adaptive I-frame insertion","","","","17","","16 Oct 2020","","","IEEE","IEEE Conferences"
"Knowledge-Enhanced Mobile Video Broadcasting Framework With Cloud Support","X. Huang; J. Wu; F. Hu","Department of Information and Communication Engineering, Tongji University, Shanghai, China; Department of Computer Science and Technology, Tongji University, Shanghai, China; Department of Electrical and Computer Engineering, The University of Alabama, Tuscaloosa, AL, USA","IEEE Transactions on Circuits and Systems for Video Technology","5 Jan 2017","2017","27","1","6","18","The convergence of mobile communications and cloud computing facilitates the cross-layer network design and content-assisted communication. Mobile video broadcasting can benefit from this trend by utilizing joint source-channel coding and strong information correlation in clouds. In this paper, a knowledge-enhanced mobile video broadcasting (KMV-Cast) is proposed. The KMV-Cast is built on a linear video transmission instead of a traditional digital video system, and exploits the hierarchical Bayesian model to integrate the correlated information into the video reconstruction at the receiver. The correlated information is distilled to obtain its intrinsic features, and the Bayesian estimation algorithm is used to maximize the video quality. The KMV-Cast system consists of both likelihood broadcasting and prior knowledge broadcasting. The simulation results show that the proposed KMV-Cast scheme outperforms the typical linear video transmission scheme called Softcast, and achieves 8 dB more of the peak signal-to-noise ratio (PSNR) gain at low-SNR channels (i.e., -10 dB), and 5 dB more of PSNR gain at high-SNR channels (i.e., 25 dB). Compared with the traditional digital video system, the proposed scheme has 7 dB more of PSNR gain than the JPEG2000 + 802.11a scheme at a 10-dB channel SNR.","1558-2205","","10.1109/TCSVT.2016.2555758","National Natural Science Foundation of China(grant numbers:61390513,61571329); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7458835","Cloud computing;correlated information;hierarchical Bayesian model;quality of service (QoS);wireless video transmission","Mobile communication;Cloud computing;Image reconstruction;Discrete cosine transforms;Mobile computing;Bayes methods;Multimedia communication","Bayes methods;cloud computing;combined source-channel coding;mobile communication;video communication","knowledge-enhanced mobile video broadcasting framework;cloud support;mobile communications;cloud computing;cross-layer network design;content-assisted communication;joint source-channel coding;information correlation;KMV-Cast;digital video system;hierarchical Bayesian model;video reconstruction;intrinsic features;Bayesian estimation algorithm;video quality maximization;likelihood broadcasting;prior knowledge broadcasting;linear video transmission scheme;Softcast;peak signal-to-noise ratio;PSNR;JPEG2000","","8","","18","","25 Apr 2016","","","IEEE","IEEE Journals"
"Nonlinear Depth Map Resampling for Depth-Enhanced 3-D Video Coding","P. Aflaki; M. Hannuksela; D. Rusanovskyy; M. Gabbouj","Tampere University of Technology, Tampere, Finland; Nokia, Tampere, Finland; Nokia, Tampere, Finland; Tampere University of Technology, Tampere, Finland","IEEE Signal Processing Letters","3 Dec 2012","2013","20","1","87","90","Depth-enhanced 3-D video coding includes coding of texture views and associated depth maps. It has been observed that coding of depth map at reduced resolution provides better rate-distortion performance on synthesized views comparing to utilization of full resolution (FR) depth maps in many coding scenarios based on the Advanced Video Coding (H.264/AVC) standard. Conventional techniques for down and upsampling do not take typical characteristics of depth maps, such as distinct edges and smooth regions within depth objects, into account. Hence, more efficient down and upsampling tools, capable of preserving edges better, are needed. In this letter, novel non-linear methods to down and upsample depth maps are presented. Bitrate comparison of synthesized views, including texture and depth map bitstreams, is presented against a conventional linear resampling algorithm. Objective results show an average bitrate reduction of 5.29% and 3.31% for the proposed down and upsampling methods with ratio ½, respectively, comparing to the anchor method. Moreover, a joint utilization of the proposed down and upsampling brings up to 20% and on average 7.35% bitrate reduction.","1558-2361","","10.1109/LSP.2012.2228189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6373714","Depth map;MVC;nonlinear;resampling","Video coding;Transform coding;Encoding;Signal processing algorithms;Image coding;Bit rate;Joints","image enhancement;image resolution;image sampling;image texture;rate distortion theory;video coding","full resolution nonlinear depth map resampling;depth-enhanced 3D video coding;image texture view coding;image resolution;rate-distortion performance;FR depth map;H.264-AVC standard;advanced video coding standard;edge preservation;nonlinear method;average bitrate reduction;upsampling method","","15","","18","","3 Dec 2012","","","IEEE","IEEE Journals"
"Reinforcement Learning for Enhanced Content Based Video Frame Retrieval System in Low Resolution Videos","T. Ullah; A. Khan; M. Waleed","University of Engineering and Technology, Peshawar,Department of Computer Systems Engineering,Peshawar,Pakistan; University of Engineering and Technology, Peshawar,Department of Computer Systems Engineering,Peshawar,Pakistan; University of Engineering and Technology, Peshawar,Department of Computer Systems Engineering,Peshawar,Pakistan","2020 12th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)","16 Oct 2020","2020","","","1","6","Over the past two decades, evolutional advancement took place in the field of computer, communication and multimedia technologies which led to the production of massive video data production and large image repositories. This research focuses on the video frame retrieval using popular feature extraction method of computer vision called Speeded Up Robust Features (SURF) and Reinforcement Learning (RL) based Point of Interest (POI) calculation and frame reduction. The proposed method uses the concept of Content-Based-Image-Retrieval (CBIR) which retrieves similar images from the collection of large image databases. In video processing, a challenging task is to remove redundant frames as most of the videos have 30 Frames per Second (FPS). This research study consists of the removal of redundant frames and retrieving of similar frames from remaining non-redundant frames based on a query image. Validation for the proposed scheme has been achieved by simulating the system for a standard dataset as well as videos captured ourselves. The results depict the efficacy of the system and provide a platform for benchmarking.","","978-1-7281-6843-2","10.1109/ECAI50035.2020.9223199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9223199","Video Frame Retrieval;Reinforcement Learning (RL);Content Based Image Retrieval (CBIR);Feature Extraction;Redundant Frame Removal;Query Image;Histogram of Oriented Gradient (HOG)","Feature extraction;Reinforcement learning;Euclidean distance;Histograms;Visualization;Task analysis;Computer vision","computer vision;content-based retrieval;feature extraction;image resolution;image retrieval;learning (artificial intelligence);video retrieval;video signal processing;visual databases","reinforcement learning;low resolution videos;evolutional advancement;multimedia technologies;massive video data production;image repositories;popular feature extraction method;computer vision;frame reduction;content-based-image-retrieval;image databases;video processing;redundant frames;similar frames;nonredundant frames;query image;robust features;enhanced content based video frame retrieval system;enhanced content","","","","26","","16 Oct 2020","","","IEEE","IEEE Conferences"
"Transport and Storage Systems for 3-D Video Using MPEG-2 Systems, RTP, and ISO File Format","T. Schierl; S. Narasimhan","Fraunhofer Institute for Telecommunications—Heinrich Hertz Institute (HHI) and Technische Universität Berlin, Berlin, Germany; Motorola, San Diego, CA, USA","Proceedings of the IEEE","17 Mar 2011","2011","99","4","671","683","Three-dimensional video based on stereo and multiview video representations is currently being introduced to the home through various channels, including broadcast such as via cable, terrestrial and satellite transmission, streaming and download through the Internet, as well as on storage media such as Blu-ray discs. In order to deliver 3-D content to the consumer, different media system technologies have been standardized or are currently under development. The most important standards are MPEG-2 systems, which is used for digital broadcast and storage on Blu-ray discs, real-time transport protocol (RTP), which is used for real-time transmissions over the Internet, and the ISO base media file format, which can be used for progressive download in video-on-demand applications. In this paper, we give an overview of these three system layer approaches, where the main focus is on the multiview video coding (MVC) extension of H.264/AVC and the application of the system approaches to the delivery and storage of MVC.","1558-2256","","10.1109/JPROC.2010.2091370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5672764","Advanced video coding (AVC);broadcast;depth-enhanced stereo;hypertext transfer protocol (HTTP) streaming;H.264;Internet protocol television (IPTV);media transport;MPEG-4;multiview video;multiview video coding (MVC);progressive download;real time;standards;stereo 3-D","Streaming media;Automatic voltage control;Transform coding;Decoding;Encoding;Data storage systems;Video coding;Three dimensional displays","Internet;ISO standards;optical disc storage;stereo image processing;storage media;transport protocols;video coding;video on demand;video streaming","transport systems;storage systems;3D video;MPEG-2 systems;RTP;ISO file format;three-dimensional video;stereo video representations;multiview video representations;cable channels;terrestrial channels;satellite transmission channels;video streaming;video download;Internet;storage media;Blu-ray discs;media system technology;digital broadcast;real-time transport protocol;real-time transmissions;ISO base media file format;progressive download;video-on-demand applications;multiview video coding extension;MVC extension;H.264/AVC","","22","7","34","","23 Dec 2010","","","IEEE","IEEE Journals"
"Enhanced quality adaptation strategies for Scalable Video","M. Uitto; J. Vehkaperä","VTT Technical Research Centre of Finland, Kaitoväylä 1, 90571, Oulu, Finland; VTT Technical Research Centre of Finland, Kaitoväylä 1, 90571, Oulu, Finland","IEEE International Symposium on Signal Processing and Information Technology","3 Apr 2014","2013","","","000074","000079","Technology today favors tightly compressed, but still extremely high quality video transmitted wireless to mobile devices. Users want to access the video data anytime, anywhere, which sets challenges in the highly-loaded network to provide best possible video quality for the end user. Naturally, in the era of high-definition, users want to experience high quality from their mobile devices even when the channel capacity fluctuates drastically due to congestion. Video adaptation to bandwidth fluctuations plays an important role in modern video transmission in order to maximize the best available video quality, but again to use available bandwidth efficiently. In this paper, we aim to find the best strategy for how the adaptation should be performed, to find sufficient adaptation period and number of layers that maximize the compression efficiency, usage of channel resources and objective quality in video streaming. For the compression, we utilize Scalable Video Coding (SVC), which provides layered video structure and enables video adaptation to bandwidth fluctuations. The results indicate that long adaptation period with complex content encoded for two enhancement layers brings essential channel savings while also maintaining the compression efficiency and objective quality in adequate level.","2162-7843","978-1-4799-4796-6","10.1109/ISSPIT.2013.6781857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6781857","SVC;scalable video coding;video transmission;QoE;objective quality","Bandwidth","channel capacity;data compression;mobile radio;video coding;video communication;video streaming","enhanced quality adaptation strategy;scalable video coding;high-quality video;compressed video;wireless transmission;video data access;highly-loaded network;mobile devices;channel capacity;video adaptation;bandwidth fluctuations;video transmission;compression efficiency;channel resource usage;objective quality;video streaming;SVC;layered video structure;enhancement layer;channel saving","","1","","15","","3 Apr 2014","","","IEEE","IEEE Conferences"
"MRS-Net+ for Enhancing Face Quality of Compressed Videos","T. Liu; M. Xu; S. Li; R. Ding; H. Liu","School of Electronic and Information Engineering, Beihang University, Beijing, 100191 China.; School of Electronic and Information Engineering, Beihang University, Beijing, 100191 China. (e-mail: MaiXu@buaa.edu.cn); Department of Electrical and Electronic Engineering, Imperial College London, SW7 2AZ, UK.; School of Electronic and Information Engineering, Beihang University, Beijing, 100191 China.; Momo Inc, China.","IEEE Transactions on Circuits and Systems for Video Technology","","2021","PP","99","1","1","During the past few years, face videos, e.g., video conference, interviews and variety shows, have grown explosively with millions of users over social media networks. Unfortunately, the existing compression algorithms are applied to these videos for reducing bandwidth, which also bring annoying artifacts to face regions. This paper addresses the problem of face quality enhancement in compressed videos by reducing the artifacts of face regions. Specifically, we establish a compressed face video (CFV) database, which includes 196,337 faces in 214 high-quality video sequences and their corresponding 1,712 compressed sequences. We find that the faces of compressed videos exhibit tremendous scale variation and quality fluctuation. Motivated by scalable video coding, we propose a multi-scale recurrent scalable network (MRS-Net+) to enhance the quality of multi-scale faces in compressed videos. The MRS-Net+ is comprised by one base and two refined enhancement levels, corresponding to the quality enhancement of small-, medium- and large-scale faces, respectively. In the multi-level architecture of our MRS-Net+, small-/medium-scale face quality enhancement serves as the basis for facilitating the quality enhancement of medium-/large-scale faces. We further develop a landmark-assisted pyramid alignment (LPA) subnet to align faces across consecutive frames, and then apply the mask-guided quality enhancement (QE) subnet for enhancing multi-scale faces. Finally, experimental results show that our MRS-Net+ method achieves averagely 1.196 dB improvement of peak signal-to-noise ratio (PSNR) and 23.54% saving of Bjøntegaard distortion-rate (BD-rate), significantly outperforming other state-of-the-art methods.","1558-2205","","10.1109/TCSVT.2021.3103519","National Natural Science Foundation of China(grant numbers:61922009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9509420","database;face quality enhancement;scalable structure","Faces;Videos;Image coding;Image restoration;Face recognition;Databases;Video sequences","","","","","","","IEEE","9 Aug 2021","","","IEEE","IEEE Early Access Articles"
"Classification of video resolution for the enhanced display of images on HDTV","J. Ryu; G. Kim; S. H. Lee; B. Min; N. I. Cho","INMC, School of Electrical and Computer Engineering, Seoul Nat'l University, Seoul, Republic of Korea; School of Electrical Engineering, Soongsil University, Seoul, Republic of Korea; INMC, School of Electrical and Computer Engineering, Seoul Nat'l University, Seoul, Republic of Korea; Samsung Electronics Co. Ltd., Suwon, Republic of Korea; INMC, School of Electrical and Computer Engineering, Seoul Nat'l University, Seoul, Republic of Korea","2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference","2 Jan 2014","2013","","","1","4","Although video sources for HDTV broadcasting are mainly in HD resolution, some of them are still from low resolution video sources such as videos that are taken long time ago, internet-streamed or cellphone videos. When these sources are displayed on HDTV, they usually appear to be blurry and their color is not vivid. To alleviate these problems, a proper video enhancement for each source is necessary to display them on HDTV with satisfaction. However, since the decoded images on HDTV do not contain the information on the origin of sources in many cases, it needs to be classified whether their origins out from HD source or not. For this, we propose an HD/non-HD classifier based on the Support Vector Machine (SVM) with the frequency information of the selected region in the decoded image. To evaluate the performance of the proposed HD/non-HD classifier, we use a test database of 6252 HD and 6934 SD still images captured from various TV genres. The experimental results show that the proposed classifier yields high accuracy rate of 99.61%.","","978-986-90006-0-4","10.1109/APSIPA.2013.6694318","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6694318","","High definition video;Video sequences;Support vector machines;Image resolution;HDTV;Image edge detection","digital video broadcasting;high definition television;image classification;image enhancement;image resolution;support vector machines;video coding","TV genres;6934 SD still images;6252 HD still images;HD/nonHD classifier;frequency information;SVM;support vector machine;HD source;decoded images;video enhancement;cellphone videos;Internet-streamed videos;low resolution video sources;HD resolution;HDTV broadcasting;image display enhancement;video resolution classification","","","","9","","2 Jan 2014","","","IEEE","IEEE Conferences"
"Enhanced Memory Network for Video Segmentation","Z. Zhou; L. Ren; P. Xiong; Y. Ji; P. Wang; H. Fan; S. Liu","Beijing University of Post and Telecommunications; Institute of Information Engineering, CAS; Megvii Research; TsingHua University; Megvii Research; Megvii Research; Beihang University","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","689","692","This paper proposes an Enhanced Memory Network (EMN) for semi-supervised video object segmentation. Space-Time Memory Networks has proven the effectiveness of the abundant use of guidance information. To further improve the accuracy of unknown and small targets, we propose to perform fined-grained segmentation based on the correlation attention map. We introduce a siamese network to obtain the semantic similarity and relevance between the tracking objects and the whole image. The feature map extracted from the siamese network on the cropped image is multiplied onto the whole feature map as the attention of proposal objects. Also, an ASPP module is employed to increase the semantic receptive filed to further improve the segmentation accuracy on different scale. Based on the multi-object combination and multi-scale ensemble, the proposed algorithm achieves the first place on the YouTube-VOS 2019 Semi-supervised Video Object Segmentation Challenge with a J&F mean score of 81.8%.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022633","video object segmentation;memory network","Feature extraction;Object segmentation;Decoding;Image segmentation;Semantics;Training;Computer vision","feature extraction;image segmentation;neural nets;supervised learning;video signal processing","ASPP module;object tracking;semisupervised video object segmentation;enhanced memory network;feature map extraction;siamese network;correlation attention map;space-time memory networks;video segmentation","","2","","15","","5 Mar 2020","","","IEEE","IEEE Conferences"
"Enhanced lowpass filter based video predictive tracking for target with high mobility","K. Chen; G. Jiang; C. Batur","Electrical and Electronic Engineering Department, College of Information Science and Engineering, Ningbo University, Zhejiang 315211 China; Telecommunication Engineering Department, College of Information Science and Engineering, Ningbo University, Zhejiang 315211 China; Mechanical Engineering Department, College of Engineering, The University of Akron, Ohio 44325 USA","2011 Chinese Control and Decision Conference (CCDC)","1 Aug 2011","2011","","","1254","1258","A lowpass filter is designed to perform the predictive tracking of highly mobile target. The proposed filter consists of linear term and inertial terms, with each respectively responsible for non-high mobility and high mobility. The target high-mobility is detected by measuring the velocity change and the inertial term is to be reconfigured to dominate the filtering process when the target moves in a highly mobile fashion. The tracking tests are conducted on the given set of video sequences and compared against the predictive tracking performed by Kalman filter, suggesting the much better tracking quality and feasibility of the proposed approach.","1948-9447","978-1-4244-8738-7","10.1109/CCDC.2011.5968381","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5968381","Video predictive tracking;Lowpass filter;Tracking quality;Kalman filter;High mobility","Target tracking;Trajectory;Kalman filters;Mobile communication;Equations;Video sequences","image sequences;low-pass filters;target tracking;velocity measurement;video signal processing","video predictive tracking;lowpass filter;target tracking;high mobility target;nonhigh mobility;velocity change measurement;video sequences","","","","5","","1 Aug 2011","","","IEEE","IEEE Conferences"
"A Multiple Description Video Codec With Adaptive Residual Distributed Coding","J. Chen; S. Lee; C. Chen; C. Sun; J. Jhuang; C. Lu","Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan","IEEE Transactions on Circuits and Systems for Video Technology","1 May 2012","2012","22","5","754","768","Multiple description coding (MDC) decomposes one single media into several descriptions and transmits them over different channels for error resilience. Each description contributes to improving the reconstructed media quality when decoded. Distributed video coding (DVC) encodes multiple correlated images and utilizes error correction codes to shift the codec complexity to a joint decoder. Combining MDC with DVC (MDVC) yields a stable codec for mobile encoders. In this paper, to improve the MDVC codec performance, image correlations among the MDVC processing modules were exploited to improve reconstructed video quality and enhance transmission robustness. At the side encoder, a DVC-based adaptive differential pulse code modulation was designed to remove interframe redundancy to enhance rate-distortion performances. For the MDVC central decoding, intradescription and interdescription correlations were utilized to dynamically select the best reconstructed frames from two descriptions, instead of selecting just one description or all key-frames from two descriptions. Experiments showed that, as compared to previous methods, the proposed MDVC control method yielded 1-2 dB higher in image PSNRs for Wyner-Ziv reconstructed frames at the side decoder when encoding low-to-medium complexity videos. For high-complexity videos, it effectively prevents error correction of Wyner-Ziv frames from malfunctioning and yields about 3 dB higher in PSNR. The proposed MDVC central decoder control yields 1-4 dB higher PSNRs, as compared to side decoders. Under lossy transmission, it demonstrates 27-64% smaller PSNR variations, as compared to that of combining key-frames as the decoded video. The proposed MDVC system and control not only improve the DVC reconstructed video quality, but also reduce the quality fluctuation artifacts of MDC coded video for mobile coders.","1558-2205","","10.1109/TCSVT.2011.2179459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6101563","Adaptive central decoder control;multiple description video coding;residual distributed video coding;Wyner–Ziv video coder","Decoding;Image reconstruction;Silicon;Encoding;Codecs;Quantization;Static VAr compensators","adaptive codes;computational complexity;differential pulse code modulation;error correction;error statistics;image reconstruction;video codecs;video coding","multiple description video codec;adaptive residual distributed coding;multiple description coding;error resilience;reconstructed media quality;distributed video coding;multiple correlated images;utilizes error correction codes;codec complexity;joint decoder;stable codec;mobile encoders;MDVC codec performance;image correlations;MDVC processing modules;enhance transmission robustness;side encoder;DVC-based adaptive differential pulse code modulation;interframe redundancy;rate-distortion performances;MDVC central decoding;intradescription;interdescription correlations;MDVC control method;Wyner-Ziv reconstructed frames;side decoder;low-to-medium complexity videos","","1","1","32","","12 Dec 2011","","","IEEE","IEEE Journals"
"Video Processing for Human Perceptual Visual Quality-Oriented Video Coding","H. Oh; W. Kim","College of Electronics and Information, Kyung Hee University, Kyunggi-do, Korea; College of Electronics and Information, Kyung Hee University, Kyunggi-do, Korea","IEEE Transactions on Image Processing","7 Feb 2013","2013","22","4","1526","1535","We have developed a video processing method that achieves human perceptual visual quality-oriented video coding. The patterns of moving objects are modeled by considering the limited human capacity for spatial-temporal resolution and the visual sensory memory together, and an online moving pattern classifier is devised by using the Hedge algorithm. The moving pattern classifier is embedded in the existing visual saliency with the purpose of providing a human perceptual video quality saliency model. In order to apply the developed saliency model to video coding, the conventional foveation filtering method is extended. The proposed foveation filter can smooth and enhance the video signals locally, in conformance with the developed saliency model, without causing any artifacts. The performance evaluation results confirm that the proposed video processing method shows reliable improvements in the perceptual quality for various sequences and at various bandwidths, compared to existing saliency-based video coding methods.","1941-0042","","10.1109/TIP.2012.2233485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6378454","Hedge algorithm;human visual perception;video coding;visual saliency","Visualization;Humans;Video recording;Quality assessment;Spatial resolution;Vectors;Video coding","filtering theory;image classification;image resolution;video coding","human perceptual visual quality-oriented video coding;video processing method;spatial-temporal resolution;human capacity;visual sensory memory;online moving pattern classifier;hedge algorithm;visual saliency;human perceptual video quality saliency model;foveation filter;video signals;artifacts;saliency-based video coding methods","Algorithms;Humans;Image Processing, Computer-Assisted;Models, Theoretical;Motion;Pattern Recognition, Automated;Video Recording;Video Recording;Visual Perception","22","1","33","","11 Dec 2012","","","IEEE","IEEE Journals"
"Coupled Primary and Secondary Transform for Next Generation Video Coding","X. Zhao; L. Li; Z. Li; X. Li; S. Liu","Media Lab, Tencent America, Palo Alto, CA, USA; Department of Computer Science Electrical Engineering, University of Missouri-Kansas City, Kansas City, MO, USA; Department of Computer Science Electrical Engineering, University of Missouri-Kansas City, Kansas City, MO, USA; Media Lab, Tencent America, Palo Alto, CA, USA; Media Lab, Tencent America, Palo Alto, CA, USA","2018 IEEE Visual Communications and Image Processing (VCIP)","25 Apr 2019","2018","","","1","4","The discrete cosine transform type II can efficiently approximate the Karhunen-Loeve transform under the first-order stationary Markov condition. However, the highly dynamic characteristics of natural images will not always follow the first-order stationary Markov condition. It is well known that multi-core transforms and non-separable transforms capture diversified and directional texture patterns more efficiently. And a combination of enhanced multiple transform (EMT) and non-separable secondary transform (NSST) are provided in the reference software of the next generation video coding standard to solve this problem. However, the current method of combining the EMT and NSST may lead to quite significant encoder complexity increase, which makes the video codec rather impractical for real applications. Therefore, in this paper, we investigate the interactions between EMT and NSST, and propose a coupled primary and secondary transform to simplify the combination to obtain a better trade-off between the performance and the encoder complexity. With the proposed method, the transform for the Luma and Chroma components is also unified for a consistent design as an additional benefit. We implement the proposed transform on top of the Next software, which has been proposed for the next generation video coding standard. The experimental results demonstrate that the proposed algorithm can provide significant time reduction while keeping the majority of the performance.","1018-8770","978-1-5386-4458-4","10.1109/VCIP.2018.8698635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698635","Next generation video coding;transform design;enhanced multiple transform;non-separable secondary transform;video coding standard","Transforms;Complexity theory;Software;Video coding;Standards;Encoding;Training","code standards;discrete cosine transforms;image texture;Karhunen-Loeve transforms;Markov processes;video codecs;video coding","first-order stationary Markov condition;natural images;multicore transforms;directional texture patterns;reference software;video codec;discrete cosine transform;Karhunen-Loeve transform;enhanced multiple transform;next generation video coding standard;EMT;encoder complexity;nonseparable secondary transform;NSST;luma-chroma component","","3","","11","","25 Apr 2019","","","IEEE","IEEE Conferences"
"Enhancing the HEVC video analyzer for medical diagnostic videos","M. Tahir; Z. Ul-Abdin; M. A. Qadir","Mohammad Ali Jinnah University, Islamabad, Pakistan; TeleSehat Private Limited, Islamabad, Pakistan; Mohammad Ali Jinnah University, Islamabad, Pakistan","2015 12th International Conference on High-capacity Optical Networks and Enabling/Emerging Technologies (HONET)","4 Feb 2016","2015","","","1","5","Video analyzers are employed to perform an in depth analysis of coding decisions undertaken during the execution of a video codec. Medical diagnostic videos, which are typically dealt with in telemedicine scenarios need careful examination to incorporate the most optimum coding decisions. This paper deals with the enhancement of an open-source video stream analyzer to facilitate codec development tailored for medical diagnostic videos. The proposed extensions include visual representation of quantitative information for the bit count used at CTU level, as well as displaying the different mode decisions adopted in the case of merge mode, prediction mode, and intra mode. We have incorporated the said extensions in HEVC analyzer and validated the approach by using test video sequences for Ultrasound, Eye, and Skin examination.","","978-1-4673-9268-6","10.1109/HONET.2015.7395417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395417","video stream analyzer;telemedicine;medical diagnostic videos;HEVC","Encoding;Streaming media;Video sequences;Video coding;Standards;Medical diagnostic imaging;Skin","medical image processing;telemedicine;video coding","HEVC video analyzer;medical diagnostic videos;telemedicine scenarios;open-source video stream analyzer;codec development;CTU level;test video sequences","","2","","9","","4 Feb 2016","","","IEEE","IEEE Conferences"
"Binocular Suppression-Based Stereoscopic Video Coding by Joint Rate Control With KKT Conditions for a Hybrid Video Codec System","Y. Chang; M. Kim","Asan Medical Center, Seoul, Korea; Korea Advanced Institute of Science and Technology, Daejeon, Korea","IEEE Transactions on Circuits and Systems for Video Technology","6 Jan 2015","2015","25","1","99","111","Asymmetric video coding based on binocular suppression provides a prospect for improving coding efficiency since one view in stereoscopic video can be encoded at lower bitrates than the other view without a loss of perceptual video quality. In this paper, we propose a joint rate control scheme for asymmetric stereoscopic video coding in a hybrid video codec system, which supports binocular suppression-based asymmetric stereoscopic video coding in a theoretical basis within an optimization framework. To obtain an optimal solution to quantization steps for joint rate control, we apply the Karush-Kuhn-Tucker (KKT) conditions in minimizing the perceptual distortion of decoded video under the conditions that: 1) the sum of bits generated from two video encoders is constrained within a given bit budget; 2) the quality of a primary view is superior to that of an auxiliary view with an allowable difference between two view qualities; and 3) the distortion of an auxiliary view is less than a given threshold value for high bitrate encoding. As a result, the proposed rate control scheme effectively enhances the perceptual quality based on an optimization framework by taking into account the binocular suppression and also simultaneously controlling the rates of both the left and right views of stereoscopic video under the constraints with a given target bit budget and an allowed quality difference. Experimental results demonstrate that, compared with the independent rate control scheme, the proposed joint rate control scheme with KKT conditions for asymmetric stereoscopic video coding not only demonstrates comparable 3-D perceived visual quality with an overall gain of 0.33 dB, but also attains 2-D visual quality gain by an average of 2.49 dB while accurately satisfying the given constraints in the proposed optimization framework","1558-2205","","10.1109/TCSVT.2014.2330658","ICT R&D program of MSIP/IITP (10039199, “A Study on Core Technologies of Perceptual Quality based Scalable 3D Video Codecs”); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832526","Asymmetric video coding;binocular suppression;hybrid video codec system;Karush–Kuhn–Tucker (KKT) optimality conditions;rate control","Video coding;Stereo image processing;Bit rate;Visualization;Optimization;Joints;Quantization (signal)","optimisation;quantisation (signal);video codecs;video coding;visual perception","3D perceived visual quality;target bit budget;auxiliary view;video encoders;decoded video;perceptual distortion;quantization steps;optimal solution;optimization framework;asymmetric video coding;KKT conditions;Karush-Kuhn-Tucker conditions;joint rate control;stereoscopic video coding;binocular suppression;hybrid video codec system","","10","","18","","12 Jun 2014","","","IEEE","IEEE Journals"
"VideoMec: A Metadata-Enhanced Crowdsourcing System for Mobile Videos","Y. Wu; G. Cao","Pennsylvania State Univ., University Park, PA, USA; Pennsylvania State Univ., University Park, PA, USA","2017 16th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)","12 Jun 2017","2017","","","143","154","The exponential growth of mobile videos has enabled a variety of video crowdsourcing applications. However, existing crowdsourcing approaches require all video files to be uploaded, wasting a large amount of bandwidth since not all crowdsourced videos are useful. Moreover, it is difficult for applications to find desired videos based on user-generated annotations, which can be inaccurate or miss important information. To address these issues, we present VideoMec, a video crowdsourcing system that automatically generates video descriptions based on various geographical and geometrical information, called metadata, from multiple embedded sensors in off-the-shelf mobile devices. With VideoMec, only a small amount of metadata needs to be uploaded to the server, hence reducing the bandwidth and energy consumption of mobile devices. Based on the uploaded metadata, VideoMec supports comprehensive queries for applications to find and fetch desired videos. For time-sensitive applications, it may not be possible to upload all desired videos in time due to limited wireless bandwidth and large video files. Thus, we formalize two optimization problems and propose efficient algorithms to select the most important videos to upload under bandwidth and time constraints. We have implemented a prototype of VideoMec, evaluated its performance, and demonstrated its effectiveness based on real experiments.","","978-1-4503-4890-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7944786","mobile video;metadata;crowdsourcing;video query;video selection","Videos;Metadata;Mobile communication;Cameras;Mobile handsets;Servers;Bandwidth","crowdsourcing;image sensors;meta data;mobile computing;optimisation;video signal processing","VideoMec;metadata-enhanced crowdsourcing system;mobile videos;video crowdsourcing applications;user-generated annotations;video crowdsourcing system;automatic video description generation;geographical information;geometrical information;multiple embedded sensors;off-the-shelf mobile devices;bandwidth reduction;energy consumption reduction;mobile devices;video files;optimization problems;bandwidth constraints;time constraints","","","","40","","12 Jun 2017","","","IEEE","IEEE Conferences"
"Perception-based Filtering for Asymmetric Video Coding of 3D Video","N. Xu; X. Fang; W. Li; Y. An","Shanghai Jiao Tong University,Shanghai,China; Shanghai Jiao Tong University,Shanghai,China; Shanghai Jiao Tong University,Shanghai,China; Shanghai Jiao Tong University,Shanghai,China","2020 IEEE 2nd International Conference on Civil Aviation Safety and Information Technology (ICCASIT","9 Mar 2021","2020","","","667","669","Asymmetric video coding is an effective method to enhance the efficiency of encoding full-resolution stereo format 3D video. Related researches demonstrate that a suitable reduction on video quality of auxiliary view will not damage the entire perceptual video coding of 3D video, where main view is not changed. In paper, a perception-based filtering method is proposed for asymmetric video coding. Experiments demonstrate that it can save bit significantly rate without any perceptual video quality degradation.","","978-1-7281-9948-1","10.1109/ICCASIT50869.2020.9368844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9368844","asymmetric video coding;perceptulal video coding;3D video;low-pass filtering","Video coding;Visualization;Three-dimensional displays;Sensitivity;Filtering;Quality assessment;Video recording","data compression;filtering theory;stereo image processing;video coding;video signal processing","perception-based;asymmetric video coding;full-resolution stereo format 3D video;entire perceptual video coding;perceptual video quality degradation","","","","15","","9 Mar 2021","","","IEEE","IEEE Conferences"
"Encoding Shaky Videos by Integrating Efficient Video Stabilization","H. Huang; X. Wei; L. Zhang","Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","3 May 2019","2019","29","5","1503","1514","This paper presents a novel video coding method by integrating video stabilization for shaky videos. By reusing the stabilized motion of feature points and geometric transformations, a better predictor can be established to replace the original motion vectors in the motion estimation stage of video coding. Then, these motion vectors are optimized based on statistics of the residuals between the stabilization predictor and the standard one to improve the efficiency of motion search. As a result, our method brings much less computational cost to motion estimation than encoding the stabilized frames separately (e.g., 24% for the enhanced predictive zonal search algorithm, 17% for the unsymmetrical-cross multi-hexagon-grid search algorithm), while the Bjontegaard distortion (BD) bit rate and BDpeak signal-to-noise ratio still have the comparable performance with multiple quantization parameters. Specially, the implementation of our integrative system based on ×264 is very fast and of low latency, by which full HD videos can be simultaneously stabilized and encoded with more than 30 frames per second in the fastest mode, even on the mobile platform. The experiments on a variety of shaky videos demonstrate the potential of our method in terms of effectiveness and efficiency.","1558-2205","","10.1109/TCSVT.2018.2833476","National Natural Science Foundation of China(grant numbers:61772069,61425013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8355666","Video coding;video stabilization;motion vector","Videos;Video coding;Encoding;Motion estimation;Computational efficiency;Two dimensional displays;Three-dimensional displays","motion estimation;video coding","shaky videos;efficient video stabilization;video coding method;stabilized motion;original motion vectors;motion estimation stage;stabilization predictor;motion search;stabilized frames;enhanced predictive zonal search algorithm;unsymmetrical-cross multihexagon-grid search algorithm;integrative system;HD videos","","2","","34","","7 May 2018","","","IEEE","IEEE Journals"
"VideoPuzzle: Descriptive One-Shot Video Composition","Q. Chen; M. Wang; Z. Huang; Y. Hua; Z. Song; S. Yan","Hefei University of Technology, China; School of Computing, National University of Singapore; Panasonic Singapore Laboratories, Panasonic R&D Centre Singapore; Panasonic Singapore Laboratories, Panasonic R&D Centre Singapore; Hefei University of Technology, China; Hefei University of Technology, China","IEEE Transactions on Multimedia","13 Mar 2013","2013","15","3","521","534","A large amount of short, single-shot videos are created by personal camcorder every day, such as the small video clips in family albums, and thus a solution for presenting and managing these video clips is highly desired. From the perspective of professionalism and artistry, long-take/shot video, also termed one-shot video, is able to present events, persons or scenic spots in an informative manner. This paper presents a novel video composition system “Video Puzzle” which generates aesthetically enhanced long-shot videos from short video clips. Our task here is to automatically composite several related single shots into a virtual long-take video with spatial and temporal consistency. We propose a novel framework to compose descriptive long-take video with content-consistent shots retrieved from a video pool. For each video, frame-by-frame search is performed over the entire pool to find start-end content correspondences through a coarse-to-fine partial matching process. The content correspondence here is general and can refer to the matched regions or objects, such as human body and face. The content consistency of these correspondences enables us to design several shot transition schemes to seamlessly stitch one shot to another in a spatially and temporally consistent manner. The entire long-take video thus comprises several single shots with consistent contents and ίuent transitions. Meanwhile, with the generated matching graph of videos, the proposed system can also provide an efficient video browsing mode. Experiments are conducted on multiple video albums and the results demonstrate the effectiveness and the usefulness of the proposed scheme.","1941-0077","","10.1109/TMM.2012.2236306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392964","Image retrieval;one-shot video;video authoring;video transition","Visualization;Video sequences;Educational institutions;Video equipment;Materials;Cameras;Laboratories","graph theory;search problems;video signal processing","video puzzle;descriptive one-shot video composition;single-shot videos;events;persons;scenic spots;short video clips;virtual long-take video;spatial consistency;temporal consistency;content-consistent shots;video pool;frame-by-frame search;start-end content correspondences;coarse-to-fine partial matching process;human body;human face;shot transition schemes;generated matching graph;video browsing mode;multiple video albums","","9","1","47","","24 Dec 2012","","","IEEE","IEEE Journals"
"MVC based scalable codec enhancing frame-compatible stereoscopic video","Y. Chen; R. Zhang; M. Karczewicz","QCT Multimedia R&D and Standards, Qualcomm Inc., 5776 Morehouse Dr. San Diego, CA, 92121, USA; QCT Multimedia R&D and Standards, Qualcomm Inc., 5776 Morehouse Dr. San Diego, CA, 92121, USA; QCT Multimedia R&D and Standards, Qualcomm Inc., 5776 Morehouse Dr. San Diego, CA, 92121, USA","2011 IEEE International Conference on Multimedia and Expo","5 Sep 2011","2011","","","1","4","Existing 3D video solutions take advantages of the 2D video infrastructure, by coding two views of the stereoscopic video content in a frame-compatible manner, such as side-by-side or top-bottom. Content providers and service providers are using the existing 2D video authorizing tools and delivery infrastructure to enhance the 2D video experience into 3D with relatively small additional cost. However, such a frame-compatible solution typically sacrifices the quality of each view, by providing a half-resolution representation. In the future, it is expected that more and more devices are capable of rendering full-resolution 1080p stereoscopic video content. Co-existing of services based on half-resolution stereoscopic video, full-resolution stereoscopic video, as well as full-resolution 2D video is expected in the marketplace. In this paper, a scalable codec is proposed to support the decoding and rendering of frame-compatible stereo, full-resolution 2D and full-resolution stereo video representations. Compared to codecs providing similar functionalities, significant coding gain can be achieved.","1945-788X","978-1-61284-350-6","10.1109/ICME.2011.6012099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012099","3DTV;frame-compatible stereoscopic video;H.264/AVC;MVC: Multiview Video Coding;Scalable Video","Encoding;IEC;Stereo image processing","decoding;image enhancement;image representation;image resolution;stereo image processing;video codecs;video coding;visual perception","MVC based scalable codec;frame-compatible stereoscopic video enhancement;3D video solution;2D video infrastructure;2D video authorizing tool;full-resolution stereoscopic video content;half-resolution stereoscopic video;full-resolution stereo video representation","","2","","9","","5 Sep 2011","","","IEEE","IEEE Conferences"
"Enhanced Down/Up-Sampling-Based Video Coding Using the Residual Compensation","H. Cao; X. Liu; Y. Wang; Y. Li; W. Lei","Northeastern University,School of Computer Science and Engineering,Shenyang,China; Northeastern University,School of Computer Science and Engineering,Shenyang,China; Northeastern University,School of Computer Science and Engineering,Shenyang,China; Northeastern University,School of Computer Science and Engineering,Shenyang,China; Northeastern University,School of Computer Science and Engineering,Shenyang,China","2020 5th International Conference on Computer and Communication Systems (ICCCS)","16 Jun 2020","2020","","","286","290","Down/up-sampling-based video coding is an effective strategy to cope with limited bandwidth for transmission. In recent years, CNN-based super-resolution methods have also been integrated into the above coding strategy. However, it raises a problem of how to balance the calculation and reconstruction quality. In order to alleviate this problem, we propose an enhanced down/up-sampling-based video coding scheme, in which residual data is used to compensate the unrecovered details of the reconstructed video. We first down-sample the video prior to encoding at the encoder and then obtain the residual data by subtracting the super-resolution reconstructed video frame from the original video frame. The residual data is encoded and transmitted together with the low-resolution video sequence. The decoder first reconstructs the encoded low-resolution video with CNN-based super resolution method, and then uses residual data to compensate the missing details of the reconstructed video. To further enhance the perceived quality, we use a quality enhancement network to enhance the quality-compensated video. The experimental results show that the proposed scheme achieves superior quality improvement compared with HEVC anchor and the general down/up-sampling schemes.","","978-1-7281-6136-5","10.1109/ICCCS49078.2020.9118561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9118561","video coding;down/up-sampling;residual signal;compensation;Convolutional Neural Network (CNN)","Image reconstruction;Encoding;Image coding;Bit rate;Video sequences;Video coding","image reconstruction;image resolution;image sequences;video coding","residual compensation;CNN-based super-resolution methods;coding strategy;reconstruction quality;residual data;super-resolution reconstructed video frame;original video frame;low-resolution video sequence;quality enhancement network;quality-compensated video;enhanced down/up-sampling-based video coding","","","","18","","16 Jun 2020","","","IEEE","IEEE Conferences"
"Real time unusual event detection using video surveillance system for enhancing security","V. S. Rasmi; K. R. Vinothini","M.E Applied Electronics, A.V.C College of Engineering; ECE Dept, A.V.C College of Engineering","2015 Online International Conference on Green Engineering and Technologies (IC-GET)","19 Apr 2016","2015","","","1","4","In this paper the challenging problem of unusual event detection in video surveillance system is considered. In real time applications, tracking target in low resolution(LR) video is really a challenging task. The existing methods require high computational cost because it uses super resolution techniques for the enhancement of LR (low resolution) video. Proposed algorithm require low cost and is fast enough because it process low resolution frames and it is able to recognize the occurrence of uncommon events such as overcrowding and fight in the low resolution video without using any classifier and training datasets initially. The application of this proposed method is to enhance the ATM security.","","978-1-4673-9781-0","10.1109/GET.2015.7453788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7453788","video surveillance;Target Tracking;Unusual event detection;ATM security","Standards;Video surveillance;Morphological operations;Security;Event detection;Target tracking","security;video surveillance","ATM security;low resolution video;security;video surveillance system;real time unusual event detection","","6","","8","","19 Apr 2016","","","IEEE","IEEE Conferences"
"Enabling Adaptive High-Frame-Rate Video Streaming in Mobile Cloud Gaming Applications","J. Wu; C. Yuen; N. Cheung; J. Chen; C. W. Chen","Engineering Product Development Pillar, Singapore University of Technology and Design, Singapore; Engineering Product Development Pillar, Singapore University of Technology and Design, Singapore; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore, 487372; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Department of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY, USA","IEEE Transactions on Circuits and Systems for Video Technology","2 Dec 2015","2015","25","12","1988","2001","High-frame-rate (HFR) video is emerging in popular gaming applications to enhance the smooth experience perceived by end users. However, it is challenging to guarantee the delivery quality of HFR video in mobile cloud gaming scenarios because of the high transmission rate and limited wireless resources. To address this critical problem, we develop a novel transmission scheduling framework dubbed AdaPtive HFR vIdeo Streaming (APHIS). The term adaptive indicates this scheme's capability in dynamically adjusting the video traffic load and forward error correction (FEC) coding. First, we propose an online video frame selection algorithm to minimize the total distortion based on the network status, input video data, and delay constraint. Second, we introduce an unequal FEC coding scheme to provide differentiated protection for Intra (I) and Predicted (P) frames with low-latency cost. The proposed APHIS framework is able to appropriately filter video frames and adjust data protection levels to optimize the quality of HFR video streaming. We conduct extensive emulations in Exata involving HFR video encoded with H.264 codec. Experimental results show that APHIS outperforms the reference transmission schemes in terms of video peak signal-to-noise ratio, end-to-end delay, and goodput. Therefore, we recommend APHIS for delivering HFR video streaming in mobile cloud gaming systems.","1558-2205","","10.1109/TCSVT.2015.2441412","National Research Foundation, Prime Minister’s Office, Singapore, through the IDM Futures Funding Initiative administered by the Interactive and Digital Media Programme Office; US NSF(grant numbers:1405594); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7118144","High frame rate video;mobile cloud gaming;video frame selection;unequal error protection;forward error correction;Forward error correction (FEC);high-frame-rate (HFR) video;mobile cloud gaming;unequal error protection;video frame selection","Streaming media;Distortion;Video coding;Cloud computing;Forward error correction;Bandwidth;Packet loss","cloud computing;computer games;data protection;video coding;video streaming","adaptive high-frame-rate video streaming;mobile cloud gaming applications;HFR video quality;transmission rate;wireless resources;transmission scheduling framework;adaptive HFR vIdeo streaming;APHIS;video traffic load;forward error correction coding;online video frame selection algorithm;delay constraint;FEC coding scheme;differentiated protection;intra frames;predicted frames;low-latency cost;APHIS framework;video frames;data protection;HFR video streaming;Exata;HFR video encoding;H.264 codec;video peak signal-to-noise ratio;end-to-end delay;goodput","","54","2","60","","4 Jun 2015","","","IEEE","IEEE Journals"
"Enhanced hybrid quality prediction model for 8K UHD H.265 video using ANFIS","A. Dhanalakshmi; G. Nagarajan","Dept of EIE, Panimalar Engineering College, Faculty of Electronics Engineering, Sathyabama University, Chennai, India; Dept of CSE, Sathyabama University, Chennai, India","2017 4th International Conference on Advanced Computing and Communication Systems (ICACCS)","24 Aug 2017","2017","","","1","6","Enhanced video coding standard: H.265/ High Efficiency Video Coding (HEVC) elaborated by using SHVC reference software model (SHM), has an aim to develop performance above the current H.265 standard. This paper represents a Quality of Experience (QoE) and Quality of service (QoS) estimate model in a video streaming from the origin of network visual application which is developed in recent years and also 8K video streaming illustration based on the approaching standard of SHVC (Scalable High efficiency Video Coding). SHVC decoder is utilized below the monitor to decode moreover demonstrate in real time the expected SHVC layers. Conversely, ANFIS technique is carried out to interface the Quality of Experience and Quality of service and real time streaming is approved in an unfeasible model which entails an original video sequence. The fundamental objective of this paper is to represent a wireless domain considering 8K UHD H. 265-coded video of the hybrid estimate model of excellent quality. The main contribution of this paper are split up as the examination of the quality impact of the service parameter on the video transmission of 8K UHD H.265-coded and the objective based upon development of quality of visualization by QoS parameters using ANFIS (Adaptive neuro fuzzy interference system). The results showed that the prediction of accurateness is obtained better using an enhanced hybrid model in the series of 50 percentage bit rate reduction and Peak Signal to Noise Ratio (PSNR) for the same quality of video, reliability of QoE and 8KUHD video streaming using QoS.","","978-1-5090-4559-4","10.1109/ICACCS.2017.8014626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014626","H.265;SHVC;Quality of Experience (QoE);Quality of service (QoS);8K Ultra High Definition;ANFIS","Standards;Streaming media;Quality of service;Bit rate;Quality assessment;Video coding","decoding;fuzzy neural nets;fuzzy reasoning;high definition video;image sequences;quality of experience;quality of service;video coding;video streaming","enhanced hybrid quality prediction model;8K UHD H.265 video;enhanced video coding standard;H.265-high efficiency video coding;HEVC;SHVC reference software model;quality of experience;QoE;quality of service;QoS estimate model;network visual application;8K video streaming;scalable high efficiency video coding;SHVC decoder;ANFIS technique;real time streaming;video sequence;wireless domain;video quality;service parameter;video transmission;QoS parameters;adaptive neuro fuzzy inference system;bit rate reduction;peak signal to noise ratio;PSNR","","1","","30","","24 Aug 2017","","","IEEE","IEEE Conferences"
"Periodic Enhanced Frame Based Long-Short-Term Reference in HEVC for Conference and Surveillance Video Coding","J. Xiong; X. Long; R. Shi; M. Wang; Q. Zhou; G. Gui","National Engineering Research Center of Communications and Networking, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science, Nanjing University of Science and Technology, Nanjing, China; School of Information Engineering, Shenzhen University, Shenzhen, China; National Engineering Research Center of Communications and Networking, Nanjing University of Posts and Telecommunications, Nanjing, China; National Engineering Research Center of Communications and Networking, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Access","15 Apr 2019","2019","7","","46422","46433","The coding tools of the High Efficiency Video Coding (HEVC) standard were essentially designed for ordinary video contents. They have not fully exploited the long-term temporal dependence of surveillance videos and conference videos, in which the backgrounds are generally stationary. In this paper, a periodic-enhanced frame-based long-short-term reference scheme is presented for coding the videos with stationary backgrounds. First, periodic-enhanced frames are proposed based on global rate-distortion optimization, in which the background error propagation characteristic is introduced to reduce the long-term temporal redundancy. Second, two limitations, such as temporary occlusion and quantization error, are analyzed as applying the enhanced frame on a default HEVC hierarchical prediction structure. In order to reduce the limitations, a long-short-term reference scheme is presented based on the enhanced frames. In this scheme, the periodic-enhanced frames are employed as a long-term reference picture, and the adjacent frames are set as short-term reference pictures. The proposed method is verified by testing on the conference videos and surveillance videos.","2169-3536","","10.1109/ACCESS.2019.2909270","National Natural Science Foundation of China(grant numbers:61701258,61701310,61801219,61876093); Natural Science Foundation of Jiangsu Province(grant numbers:BK20170906,BK20181393); Natural Science Foundation of Jiangsu Higher Education Institutions(grant numbers:17KJB510044,17KJB510038); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8681544","Surveillance;background;HEVC;reference picture;enhanced frame","Encoding;Surveillance;Redundancy;High efficiency video coding;Conferences;Tools","video coding;video surveillance","ordinary video contents;long-term temporal dependence;surveillance videos;conference videos;stationary backgrounds;periodic-enhanced frames;background error propagation characteristic;long-term temporal redundancy;long-term reference picture;short-term reference pictures;coding tools;High Efficiency Video Coding standard;long-short-term reference scheme;HEVC hierarchical prediction structure","","1","","31","OAPA","4 Apr 2019","","","IEEE","IEEE Journals"
"Depth-based inter-view motion data prediction for HEVC-based multiview video coding","J. Konieczny; M. Domański","Chair of Multimedia Telecommunication and Microelectronics, Poznań University of Technology, Poznań, Poland; Chair of Multimedia Telecommunication and Microelectronics, Poznań University of Technology, Poznań, Poland","2012 Picture Coding Symposium","7 Jun 2012","2012","","","33","36","The paper deals with efficient coding of motion data for compression of multiview video with depth maps. This research was done in the context of the new compression technology called High Efficiency Video Coding. In the proposed approach, motion vectors and reference frame indices are predicted from reference view for each pixel using the Depth-Based Motion Prediction (DBMP). The new codec with DBMP compression tool was compared to the multiview HEVC-based codec i.e. a set of HEVC codecs augmented with inter-view texture prediction tools adopted from classic multiview codec (MVC) based on the AVC technology. Extensive experiments show the potential of the new DBMP predictor to reduce bitrate up to 12% against the multiview HEVC-based codec.","","978-1-4577-2049-9","10.1109/PCS.2012.6213279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6213279","Depth-Based Motion Prediction;Inter-View Direct mode;multiview video coding;depth-enhanced video;HEVC","Encoding;Codecs;Bit rate;Video coding;Prediction algorithms;Vectors;Three dimensional displays","codecs;data compression;video coding","depth-based inter-view motion data prediction;HEVC-based multiview video coding;multiview video compression;depth maps;high efficiency video coding;codec;AVC technology;DBMP compression tool","","3","2","10","","7 Jun 2012","","","IEEE","IEEE Conferences"
"ECast: An Enhanced Video Transmission Design for Wireless Multicast Systems Over Fading Channels","Z. Zhang; D. Liu; X. Ma; X. Wang","Beijing Key Laboratory of Network System Architecture and Convergence, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Network System Architecture and Convergence, Beijing University of Posts and Telecommunications, Beijing, China; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Stony Brook University, State University of New York, Stony Brook, NY, USA","IEEE Systems Journal","22 Nov 2017","2017","11","4","2566","2577","Conventional wireless video multicast schemes face a challenge due to their stair-shaped performance curves called “cliff effect” and the limited ability to accommodate multiple users with diverse channel conditions. Recently, a series of analog video coding approaches aiming to solve the problem have been proposed. In this paper, our goal is to enhance such schemes through a novel cross-layer design. A joint subcarrier matching and power-allocation scheme, which is proved to be optimal in terms of minimizing the mean square error, is proposed. The scheme can dynamically adapt to both channel conditions and video contents and, at the same time, maintain continuous quality scalability. Furthermore, the feedback overhead of the channel state information from multiple users is addressed, and a novel analog feedback method is proposed. The video qualities at various receivers are controllable through assigning different weights to different users. Therefore, this scheme is suitable for wireless video multicast systems. Simulation results validate the effectiveness of our proposal.","1937-9234","","10.1109/JSYST.2015.2438071","National Natural Science Foundation of China(grant numbers:61171107,61271257); National Science Foundation (NSF)(grant numbers:ECCS-1202286); National Science Foundation (NSF)(grant numbers:1247924.); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7147817","Analog feedback;analog video coding;power allocation;subcarrier matching;wireless video multicast","Wireless communication;Resource management;Fading;Receivers;Encoding;Video coding","fading channels;feedback;mean square error methods;multicast communication;optimisation;radio networks;video coding","enhanced video transmission design;wireless multicast systems;fading channels;stair-shaped performance curves;cliff effect;diverse channel conditions;novel cross-layer design;video contents;continuous quality scalability;channel state information;novel analog feedback method;video qualities;wireless video multicast systems;mean square error minimization;ECast;wireless video multicast schemes;analog video coding approach;joint subcarrier matching and power allocation scheme;feedback overhead","","8","","22","Traditional","2 Jul 2015","","","IEEE","IEEE Journals"
"An ICA Mixture Hidden Conditional Random Field Model for Video Event Classification","X. Wang; X. Zhang","Department of Electrical and Computer Engineering, Ryerson University, Toronto, Canada; Department of Electrical and Computer Engineering, Ryerson University, Toronto, Canada","IEEE Transactions on Circuits and Systems for Video Technology","9 Jan 2013","2013","23","1","46","59","In this paper, a hidden conditional random field (HCRF) model with independent component analysis (ICA) mixture feature functions is developed for video event classification. Video content analysis problems can be modeled using graphical models. The hidden Markov model (HMM) is a commonly used graphical model, but the HMM has several limitations such as the assumption of observation independence, the form of observation distribution and the Markov chain interaction. Unlike the HMM, the HCRF is a discriminative model without conditional independence assumption of observations, and is more suitable for video content analysis. We formulate the video content analysis problem using a new HCRF framework based on the temporal interactions between video frames. In addition, according to the non-Gaussian property of video event features, a new feature function using the likelihoods of ICA mixture components is proposed for local observation to further enhance the HCRF model. The discriminative power of the HCRF and representation power of the ICA mixture for non-Gaussian distributions are combined in the new model. The new model is applied to the challenging bowling and golf event classifications as case studies. The simulation results support the analysis that the new ICA mixture HCRF (ICAMHCRF) outperforms the existing mixture HMM models in terms of classification accuracy.","1558-2205","","10.1109/TCSVT.2012.2203195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6213103","Bowling video events;golf video events;graphical models;hockey video events;ICA mixture hidden conditional random field (ICAMHCRF);sport video events;video event classification","Hidden Markov models;Analytical models;Vectors;Graphical models;Training;Feature extraction;Labeling","Gaussian distribution;hidden Markov models;image classification;independent component analysis;sport;video signal processing","independent component analysis;ICA mixture feature functions;hidden conditional random field model;video event classification;video content analysis;graphical models;hidden Markov model;Markov chain interaction;observation distribution;video frames;nonGaussian distributions;classification accuracy;bowling video event;golf video event","","4","","36","","6 Jun 2012","","","IEEE","IEEE Journals"
"End User Video Quality Prediction and Coding Parameters Selection at the Encoder for Robust HEVC Video Transmission","G. Kulupana; D. S. Talagala; H. K. Arachchi; A. Fernando","CVSSP, University of Surrey, Guildford, U.K.; ARM Ltd., Leicester, U.K.; School of Science & Technology, Nottingham Trent University, Nottingham, U.K.; CVSSP, University of Surrey, Guildford, U.K.","IEEE Transactions on Circuits and Systems for Video Technology","30 Oct 2019","2019","29","11","3367","3381","Along with the rapid increase in the availability of high-quality video formats such as high definition (HD), ultra HD, and high dynamic range, a huge demand for data rates during their transmission has become inevitable. Consequently, the role of video compression techniques has become crucially important in the process of mitigating the data rate requirements. Even though the latest video codec high efficiency video coding (HEVC) has succeeded in significantly reducing the data rate compared to its immediate predecessor H.264/advanced video coding, the HEVC coded videos in the meantime have become even more vulnerable to network impairments. Therefore, it is equally important to assess the consumers' perceived quality degradation prior to transmitting HEVC coded videos over an error-prone network, and to include error resilient features so as to minimize the adverse effects of those impairments. To this end, this paper proposes a probabilistic model which accurately predicts the overall distortion of the decoded video at the encoder followed by an accurate QP-λ relationship which can be used in the rate-distortion optimization (RDO) process. During the derivation process of the probabilistic model, the impacts from the motion vectors, the pixels in the reference frames, and the clipping operations are accounted, and consequently, the model is capable of minimizing the prediction error to as low as 3.11%, whereas the state-of-the-art methods cannot reach below 20.08%, under identical conditions. Furthermore, the enhanced RDO process has resulted in 21.41%-43.59% improvement in the BD-rate compared to the state-of-the-art error resilient algorithms.","1558-2205","","10.1109/TCSVT.2018.2879956","EU H2020 Program through the CONTENT4ALL(grant numbers:762021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8529247","HEVC;error resilient video coding;AMVP;error concealment","Distortion;Encoding;Decoding;Video recording;Quality assessment;Video coding;Prediction algorithms","data compression;decoding;minimisation;probability;rate distortion theory;video codecs;video coding","BD-rate;end user video quality prediction;encoder;robust HEVC video transmission;high-quality video formats;high dynamic range;video compression techniques;data rate requirements;error-prone network;probabilistic model;video decoding;rate-distortion optimization process;prediction error minimisation;coding parameter selection;video codec high efficiency video coding;RDO process;error resilient algorithms;H.264-advanced video coding;QP-λ relationship","","2","","22","","9 Nov 2018","","","IEEE","IEEE Journals"
"Enhanced Intra Prediction with Recurrent Neural Network in Video Coding","Y. Hu; W. Yang; S. Xia; W. Cheng; J. Liu","Inst. of Comput. Sci. & Technol., Peking Univ., Beijing, China; Inst. of Comput. Sci. & Technol., Peking Univ., Beijing, China; Inst. of Comput. Sci. & Technol., Peking Univ., Beijing, China; Res. Center for Inf. Technol. Innovation, Taipei, Taiwan; Inst. of Comput. Sci. & Technol., Peking Univ., Beijing, China","2018 Data Compression Conference","23 Jul 2018","2018","","","413","413","Intra prediction is one of the important parts in video/image codec. With intra prediction mechanism, spatial redundancy can be largely removed for further bit saving. However, current state-of-the-art intra prediction method does not produce satisfactory prediction result due to its limits in reference samples and modeling ability. To enhance the intra prediction in HEVC, in this paper, a deep neural network featuring spatial RNN, which models the spatial dependency of pixels as sequential dynamics, is proposed to generate better prediction signals. Experimental results show improvement in BD-Rate for the proposed method compared with the original HEVC prediction scheme.","2375-0359","978-1-5386-4883-4","10.1109/DCC.2018.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8416630","Video Coding;HEVC;Intra Prediction;Recurrent Neural Network","Recurrent neural networks;Copper;Data compression;Video coding;Computer science;Information technology;Technological innovation","prediction theory;recurrent neural nets;video coding","enhanced intra prediction;recurrent neural network;video coding;video/image codec;intra prediction mechanism;spatial redundancy;deep neural network;spatial RNN;prediction signals;HEVC prediction scheme","","12","","0","","23 Jul 2018","","","IEEE","IEEE Conferences"
"High-Definition Video Compression System Based on Perception Guidance of Salient Information of a Convolutional Neural Network and HEVC Compression Domain","S. Zhu; C. Liu; Z. Xu","Department of Measurement Control and Information Technology, School of Instrumentation Science and Optoelectronics Engineering, Beihang University, Beijing, China; Department of Measurement Control and Information Technology, School of Instrumentation Science and Optoelectronics Engineering, Beihang University, Beijing, China; Department of Measurement Control and Information Technology, School of Instrumentation Science and Optoelectronics Engineering, Beihang University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","1 Jul 2020","2020","30","7","1946","1959","The new generation of the high-efficiency video coding (HEVC) video coding standard has improved compression performance and brought considerable coding complexity. Therefore, reducing the perception redundancy of video to obtain a better compression effect is the new direction of video development at present. In this paper, the HEVC is improved and enhanced from the aspects of a video saliency algorithm based on an attention mechanism and a video compression algorithm based on perception priority. In terms of video saliency, this paper proposes a spatial saliency algorithm based on a convolutional neural network and a temporal saliency algorithm based on motion vector. The saliency algorithm can combine the motion estimation results of each block during the HEVC compression on the basis of convolutional neural network and carry out adaptive dynamic fusion of the two, to complete the saliency map of the input video. In the aspect of the video compression algorithm with perception priority, this paper proposes a more flexible QP selection method, which selects its corresponding QP according to the saliency value of CU. At the same time, we propose a new rate-distortion optimization algorithm, which integrates the current block's saliency feature into the traditional rate-distortion calculation method, to guide the allocation of bits and achieve the purpose of perception priority. The experimental results proved the superiority of the proposed method over the state-of-the-art perceptual coding algorithms in terms of saliency detection and perceptual compression quality.","1558-2205","","10.1109/TCSVT.2019.2911396","National Natural Science Foundation of China (NSFC)(grant numbers:61375025,61075011,60675018); Scientific Research Foundation for the Returned Overseas Chinese Scholars from the State Education Ministry of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8692638","Spatiotemporal saliency;HD video;HEVC;rate-distortion optimization","Video compression;Visualization;Encoding;Databases;Standards;Saliency detection;High definition video","data compression;motion estimation;optimisation;video coding","high-definition video compression system;perception guidance;convolutional neural network;HEVC compression;high-efficiency video coding video coding standard;considerable coding complexity;compression effect;video development;video saliency algorithm;video compression algorithm;perception priority;spatial saliency algorithm;temporal saliency algorithm;saliency map;input video;rate-distortion optimization algorithm;current block;saliency detection;perceptual compression quality","","","","48","IEEE","16 Apr 2019","","","IEEE","IEEE Journals"
"UAV Video Processing for Traffic Surveillence with Enhanced Vehicle Detection","K. V. Najiya; M. Archana","Dept. of Electronics and Communication Engg., Cochin College of Engineering and Technology, Valanchery, India; Dept. of Electronics and Communication Engg., Cochin College of Engineering and Technology, Valanchery, India","2018 Second International Conference on Inventive Communication and Computational Technologies (ICICCT)","27 Sep 2018","2018","","","662","668","Remotely sensed data using aerial video is gaining vast popularity in traffic surveillance. Traditional monitoring devices are usually kept at fixed locations to achieve a fixed surveillance coverage range. Unmanned aerial vehicles (UAVs) are receiving much attention from researchers in traffic monitoring due to their low cost, high flexibility, and wide view range. Unlike stationary surveillance, the camera platform of UAVs is in constant motion and makes it difficult to process for data extraction. To address this problem, a novel framework for traffic flow parameter estimation with enhanced vehicle detection from aerial videos is proposed. The frames initially undergo Adaptive Gamma Correction for contrast enhancement. The proposed method incorporates steps that make use of the Kanade-Lucas-Tomasi (KLT) tracker, Support Vector Machine (SVM), and connected graphs. Interest-point-based motion analysis is done using KLT tracker. The SVM classification is used to identify vehicles from other moving objects. Contourlet transform coefficients and Gray Level Co-occurrence matrix (GLCM) are among the seven features extracted from the frames. Finally the number of vehicles, average speed and densities of bi-directional flow are found. The method shows high detection rate and low false positive alarms.","","978-1-5386-1974-2","10.1109/ICICCT.2018.8473204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8473204","Aerial video;traffic surveillence;traffic image analysis;unmanned aerial vehicle;traffic video processing;enhanced vehicle detection;Adaptive Gamma correction;SVM;Contoulet transform;GLCM","Support vector machines;Feature extraction;Roads;Surveillance;Vehicle detection;Cameras;Tracking","autonomous aerial vehicles;feature extraction;geophysical image processing;graph theory;image classification;image enhancement;image motion analysis;matrix algebra;object detection;object tracking;remote sensing;road traffic;road vehicles;support vector machines;traffic engineering computing;transforms;video cameras;video signal processing;video surveillance","UAV video processing;enhanced vehicle detection;remotely sensed data;aerial video;traffic surveillance;fixed surveillance coverage range;unmanned aerial vehicles;UAVs;traffic monitoring;camera platform;constant motion;data extraction;traffic flow parameter estimation;Adaptive Gamma Correction;contrast enhancement;Kanade-Lucas-Tomasi tracker;interest-point-based motion analysis;high detection rate;monitoring devices;Support Vector Machine;connected graphs;KLT tracker;SVM classification;Contourlet transform coefficients;Gray Level Co-occurrence matrix;feature extraction","","8","","34","","27 Sep 2018","","","IEEE","IEEE Conferences"
"An enhanced seam carving approach for video retargeting","T. Chao; J. Leou; H. Hsiao","Department of Computer Science and Information Engineering, National Chung Cheng University, Chiayi 621, Taiwan; Department of Computer Science and Information Engineering, National Chung Cheng University, Chiayi 621, Taiwan; Department of Computer Science and Information Engineering, National Chung Cheng University, Chiayi 621, Taiwan","Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference","17 Jan 2013","2012","","","1","5","Video retargeting (resizing) is an important task for displaying videos on various display devices. In this study, an enhanced seam carving approach for video retargeting is proposed, in which a seam may be a non-8-connected one. Both the search window size and the temporal weight can be adaptively adjusted according to video contents (motion information). Additionally, to preserve temporal coherence, the appearance-based method is employed. The spatial and temporal costs of a pixel are linearly combined to compute the cumulative cost with an adaptive temporal weight. Finally, dynamic programming is used to determine the optimal non-8-connected seam (with the minimum cumulative cost) for carving out. Based on the experimental results obtained in this study, the performance of the proposed approach is better than those of two comparison approaches.","","978-0-6157-0050-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6411981","","Streaming media;Coherence;Video sequences;Dynamic programming;Visualization;Search problems;Vectors","dynamic programming;video signal processing","enhanced seam carving;video retargeting;video resizing;display device;search window size;video contents;motion information;temporal coherence;appearance based method;adaptive temporal weight;dynamic programming","","","","13","","17 Jan 2013","","","IEEE","IEEE Conferences"
"Enhancing the performance of video compression on display devices using macro block classification and H.264 AVC","S. Saravanan","Maamallan Institute of Technology, Anna University (Affiliated), Chennai, India","2013 IEEE International Conference on Computational Intelligence and Computing Research","27 Jan 2014","2013","","","1","4","This paper presents a 30-frames/s H.264 intra encoder for digital video recorder or digital still camera applications. To achieve high throughput and low area cost for high-definition video, we apply the modified three-step fast intra prediction technique to reduce the cycle count while keeping the quality as close as full search. A macro block classification method is proposed for various video processing applications involving moving images. Based on the analysis of the Motion Vector field in the compressed video, we propose to classify Macro blocks of each video frame into different classes and use this class information to describe the frame content. In the proposed algorithm, macro block classification and H.264 results can work effectively on video processing applications.","","978-1-4799-1597-2","10.1109/ICCIC.2013.6724223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724223","Macro block;H.264 AVC;Motion estimation;Motion segmentation;Transform;Video compression","Image coding;Video compression;Video coding;Standards;PSNR;Transform coding;Streaming media","data compression;discrete cosine transforms;display devices;image classification;image segmentation;image sensors;motion estimation;video coding;video recording","performance enhancement;video compression;display devices;macro block classification method;H.264 AVC;H.264 intra encoder;digital video recorder;digital still camera applications;fast intra prediction technique;cycle count reduction;motion vector field analysis;video processing applications;motion segmentation;motion estimation;frame content;discrete cosine transform","","","","22","","27 Jan 2014","","","IEEE","IEEE Conferences"
"Temporal Feature Enhancing Network for Human Pose Estimation in Videos","H. Li; W. Yang; Q. Liao","Department of Electronics Engineering, Tsinghua University, China; Department of Electronics Engineering, Tsinghua University, China; Department of Electronics Engineering, Tsinghua University, China","2019 IEEE International Conference on Image Processing (ICIP)","26 Aug 2019","2019","","","579","583","Although state-of-the-art methods for human pose estimation have achieved superior results on the single image, their performance on videos usually deteriorates dramatically due to motion blur and occlusion. Since there is close temporal correlation among video frames, exploiting the contextual information properly can be helpful to tackle the problem. In this paper, we present a Temporal Feature Enhancing Network (TFEN) for video human pose estimation. It boosts the per-frame features by utilizing motion information in terms of optical flow and conducting temporal feature encoding by the convolution gated recurrent units (convGRU). It is an end-to-end learning framework and can extend any image based algorithm to video pose estimation. The experimental results validate the effectiveness of the proposed approach on two large-scale video pose estimation benchmarks.","2381-8549","978-1-5386-6249-6","10.1109/ICIP.2019.8803783","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803783","video pose estimation;feature enhancement;temporal encoding;feature warping","Videos;Pose estimation;Encoding;Feature extraction;Convolution;Optical imaging;Logic gates","feature extraction;image motion analysis;image sequences;learning (artificial intelligence);pose estimation;video signal processing","large-scale video pose estimation benchmarks;temporal feature enhancing network;single image;human pose estimation;estimation benchmarks;temporal feature encoding;optical flow;motion information;per-frame features;video human;contextual information;video frames;temporal correlation;motion blur","","","","20","","26 Aug 2019","","","IEEE","IEEE Conferences"
"Robust feature detection using particle keypoints and its application to video stabilization in a consumer handheld camera","S. Jeon; I. Yoon; S. Yang; B. Kim; J. Kim; J. Paik","Image Processing and Intelligent Systems Laboratory, Graduate School of Advanced Imaging Science, Multimedia, and Film, Chung-Ang University, Seoul, Korea; Image Processing and Intelligent Systems Laboratory, Graduate School of Advanced Imaging Science, Multimedia, and Film, Chung-Ang University, Seoul, Korea; Video Technology Lab., Future Technology R&D Division, SK Telecom; Video Technology Lab., Future Technology R&D Division, SK Telecom; Video Technology Lab., Future Technology R&D Division, SK Telecom; Image Processing and Intelligent Systems Laboratory, Graduate School of Advanced Imaging Science, Multimedia, and Film, Chung-Ang University, Seoul, Korea","2016 IEEE International Conference on Consumer Electronics (ICCE)","14 Mar 2016","2016","","","217","218","Compact, portable digital cameras have been popular to consumers. This paper presents a robust feature detection method using particle keypoints and its application to video stabilization. The proposed video stabilization algorithm consists of three steps: i) generation of a flat region map based on the Gaussian filter, ii) robust feature point detection using particle keypoints, and iii) camera paths estimation for enhancing a shaky video. As a result, the proposed algorithm can estimate optimal homography by redefining important feature points using particle keypoints in the flat region map. The proposed robust feature detection algorithm is suitable for enhancing the quality of video acquired by consumer handheld imaging devices.","2158-4001","978-1-4673-8364-6","10.1109/ICCE.2016.7430587","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7430587","","Conferences;Consumer electronics","cameras;feature extraction;Gaussian processes;image filtering;video signal processing","particle keypoint;consumer handheld camera;portable digital camera;video stabilization algorithm;flat region map;Gaussian filter;shaky video;optimal homography;camera path estimation;video quality;robust feature detection","","1","","4","","14 Mar 2016","","","IEEE","IEEE Conferences"
"Work in progress — Enhancing students learning through instructional videos during hands-on laboratories on renewable energy sources","O. Pantchenko; S. Shahab; D. Tate; P. Matteini; M. Isaacson; A. Shakouri","Baskin School of Engineering, University of California Santa Cruz; Baskin School of Engineering, University of California Santa Cruz; Baskin School of Engineering, University of California Santa Cruz; Baskin School of Engineering, University of California Santa Cruz; Baskin School of Engineering, University of California Santa Cruz; Baskin School of Engineering, University of California Santa Cruz","2011 Frontiers in Education Conference (FIE)","2 Feb 2012","2011","","","T2G-1","T2G-2","At the University of California Santa Cruz, a renewable energy sources course is a theory based course that includes six hands-on laboratory experiments. The course is designed for engineering and non-engineering undergraduate students and does not require any advanced mathematics or physics background. Each laboratory experiment introduces a miniature version of an energy conversion device that mimics the insights and workings of a real scale device. The hands-on laboratory experiments illustrate principles of the; solar pathfinder, flywheel, hydroelectricity, wind turbine, thermoelectricity and a fuel cell. In the past, each laboratory consisted of paper based instructions, pre and post questionnaires and a laboratory kit. Since many students in the class were non-science majors and had difficulty following the paper based instructions, we substituted the paper based instructions with instructional videos to ease the kit assembly and enhance student learning by providing more time to focus on the data gathering and analysis processes by minimizing the assembly time. The instructional videos demonstrate the experimental set-up and a method for collecting the data during each hands-on experiment. This work in progress paper presents description of our methods.","2377-634X","978-1-61284-469-5","10.1109/FIE.2011.6142717","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6142717","Hands-on laboratories;instructional videos;renewable energy sources","Laboratories;Videos;Renewable energy resources;Educational institutions;Assembly;Flywheels","computer aided instruction;educational courses;further education;interactive video;power engineering education;renewable energy sources;work in progress","work in progress;student learning;instructional video;renewable energy source;nonengineering undergraduate students;laboratory experiment;energy conversion device;real scale device;hands-on laboratory experiment;solar pathfinder;hydroelectricity;wind turbine;thermoelectricity;fuel cell;paper based instruction;kit assembly;data analysis","","4","","4","","2 Feb 2012","","","IEEE","IEEE Conferences"
"A Hybrid Stereoscopic Video Coding Scheme Based on MPEG-2 and HEVC for 3DTV Services","T. Na; S. Ahn; H. Sabirin; M. Kim; B. Kim; S. Hahm; K. Lee","Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong-gu, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong-gu, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong-gu, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong-gu, Daejeon, Korea; Technical Research Institute, Korean Broadcasting System, Yeongdeungpo-gu, Seoul, Korea; Technical Research Institute, Korean Broadcasting System, Yeongdeungpo-gu, Seoul, Korea; Technical Research Institute, Korean Broadcasting System, Yeongdeungpo-gu, Seoul, Korea","IEEE Transactions on Circuits and Systems for Video Technology","30 Aug 2013","2013","23","9","1542","1554","Recently, 3DTV has drawn much attention as a new broadcasting service. In spite of technical advances in 3DTV broadcasting services, there are two barriers that hinder the new services from being launched for terrestrial broadcasting: the lack of available bandwidth for transmission of additional-view video via terrestrial channel and backward compatibility with the legacy 2-D HDTV services. As an alternative, a hybrid stereoscopic video coding scheme is proposed for a stereoscopic 3DTV service, where one-view video is transmitted through the legacy broadcasting systems and the other one is delivered via broadband networks to which TV terminals are connected. In this paper, the proposed hybrid stereoscopic video coding scheme incorporates an MPEG-2 codec for backward compatibility with a legacy 2-D HDTV service and an HEVC codec with inter-view prediction coding as an extension to encode additional-view video sequences with high coding efficiency. The proposed inter-view prediction coding scheme in the extended HEVC incorporates an advanced motion and disparity vector prediction (AMDVP) method for enhanced motion- and disparity-compensated coding. The experimental results demonstrate that the proposed hybrid stereoscopic video coding scheme achieves an average coding efficiency of 38.22% (32.86%) in BD-rate or 1.394 dB (1.235 dB) in BD-PSNR for the out-band (in-band) scenario.","1558-2205","","10.1109/TCSVT.2013.2249021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6470666","3DTV;high efficiency video coding (HEVC);hybrid stereoscopic video codec;inter-view prediction coding;MPEG-2","Encoding;Transform coding;Video coding;Stereo image processing;Vectors;Broadcasting;Streaming media","broadband networks;high definition television;image motion analysis;image sequences;stereo image processing;telecommunication services;television broadcasting;three-dimensional television;vectors;video codecs;video coding;video communication;video streaming;visual perception","hybrid stereoscopic video coding scheme;3DTV broadcasting service;terrestrial channel broadcasting;2D HDTV service;stereoscopic 3DTV service;one-view video transmission;broadband network;MPEG-2 codec;HEVC codec;additional-view video sequences encoding;inter-view prediction coding scheme;advanced motion and disparity vector prediction method;AMDVP;enhanced motion-compensated coding;disparity-compensated coding;BD-PSNR;efficiency 38.22 percent;efficiency 32.86 percent;noise figure 1.394 dB;noise figure 1.235 dB","","7","","41","","25 Feb 2013","","","IEEE","IEEE Journals"
"Reduced Complexity Superresolution for Low-Bitrate Video Compression","G. Georgis; G. Lentaris; D. Reisis","Department of PhysicsElectronics Laboratory, National and Kapodistrian University of Athens, Athens, Greece; Department of PhysicsElectronics Laboratory, National and Kapodistrian University of Athens, Athens, Greece; Department of PhysicsElectronics Laboratory, National and Kapodistrian University of Athens, Athens, Greece","IEEE Transactions on Circuits and Systems for Video Technology","8 Feb 2016","2016","26","2","332","345","Evolving video applications impose requirements for high image quality, low bitrate, and/or small computational cost. This paper combines state-of-the-art coding and superresolution (SR) techniques to improve video compression both in terms of coding efficiency and complexity. The proposed approach improves a generic decimation-quantization compression scheme by introducing low complexity single-image SR techniques for rescaling the data at the decoder side and by jointly exploring/optimizing the downsampling/upsampling processes. The enhanced scheme achieves improvement of the quality and system's complexity compared with conventional codecs and can be easily modified to meet various diverse requirements, such as effectively supporting any off-the-shelf video codec, for instance H.264/Advanced Video Coding or High Efficiency Video Coding. Our approach builds on studying the generic scheme's parameterization with common rescaling techniques to achieve 2.4-dB peak signal-to-noise ratio (PSNR) quality improvement at low-bitrates compared with the conventional codecs and proposes a novel SR algorithm to advance the critical bitrate at the level of 10 Mb/s. The evaluation of the SR algorithm includes the comparison of its performance to other image rescaling solutions of the literature. The results show quality improvement by 5-dB PSNR over straightforward interpolation techniques and computational time reduction by three orders of magnitude when compared with the highly involved methods of the field. Therefore, our algorithm proves to be most suitable for use in reduced complexity downsampled compression schemes.","1558-2205","","10.1109/TCSVT.2015.2389431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005495","Super-resolution;video compression;singleimage;low-complexity codecs;high definition video;High-definition video;low-complexity codecs;single image;superresolution (SR);video compression","Encoding;Complexity theory;Bit rate;Codecs;PSNR;Decoding;Image coding","data compression;image resolution;interpolation;video codecs;video coding","reduced complexity superresolution;low-bitrate video compression;SR techniques;coding efficiency;coding complexity;image quality;generic decimation-quantization compression scheme;video codec;H.264-advanced video coding;high efficiency video coding;common rescaling techniques;signal-to-noise ratio quality improvement;PSNR;image rescaling solutions;interpolation techniques;computational time reduction;reduced complexity downsampled compression schemes","","19","","46","","9 Jan 2015","","","IEEE","IEEE Journals"
"Visual Summarization of Lecture Video Segments for Enhanced Navigation","M. R. Rahman; S. Shah; J. Subhlok","University of Houston,Department of Computer Science,Houston,TX,77204; University of Houston,Department of Computer Science,Houston,TX,77204; University of Houston,Department of Computer Science,Houston,TX,77204","2020 IEEE International Symposium on Multimedia (ISM)","22 Jan 2021","2020","","","154","157","The following topics are dealt with: video signal processing; video streaming; feature extraction; deep learning (artificial intelligence); video coding; learning (artificial intelligence); quality of experience; computer aided instruction; convolutional neural nets; image segmentation.","","978-1-7281-8697-9","10.1109/ISM.2020.00033","National Science Foundation(grant numbers:NSF-SBIR-1820045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327964","Lecture video summarization;Lecture video navigation;Visual summary;Algorithm generated summary;Automatic visual summarization","Visualization;Image segmentation;Navigation;Clustering algorithms;Particle measurements;Indexing;Feature extraction","deep learning (artificial intelligence);feature extraction;video coding;video streaming","video signal processing;video streaming;feature extraction;deep learning (artificial intelligence);video coding;learning (artificial intelligence);quality of experience;computer aided instruction;convolutional neural nets;image segmentation","","","","14","","22 Jan 2021","","","IEEE","IEEE Conferences"
"Adaptive loop filtering based interview video coding in an hybrid video codec with MPEG-2 and HEVC for stereosopic video coding","S. Ahn; M. Kim","Department of Information and Communications Engineering, Korea Advanced Institute of Science and Technology (KAIST), Korea; Department of Information and Communications Engineering, Korea Advanced Institute of Science and Technology (KAIST), Korea","IVMSP 2013","26 Sep 2013","2013","","","1","4","In this paper, a hybrid stereoscopic video codec is proposed based on MPEG-2 and an extended HEVC with an interview coding scheme for stereoscopic TV services through heterogeneous networks. The left-view sequences are encoded in an MPEG-2 video encoder for conventional 2D TV services via the traditional terrestrial broadcasting networks. On the other hand, the right-view sequences are encoded by an extended HEVC with a proposed interview coding scheme and the resulting bitstreams are transmitted over Internet. So, a 3D TV terminal to support the hybrid stereoscopic video streams receives the MPEG-2 data for the left-view sequences via the terrestrial broadcasting networks, and receives the right-view sequence streams of an extended HEVC data over Internet. The proposed interview coding scheme in an extended HEVC utilizes as reference frames the reconstructed MPEG-2 frames of the left-view sequences to perform predictive coding for the current frames of the right-view sequences. To enhance the texture qualities of the reference frames, an ALF tool is applied for the reconstructed MPEG-2 frames and HEVC as well. The ALF ON/OFF signaling map and ALF coefficients for the MPEG-2 reconstructed frames are transmitted in conjunction with HEVC bitstreams via Internet. The experimental results show that the proposed hybrid stereoscopic codec with ALF-based interview coding improves the coding efficiency with average 16.81% BD-rate gain, compared to a hybrid stereoscopic codec of independent MPEG-2 and HEVC codec without interview coding.","","978-1-4673-5858-3","10.1109/IVMSPW.2013.6611926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6611926","3DTV;HEVC;ALF;MPEG-2;stereoscopic video coding","Transform coding;Filtering;Encoding;Stereo image processing;Codecs;Interviews;Internet","adaptive filters;image reconstruction;image sequences;Internet;stereo image processing;television broadcasting;three-dimensional television;video codecs;video coding;video streaming","adaptive loop filtering-based interview video coding;stereosopic video coding;hybrid stereoscopic video codec;stereoscopic TV services;heterogeneous networks;left-view sequences;MPEG-2 video encoder;2D TV services;terrestrial broadcasting networks;3D TV terminal;hybrid stereoscopic video streams;terrestrial broadcasting networks;right-view sequence streams;HEVC data over Internet;MPEG-2 frame reconstruction;ALF tool;ALF on-off signaling map;HEVC bitstreams;coding efficiency;BD-rate gain","","","","6","","26 Sep 2013","","","IEEE","IEEE Conferences"
"Transform Coding Techniques in HEVC","T. Nguyen; P. Helle; M. Winken; B. Bross; D. Marpe; H. Schwarz; T. Wiegand","Image and Video Coding Group, Fraunhofer Institute for Telecommunications-Heinrich Hertz Institute, Berlin, Germany; Image and Video Coding Group, Fraunhofer Institute for Telecommunications-Heinrich Hertz Institute, Berlin, Germany; Image and Video Coding Group, Fraunhofer Institute for Telecommunications-Heinrich Hertz Institute, Berlin, Germany; Image and Video Coding Group, Fraunhofer Institute for Telecommunications-Heinrich Hertz Institute, Berlin, Germany; Image and Video Coding Group, Fraunhofer Institute for Telecommunications-Heinrich Hertz Institute, Berlin, Germany; Image and Video Coding Group, Fraunhofer Institute for Telecommunications-Heinrich Hertz Institute, Berlin, Germany; Department of Image Processing, Fraunhofer Institute for Telecommunications-Heinrich Hertz Institute, Berlin, Germany","IEEE Journal of Selected Topics in Signal Processing","19 Nov 2013","2013","7","6","978","989","High Efficiency Video Coding (HEVC) is the most recent jointly developed video coding standard of ITU-T Visual Coding Experts Group (VCEG) and ISO/IEC Moving Picture Experts Group (MPEG). Although its basic architecture is built along the conventional hybrid block-based approach of combining prediction with transform coding, HEVC includes a number of coding tools with greatly enhanced coding-efficiency capabilities relative to those of prior video coding standards. Among these tools are new transform coding techniques that include the support for dyadically increasing transform block sizes ranging from 4 × 4 to 32 × 32, the partitioning of residual blocks into variable block-size transforms by using a quadtree-based partitioning dubbed as residual quadtree (RQT) as well as some properly designed entropy coding techniques for quantized transform coefficients of variable transform block sizes. In this paper, we describe these HEVC techniques for transform coding with a particular focus on the RQT structure and the entropy coding stage and demonstrate their benefit in terms of improved coding efficiency by experimental results.","1941-0484","","10.1109/JSTSP.2013.2278071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578061","Entropy coding;H.265;HEVC;MPEG-H;residual quadtree;transform coding;video compression","Transforms;Transform coding;Video coding;Video compression;Entropy coding;Standards","entropy codes;quadtrees;video coding","transform coding techniques;high efficiency video coding;video coding standard;ITU-T visual coding experts group;ISO-IEC moving picture experts group;MPEG;hybrid block-based approach;transform coding;coding tools;enhanced coding-efficiency capabilities;video coding standards;transform block sizes;residual blocks partitioning;variable block-size transforms;quadtree-based partitioning;residual quadtree;entropy coding techniques;quantized transform coefficients;HEVC techniques;entropy coding stage;coding efficiency","","37","5","38","","15 Aug 2013","","","IEEE","IEEE Journals"
"Enhancing Temporal Quality Measurements in a Globally Deployed Streaming Video Quality Predictor","C. G. Bampis; Z. Li; A. C. Bovik","Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, USA; Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, USA; Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, USA","2018 25th IEEE International Conference on Image Processing (ICIP)","6 Sep 2018","2018","","","614","618","Most successful perceptual video quality assessment models are either frame-based, or perform spatiotemporal filtering or motion estimation to model the temporal aspects of video distortions. While good results are obtained on video quality databases, their increased computational complexity often causes video quality engineers to instead rely on simpler image-based quality algorithms. Towards balancing demands between prediction accuracy and compute efficiency, Netflix developed the Video Multi-method Assessment Fusion (VMAF) Framework, an efficient feature-based system that combines multiple perception-based elementary image measurements to produce video quality predictions. However, the current version of VMAF only weakly captures temporal video features which are sensitive to perceptual temporal video distortions. To this end, we propose an enhanced model we call SpatioTemporal VMAF (ST- VMAF) that incorporates temporal features that are easy to compute. We demonstrate the improved performance of ST- VMAF on many subjective video databases. The proposed model will be made available as part of the open source package in https://github.com/Netflix/vmaf.","2381-8549","978-1-4799-7061-2","10.1109/ICIP.2018.8451275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451275","full-reference video quality;feature-based perceptual video quality assessment;VMAF","Streaming media;Video recording;Quality assessment;Computational modeling;Feature extraction;Distortion measurement;Distortion","computational complexity;feature extraction;image sequences;motion estimation;spatiotemporal phenomena;video databases;video signal processing;video streaming","temporal quality measurements;successful perceptual video quality assessment models;frame-based;spatiotemporal filtering motion estimation;temporal aspects;video quality databases;increased computational complexity;video quality engineers;simpler image-based quality algorithms;efficient feature-based system;multiple perception-based elementary image measurements;video quality predictions;temporal video features;perceptual temporal video distortions;enhanced model;SpatioTemporal VMAF;ST- VMAF;temporal features;subjective video databases;video multimethod assessment fusion framework;globally deployed streaming video quality predictor","","2","","44","","6 Sep 2018","","","IEEE","IEEE Conferences"
"An enhanced video PCF mechanism for video PAN and its implementation based on HiSilicon Hi3518 embedded platform","L. Yong; F. Yunpeng","Key Laboratory of Industrial Internet of Things & Intelligent, Chongqing University of Posts and Telecommunications, Chongqing, China; Key Laboratory of Industrial Internet of Things & Intelligent, Chongqing University of Posts and Telecommunications, Chongqing, China","2017 Chinese Automation Congress (CAC)","1 Jan 2018","2017","","","5497","5502","Aiming at meeting the application requirements of low time delay and packet lose rate of wireless HD video PAN, an Video Point Coordination Function (VPCF) mechanism for MAC layer scheduling based on IEEE 802.11 PCF is proposed in this paper. TSP service periods are allocated to video station, which has burst video frames to transfer, and the number of needed CBAP (Slot Period Transmission), is evaluated using a preemption counter. AT video station, Time Delay Threshold and Cache Use Rate Threshold are used to decide its station priority. Finally AP station of VPAN allocates channel resources to each video station, according to its time delay weight, cache weight, and station priority. Simulating results indicate that VPCF mechanism can improve the real time transfer of high-definition video stream in VPAN network effectively. Based on an embedded video platform HiSilicon Hi3518, A 3-station VPAN prototype system is developed and VPCF is programmed and evaluated.","","978-1-5386-3524-7","10.1109/CAC.2017.8243760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8243760","Video PAN;PCF;H264;Hi3518compon","Streaming media;Delays;Delay effects;Monitoring;Quality of service;High definition video;Wireless communication","access protocols;video streaming;video surveillance;wireless LAN","time transfer;high-definition video stream;3-station VPAN prototype system;enhanced video PCF mechanism;HiSilicon Hi3518 embedded platform;wireless HD video PAN;Video Point Coordination Function mechanism;TSP service periods;Time Delay Threshold;station priority;time delay weight;VPCF mechanism;HiSilicon Hi3518 embedded video platform;MAC layer scheduling;IEEE 802.11 PCF;burst video frames","","","","10","","1 Jan 2018","","","IEEE","IEEE Conferences"
"Robust Video Frame Interpolation With Exceptional Motion Map","M. Park; H. G. Kim; S. Lee; Y. M. Ro","Image and Video Systems Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; Image and Video Systems Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; Image and Video Systems Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; Image and Video Systems Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea","IEEE Transactions on Circuits and Systems for Video Technology","3 Feb 2021","2021","31","2","754","764","Video frame interpolation has increasingly attracted attention in computer vision and video processing fields. When motion patterns in a video are complex, large and non-linear (exceptional motion), the generated intermediate frame is blurred and likely to have large artifacts. In this paper, we propose a novel video frame interpolation considering the exceptional motion patterns. The proposed video frame interpolation takes into account an exceptional motion map that contains the location and intensity of the exceptional motion. The proposed method consists of three parts, which are optical flow based frame interpolation, exceptional motion detection, and frame refinement. The optical flow based frame interpolation predicts an optical flow which is used to synthesize the pre-generated intermediate frame. The exceptional motion detection detects the position and intensity of complex and large motion with the current frame and the previous frame sequence. The frame refinement focuses on the exceptional motion region of the pre-generated intermediate frame by using the exceptional motion map. The proposed video frame interpolation can be robust against the exceptional motion including complex and large motion. Experimental results showed that the proposed video frame interpolation achieved high performance on various public video datasets and especially on videos with exceptional motion patterns.","1558-2205","","10.1109/TCSVT.2020.2981964","Institute for Information and Communications Technology Planning and Evaluation (IITP); Korean Government (MSIT) through the “Machine learning and statistical inference framework for explainable artificial intelligence”(grant numbers:2017-0-01779); Korean Government (MSIT) through the “Development of VR sickness reduction technique for enhanced sensitivity broadcasting”(grant numbers:2017-0-00780); Brain Korea 21 Plus Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9042268","Video frame interpolation;deep learning;exceptional motion estimation","Interpolation;Optical imaging;Streaming media;Motion detection;Nonlinear optics;Optical computing;Deep learning","computer vision;image motion analysis;image sequences;interpolation;motion estimation;video signal processing","optical flow based frame interpolation;exceptional motion detection;frame refinement;exceptional motion region;exceptional motion map;complex motion;exceptional motion patterns;robust video frame interpolation;pregenerated intermediate frame","","3","","47","IEEE","19 Mar 2020","","","IEEE","IEEE Journals"
"Enhancing Digital Educational Repositories by Linking Videos and Examinations","K. Mohan; A. R. Desai; M. P. Ashish; V. Kumar","Department of Computer Science and Engineering, PES Institute of Technology, Bangalore, India; Department of Computer Science and Engineering, PES Institute of Technology, Bangalore, India; Department of Computer Science and Engineering, PES Institute of Technology, Bangalore, India; Department of Computer Science and Engineering, PES Institute of Technology, Bangalore, India","2015 IEEE Seventh International Conference on Technology for Education (T4E)","4 Feb 2016","2015","","","45","48","The last decade has seen significant global efforts to develop and maintain digital educational repositories. Several countries, including India, have created national repositories of high-quality educational videos and related digital content. We have developed a simple open-source tool called ExamLink that allows instructors to enhance such repositories by creating bidirectional links between specific segments of educational videos and specific questions from old examination papers. Our tool exploits such links, allowing students to navigate large video repositories and locate content relevant to particular examination questions, and discover more questions pertaining to the same content. We demonstrate how our tool builds on existing efforts by the National Programme on Technology Enhanced Learning (NPTEL) to create links between question papers for the Graduate Aptitude Test in Engineering (GATE) and videos in the NPTEL archive.","","978-1-4673-9509-0","10.1109/T4E.2015.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395616","Digital Educational Repositories;NPTEL;GATE;personalized content navigation","Videos;Logic gates;Navigation;Computer science;Joining processes;Education;Indexes","computer aided instruction;content-based retrieval;interactive video;public domain software","GATE;Graduate Aptitude Test in Engineering;NPTEL;National Programme on Technology Enhanced Learning;examination questions;video repositories;educational videos;bidirectional links;ExamLink;open-source tool;digital content;high-quality educational videos;national repositories;India;digital educational repositories","","2","","11","","4 Feb 2016","","","IEEE","IEEE Conferences"
"Demonstrating an FPGA implementation of a full HD real-time HEVC decoder with memory optimizations for range extensions support","B. Stabernack; J. Möller; J. Hahlbeck; J. Brandenburg","Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Video Coding & Analytics Department, Embedded Systems Group, Einsteinufer 37, 10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Video Coding & Analytics Department, Embedded Systems Group, Einsteinufer 37, 10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Video Coding & Analytics Department, Embedded Systems Group, Einsteinufer 37, 10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Video Coding & Analytics Department, Embedded Systems Group, Einsteinufer 37, 10587 Berlin, Germany","2015 Conference on Design and Architectures for Signal and Image Processing (DASIP)","4 Jan 2016","2015","","","1","2","The novel High Efficiency Video Coding (HEVC) standard targets a broad set of different video formats ranging from QVGA up to Ultra-HD (4Kp60) resolutions. Especially the high spatial and temporal resolutions combined with the high algorithmic complexity makes implementing encoders and decoders a challenging task. Existing software based implementations on multi-core CPUs and/or DSPs suffer from real-time constraints, power dissipation and hardware costs of these systems. In this paper a hardware implementation of a Full HD capable H.265/HEVC video decoder is presented targeting these constraints. The demonstrated video decoder incorporates recent standard improvements, namely H.265/HEVC version 2, by supporting enhanced video format range extensions for high quality video applications.","","978-1-4673-7738-6","10.1109/DASIP.2015.7367247","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367247","","Decoding;Streaming media;Video coding;Encoding;Memory management;Pipelines;Field programmable gate arrays","field programmable gate arrays;video codecs;video coding","FPGA implementation;full HD real-time HEVC decoder;memory optimizations;high efficiency video coding standard;QVGA;ultraHD resolutions;high spatial resolutions;high temporal resolutions;high algorithmic complexity;full HD capable H.265/HEVC video decoder;enhanced video format range extensions;high quality video applications","","1","","5","","4 Jan 2016","","","IEEE","IEEE Conferences"
"Joint Carrier Matching and Power Allocation for Wireless Video with General Distortion Measure","Z. Zhang; D. Liu; X. Wang","Beijing, China; Beijing, China; Stony Brook, NY","IEEE Transactions on Mobile Computing","5 Feb 2018","2018","17","3","577","589","In this paper, we present a cross-layer design for a family of OFDM-based video communications by jointly considering application layer information and the wireless channel conditions. Compared with traditional cross-layer designs, our proposed method targets to efficiently transmit video data generated with some emerging techniques for better wireless transmissions, where the video data are divided into multiple chunks and each chunk contributes independent distortion to the entire video quality. To minimize the end-to-end distortion, we formulate a generalized optimization problem and derive a joint optimal carrier matching and power allocation scheme. Rather than depending on a specific video encoding method as done in the conventional work, we intend our design to be applicable to a general family of new video schemes. We apply our proposed method to two applications, the enhanced analog coding and the uncompressed video transmission over OFDM. In both applications, the performance can be improved by adopting our scheme. Simulation results validate the effectiveness of our approach in achieving significantly better PSNR and visual quality compared to reference schemes.","1558-0660","","10.1109/TMC.2016.2638844","China National 863 Project(grant numbers:2014AA01A705); NSFC(grant numbers:61271257,61171107); US National Science Foundation(grant numbers:CNS 1526843,1247924); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7981374","Analog video coding;carrier matching;cross-layer design;power allocation;uncompressed video transmission","Streaming media;Encoding;Wireless communication;Distortion;OFDM;Resource management;Video coding","data compression;distortion;OFDM modulation;optimisation;video coding;video communication;video streaming;wireless channels","wireless video;general distortion measure;video quality;joint optimal carrier matching and power allocation scheme;enhanced analog coding;PSNR;uncompressed video transmission;video schemes;specific video encoding method;generalized optimization problem;end-to-end distortion;independent distortion;multiple chunks;wireless transmissions;video data;wireless channel conditions;application layer information;OFDM-based video communications;cross-layer design","","5","","42","","14 Jul 2017","","","IEEE","IEEE Journals"
"Fast DST-VII/DCT-VIII With Dual Implementation Support for Versatile Video Coding","Z. Zhang; X. Zhao; X. Li; L. Li; Y. Luo; S. Liu; Z. Li","Department of Computer Science and Electrical Engineering, University of Missouri–Kansas City, Kansas, MO, USA; Tencent America, LLC., Palo Alto, CA, USA; Tencent America, LLC., Palo Alto, CA, USA; Department of Computer Science and Electrical Engineering, University of Missouri–Kansas City, Kansas, MO, USA; Tencent America, LLC., Palo Alto, CA, USA; Tencent America, LLC., Palo Alto, CA, USA; Department of Computer Science and Electrical Engineering, University of Missouri–Kansas City, Kansas, MO, USA","IEEE Transactions on Circuits and Systems for Video Technology","7 Jan 2021","2021","31","1","355","371","The Joint Video Exploration Team (JVET) recently launched the standardization of the next-generation video coding named Versatile Video Coding (VVC) with the inherited technical framework from its predecessor High-Efficiency Video Coding (HEVC). The simplified Enhanced Multiple Transform (EMT) has been adopted as the primary residual coding transform solution, termed Multiple Transform Selection (MTS). In MTS, only the transform set consisting of DST-VII and DCT-VIII remains, excluding the other transform sets and the dependency on intra prediction modes. Significant coding gains are achieved by introducing new DST/DCT transforms, but the full matrix implementation is relatively costly compared to partial butterfly in terms of both software run-time and operation counts. In this work, we exploit the inherent features existing in DST-VII and DCT-VIII. Instead of repeating the element-wise additions and multiplications in full matrix operation, these features can be leveraged to achieve more efficient implementations which only use partial elements to derive the identical results. Existing transform matrices are further tuned to utilize these (anti-)symmetric features. A partial butterfly-type fast algorithm with dual-implementation support is proposed for DST-VII/DCT-VIII transform in VVC. Complexity analysis including operation counts and software run-time are conducted to validate the effectiveness. In addition, we prove the features are perfectly supported by theory. The proposed fast methods achieve noticeable software run-time savings without compromising on coding performance by comparing with the VVC Test Model VTM-3.0. It is shown that under Common Test Condition (CTC) with inter MTS enabled, an average of 9%, 0%, and 3% decoding time savings are achieved for All Intra (AI), Random Access (RA) and Low Delay B (LDB), respectively. Under low QP test condition with inter MTS enabled, the proposed fast methods achieve 1%, 2% and 4% decoding time savings on average for AI, RA, and LDB, respectively.","1558-2205","","10.1109/TCSVT.2020.2977118","NSF I/UCRC(grant numbers:1747751); Tencent Media Lab; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9018012","Versatile video coding (VVC);fast DST-VII/DCT-VIII;multiple transform selection (MTS);Enhanced multiple transform (EMT);adaptive multiple transform(AMT);VVC test model (VTM)","Transforms;Video coding;Encoding;Software;Complexity theory;Artificial intelligence;Transform coding","discrete cosine transforms;matrix algebra;video coding","DCT-VIII;Common Test Condition;primary residual coding transform solution;decoding time savings;VVC Test Model VTM-3;coding performance;noticeable software run-time savings;complexity analysis;dual-implementation support;partial butterfly-type fast algorithm;transform matrices;partial elements;matrix operation;element-wise additions;matrix implementation;significant coding gains;transform sets;termed Multiple Transform Selection;simplified Enhanced Multiple Transform;High-Efficiency Video Coding;inherited technical framework;next-generation video coding;Joint Video Exploration Team;versatile video coding;dual implementation support","","1","","39","IEEE","28 Feb 2020","","","IEEE","IEEE Journals"
"Enhanced Eulerian video magnification","L. Liu; L. Lu; J. Luo; J. Zhang; X. Chen","School of Digital Media, Jiangnan University, Wuxi, China; School of Digital Media, Jiangnan University, Wuxi, China; School of Digital Media, Jiangnan University, Wuxi, China; School of Digital Media, Jiangnan University, Wuxi, China; School of Digital Media, Jiangnan University, Wuxi, China","2014 7th International Congress on Image and Signal Processing","8 Jan 2015","2014","","","50","54","A post-processing technique is introduced to improve the Eulerian video magnification method, which is a state-of-the-art motion magnification method to manipulate small movements in videos based on spatio-temporal filtering. The proposed method use the Eulerian video magnification as a video spatio-temporal motion analyzer to get the pixel-level motion mapping. Then the input video pixels are warped based-on this mapping to amplify the motion. This processing does not involve pixel value modifying, which makes it supports larger amplification and is significantly less influenced by the frame noise.","","978-1-4799-5835-1","10.1109/CISP.2014.7003748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7003748","Motion magnification;Eulerian motion;Spatio-temporal analysis;Image warping;Video-based rendering","Noise;Band-pass filters;Educational institutions;Video sequences;Noise measurement","image enhancement;image filtering;image motion analysis;video signal processing","Eulerian video magnification enhancement method;post-processing technique;motion magnification method;movement manipulation;spatio-temporal filtering;video spatio-temporal motion analyzer;pixel-level motion mapping;video pixel warping;frame noise","","8","","10","","8 Jan 2015","","","IEEE","IEEE Conferences"
"Enhanced Anisotropic Diffusion-based CNN-LSTM Architecture for Video Face Liveness Detection","R. Koshy; A. Mahmood","University of Bridgeport,Computer Science and Engineering,Bridgeport,USA; University of Bridgeport,Computer Science and Engineering,Bridgeport,USA","2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA)","23 Feb 2021","2020","","","422","425","While considerable research has been recently done in improving the accuracy of face liveness detection, the best current approaches use a two-step process of first applying non-linear anisotropic diffusion to the incoming image and then using a deep network for final liveness decision. We develop a novel deep architecture for face liveness detection on video frames that uses the diffusion of images followed by a deep Convolutional Neural Network (CNN) and a Long Short-Term Memory (LSTM) to classify the video sequence as real or fake. Even though the use of CNN followed by LSTM is not new, combining it with diffusion (that has proven to be the best approach for single image liveness detection) is novel. Performance evaluation of our architecture on the REPLAY-ATTACK dataset yields competitive results with 98.64% accuracy and an HTER of 3.63%.","","978-1-7281-8470-8","10.1109/ICMLA51294.2020.00074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9356181","face liveness detection;diffusion;CNN-LSTM;Replay-Attack dataset","Performance evaluation;Image edge detection;Video sequences;Streaming media;Feature extraction;Surface texture;Faces","convolutional neural nets;deep learning (artificial intelligence);face recognition;image classification;image sequences;neural nets;recurrent neural nets;video signal processing","nonlinear anisotropic diffusion;video face liveness detection;enhanced anisotropic diffusion-based CNN-LSTM architecture;single image liveness detection;video sequence;Long Short-Term Memory;deep Convolutional Neural Network;video frames;deep architecture;final liveness decision;deep network;incoming image","","","","20","","23 Feb 2021","","","IEEE","IEEE Conferences"
"Enhanced H.264/AVC video streaming using network-adaptive multiple description coding","P. Correia; P. Assuncao; V. Silva","Institute of Telecommunications Polytechnic Institute of Tomar/ES Portugal; Institute of Telecommunications, Polytechnic Institute of Leiria/ESTG, Portugal; Institute of Telecommunications, University of Coimbra/DEEC, Portugal","2011 IEEE EUROCON - International Conference on Computer as a Tool","23 Jun 2011","2011","","","1","4","This paper presents a network-adaptive multiple description coding (MDC) method for enhanced video streaming over multipath channels. The novel feature of the proposed MDC network adaptation scheme is the generation of a controlled amount of side information to compensate for drift distortion due to packet loss. This yields higher robustness of multiple description (MD) H.264/AVC streams than single description (SD) ones. Multiple Description Scalar Quantisation (MDSQ) is used to split an SD video stream in two complementary streams (two descriptions) for transmission over a multipath channel. The simulation results show that significantly better quality is achieved by using the proposed scheme instead of either SDC streams or classic SDC-MDSQ network adaptation without side information.","","978-1-4244-7487-5","10.1109/EUROCON.2011.5929296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929296","Network-adaptive Multiple Description Video Coding;Multiple Description Scalar Quantisation;video streaming","Streaming media;Decoding;PSNR;Indexes;Encoding;Propagation losses;Quantization","multipath channels;quantisation (signal);video coding;video streaming","enhanced H.264/AVC video streaming;network-adaptive multiple description coding;multipath channels;multiple description scalar quantisation","","","","8","","23 Jun 2011","","","IEEE","IEEE Conferences"
"Video segmentation based on patch matching and enhanced Onecut","Yingchun Yang; Yuchen Peng; Shoudong Han","National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Automation, HUST, Wuhan, China; National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Automation, HUST, Wuhan, China; Research Institute of Huazhong University of Science and Technology in Shenzhen, China","2017 2nd International Conference on Image, Vision and Computing (ICIVC)","20 Jul 2017","2017","","","346","350","Video segmentation has been widely applied in many fields, such as motion identification, target tracking, video retrieval and video editing. We have proposed an approach of video segmentation, which combines the color feature, shape feature and motion information of the target together. Firstly, we introduce tiny amount of interactions into the processing of key frame, obtain the accurate contour, and then initialize the local classifiers. Secondly, we use the patch-based sparse matching, which's also called patch matching in the following context, to pass the contour of the last frame to the current frame, as a result, initial contour of the target is got estimated. Simultaneously, the position parameters are updated. Eventually, we calculate the foreground and background probability distributions of current frame through the global probability models and local classifiers, then construct the enhanced Onecut model to obtain its segmentation results. Compared with the state-of-art video segmentation methods, our proposed approach performs outstandingly on the DAVIS dataset.","","978-1-5090-6238-6","10.1109/ICIVC.2017.7984575","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7984575","video segmentation;local classifier;patch matching;enhanced onecut","Image edge detection;Color;Shape;Optical imaging;Optical noise;Optical sensors;Image color analysis","image matching;image motion analysis;image segmentation;statistical distributions;video retrieval;video signal processing","video segmentation;patch matching;motion identification;target tracking;video retrieval;video editing;color feature;shape feature;motion information;patch-based sparse matching;foreground probability distributions;background probability distributions;global probability models;enhanced Onecut model;DAVIS dataset","","","","13","","20 Jul 2017","","","IEEE","IEEE Conferences"
"Enhancing Semantic Video Adaptation Speed through Compressed Domain Adaptation","G. Abebe; D. Coquil; H. Kosch","Dept. of Distrib. Inf. Syst., Univ. of Passau, Passau, Germany; Dept. of Distrib. Inf. Syst., Univ. of Passau, Passau, Germany; Dept. of Distrib. Inf. Syst., Univ. of Passau, Passau, Germany","2011 Workshop on Multimedia on the Web","12 Mar 2012","2011","","","43","44","The delivery of videos to the increasing number of heterogeneous mobile devices over different types of networks necessitates video personalization. This task should be based on device and network capabilities as well as semantics of the video as perceived by the user. Personalization of videos based on semantic user preferences can be computationally expensive and affect the speed of video delivery. In this position paper, we present compressed domain semantic video adaptation as a means to reduce the computational cost of video personalization.","","978-0-7695-4586-8","10.1109/MMWeb.2011.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6167829","video adaptation;semantic adaptation;compressed domain adaptation;gBSD","Semantics;Multimedia communication;Streaming media;Transform coding;XML;Multimedia computing;Computational efficiency","video coding","semantic video adaptation speed enhancement;video delivery;heterogeneous mobile devices;video personalization;semantic user preferences;computational cost reduction;compressed domain semantic video adaptation","","1","","5","","12 Mar 2012","","","IEEE","IEEE Conferences"
"Low-complexity feedback-channel-free distributed video coding with enhanced classifier","Y. Wang; S. Hsu; T. Cheng; C. Lee; S. Chien","Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","2013 IEEE International Symposium on Circuits and Systems (ISCAS)","1 Aug 2013","2013","","","257","260","Distributed video coding (DVC) is an emerging video coding paradigm due to its flexibility to introduce much lower encoding complexity than conventional predictive codecs, which is beneficial for some applications such as wireless video surveillance, wireless sensor networks, and disposable video cameras. Although DVC systems without feedback channel address a wider range of applications, it is not commonly discussed in literatures due to its lower coding performance. In this paper, on the basis of PRISM DVC architecture, a low-complexity feedback-channel-free DVC system is proposed with a new classifier to improve the coding performance. Experimental results show that the proposed system can provide about 0.8 dB gain in PSNR to the state-of-the-art and 2 dB gain to IST-PRISM. It is also competitive regarding other feedback-channel-free DVC systems and can even achieve similar performance to DVC systems with feedback channel for low-motion sequences.","2158-1525","978-1-4673-5762-3","10.1109/ISCAS.2013.6571831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571831","Distributed video coding;Wyner-Ziv (WZ) Video Coding;PRISM;feedback-channel-free","Decoding;Encoding;Video coding;Complexity theory;Codecs;Transforms;Support vector machine classification","video coding","low-complexity feedback-channel-free distributed video coding;classifier enhancement;encoding complexity;PRISM DVC architecture;PSNR;IST-PRISM;low-motion sequence","","","","9","","1 Aug 2013","","","IEEE","IEEE Conferences"
"Enhanced Short Video Understanding by Integrating User Behavior and Multimedia Content Information","L. Zhu","Ctrip Travel Network Technology Co., Limited","2019 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","15 Aug 2019","2019","","","657","660","The focus of ICME 2019 Grand Challenge is short video understanding and recommendation system based on user-video interaction data and multi-modal video features, including visual, text, and audio features. This paper provides the solution of our team hanhan to track 2 of this challenge. We cast this problem as a binary classification problem and addressed it by careful feature engineering and gradient boosted decision trees. To fully exploit the implicit feedback and multi-model content information, we created truncated SVD-based and neural-net-based embedding features for users, videos and authors. Furthermore, ensemble of a collection of models that take into consideration the cold-start and imbalanced nature of the recommendation task can further significantly improve upon the best single model. By using the proposed approach, our team was able to attain the 1st place in track 2 of the competition.","","978-1-5386-9214-1","10.1109/ICMEW.2019.00127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8795024","Recommender systems, multimedia, short videos, audio features, imbalanced learning","","decision trees;learning (artificial intelligence);neural nets;recommender systems;singular value decomposition;video signal processing","SVD-based;embedding features;recommendation task;enhanced short video understanding;user behavior;multimedia content information;ICME 2019 Grand Challenge;recommendation system;user-video interaction data;multimodal video features;visual text;audio features;binary classification problem;gradient boosted decision trees;implicit feedback;feature engineering;cold-start","","","","9","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Enhanced Backward Error Concealment for H.264/AVC Videos on Error-Prone Networks","P. Wang; C. Lin","Dept. of Comput. Sci. & Inf. Eng., Nat. Univ. of Tainan, Tainan, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Univ. of Tainan, Tainan, Taiwan","2013 International Symposium on Biometrics and Security Technologies","16 Sep 2013","2013","","","62","66","Transmitting compressed video data is quite common over wireless and wired networks. In error-prone networks, however, packet loss can lead to incorrect decoding. With the new generation standard, H.264/AVC, such issues are prevalent and drastically degrade the quality of decoded video. To solve these problems, error concealment is applied in the decoder. However, traditional error concealment does not give satisfactory results in all cases, and can result in whole frame loss. This issue is addressed by backward error concealment that conceals corrupted frames by using succeeding frames that have been correctly received. This backward error concealment efficiently conceals most of the corrupted pixels, however, it excludes the unreferenced pixels. In this paper, we propose an enhanced backward error concealment method. Based on the continuity of moving objects, the proposed method provides estimated motion vectors to conceal unreferenced pixels. Experimental results show that the proposed method achieves better performance in terms of distortions.","","978-1-4673-5314-4","10.1109/ISBAST.2013.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597667","Video communication;Backward decoding;Frame loss;Error concealment;H.264/AVC","Vectors;Videos;Decoding;Video coding;Extrapolation;Resilience;Propagation losses","data compression;decoding;motion estimation;vectors;video coding;video communication","H.264/AVC videos;error-prone networks;compressed video data;wireless networks;wired networks;packet loss;incorrect decoding;new generation standard;decoded video quality;traditional error concealment does;whole frame loss;corrupted frames;corrupted pixels;unreferenced pixels;enhanced backward error concealment method;moving objects;estimated motion vectors","","","","18","","16 Sep 2013","","","IEEE","IEEE Conferences"
"A case study of bitstream syntax reconfiguration for MPEG Internet video coding","C. J. Yeon; P. Sang-hyo; E. S. Jang","Department of Computer & Software Hanyang University, Seoul, Korea; Department of Computer & Software Hanyang University, Seoul, Korea; Department of Computer & Software Hanyang University, Seoul, Korea","2014 4th IEEE International Conference on Network Infrastructure and Digital Content","5 Jan 2015","2014","","","103","106","In this paper, we propose a bitstream syntax reconfiguration approach to improve coding efficiency of video codec. The video compression within a codec is conducted by several compression tools, and the usage of the tools is dependent on how bitstream syntax is composed. Accordingly, proper modification on the bitstream syntax may leads to the improved usage of compression tools, resulting in the enhanced compression efficiency. We studied this assumption through reconfiguration of MPEG Internet video coding (IVC) bitstream syntax. We allowed the use of intra prediction during the inter-frame compression process. Experimental results show that the proposed method can increase the coding efficiency up to 2.1%.","2374-0272","978-1-4799-4734-8","10.1109/ICNIDC.2014.7000274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000274","bitstream syntax reconfiguration;IVC;intra prediction;video compression","Syntactics;Encoding;Transform coding;Internet;Video coding;Codecs;ISO standards","data compression;Internet;video codecs;video coding","bitstream syntax reconfiguration;MPEG Internet video coding;video codec;video compression;compression tools;enhanced compression efficiency","","","","7","","5 Jan 2015","","","IEEE","IEEE Conferences"
"Multi-Access Edge Computing Enhanced Video Streaming: Proof-of-Concept Implementation and Prediction/QoE Models","S. Yang; Y. Tseng; C. Huang; W. Lin","Department of Computer Science, the Institute of Information Systems and Applications, and the Institute of Communications Engineering, National Tsing Hua University, Hsinchu, Taiwan; High Tech Computer Corporation, New Taipei City, Taiwan; Institute of Information Systems and Applications, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan","IEEE Transactions on Vehicular Technology","12 Feb 2019","2019","68","2","1888","1902","ETSI multi-access edge computing (MEC) provides an IT service environment and cloud-computing capabilities at the edge of the mobile network, enabling application and content providers to deploy new use cases, such as intelligent video acceleration, with low latency and high bandwidth. Specifically, ETSI MEC introduces an MEC server that implements the edge-cloud platform to host partial server-side service logics in the form of MEC applications (MEC Apps). In this paper, we aim to implement the first proof-of-concept (PoC) in the literature for the MEC-enhanced mobile video streaming service. Our PoC consists of Android User Apps, an MEC App, and the YouTube server. The MEC App implements two main functions: popular video caching and radio analytics/video quality adaptation. The User App provides general functions of a YouTube video streaming app and can access the videos from the cache server or the YouTube server under the MEC server's guidance. In addition to the PoC implementation, this paper further develops two machine learning models to be incorporated into the MEC App for popular video prediction and radio channel quality prediction, which allows to consider the effect of non-negligible round-trip times and adjust the video quality more accurately. The experimental results justify that our models, together with other advantages from MEC, can guarantee good performance for the mobile video streaming service. Finally, we model and investigate the effectiveness of the MEC architecture for improving the quality of experience of video-streaming users.","1939-9359","","10.1109/TVT.2018.2889196","Ministry of Science and Technology, Taiwan(grant numbers:MOST-106-2923-E-002-005-MY3,MOST-106-2221-E-007-013-,MOST-107-2221-E-007-034-); Ministry of Education Higher Education Sprout Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8585081","Machine learning;multi-access edge computing (MEC);proof-of-concept;quality of experience (QoE);video quality adaptation;video streaming","Streaming media;Servers;Video recording;Quality assessment;Predictive models;Adaptation models;YouTube","cache storage;cloud computing;learning (artificial intelligence);mobile computing;quality of experience;video signal processing;video streaming","video caching;video prediction;Android user apps;QoE models;IT service environment;cloud-computing;machine learning models;quality of experience;MEC architecture;radio channel quality prediction;PoC implementation;cache server;YouTube video;radio analytics/video quality adaptation;YouTube server;MEC-enhanced mobile video streaming service;MEC applications;host partial server-side service logics;edge-cloud platform;MEC server;ETSI MEC;intelligent video acceleration;content providers;cloud-computing capabilities;ETSI multiaccess edge computing;proof-of-concept implementation;multiaccess edge computing enhanced video streaming","","16","","33","","21 Dec 2018","","","IEEE","IEEE Journals"
"Video-Quality-Driven Resource Allocation for Real-Time Surveillance Video Uplinking Over OFDMA-Based Wireless Networks","P. Wu; C. Huang; J. Hwang; J. Pyun; J. Zhang","USA Lab., Bellevue, WA, USA; Dept. of Commun. Eng., Nat. Central Univ., Jhongli, Taiwan; Dept. of Electr. Eng., Univ. of Washington, Seattle, WA, USA; Dept. of Inf. Commun. Eng., Chosun Univ., Gwangju, South Korea; Coll. of Electron. & Electr. Eng., Shanghai Univ. of Eng. Sci., Shanghai, China","IEEE Transactions on Vehicular Technology","14 Jul 2015","2015","64","7","3233","3246","This paper proposes an effective real-time video uplink (UL) framework for mobile wireless camera networks (WCN) over an orthogonal frequency-division multiple access (OFDMA), based infrastructure. Mobile wireless camera stations (CSs) transmit their videos in real time to a base station (BS) so that these live video streams can be archived or fed to subscribers to facilitate real-time video monitoring. Based on the utility function driven by video quality, the target bit rate resulting in the highest possible utility is quickly set for each UL video. To optimize system performance, a real-time video packet scheduler and a spectral-efficient resource-allocation policy are derived. This scheduler is also capable of exploiting the inherent diversity gain due to channel variations. Using fourth-generation (4G) mobile network protocols and a realistic wireless channel model, it is demonstrated through our extensive simulations that our proposed method can significantly enhance utility, boost spectral efficiency, and stabilize video quality.","1939-9359","","10.1109/TVT.2014.2350002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6880864","Cross-layer design;Long-Term Evolution (LTE);mobile surveillance system;video quality driven (VQD);video uplink (UL);Worldwide Interoperability for Microwave Access (WiMAX)","Streaming media;Bit rate;Real-time systems;Cascading style sheets;Wireless communication;Video recording;Quality assessment","4G mobile communication;frequency division multiple access;mobile radio;OFDM modulation;spectral analysis;telecommunication scheduling;video cameras;video streaming;video surveillance;wireless channels;wireless sensor networks","video quality driven resource allocation;real-time surveillance video uplinking;OFDMA-based wireless network;UL framework;mobile wireless camera network;WCN;orthogonal frequency division multiple access;mobile wireless camera station;CS;base station;BS;live video streaming;real-time video monitoring;utility function;real-time video packet scheduler;spectral-efficient resource allocation policy;fourth-generation mobile network protocol;4G mobile network protocol;wireless channel model;spectral efficiency","","7","1","40","","20 Aug 2014","","","IEEE","IEEE Journals"
"Layered screen video coding leveraging hardware video codec","Dan Miao; J. Fu; Y. Lu; S. Li; Chang Wen Chen","University of Science and Technology of China, Hefei, China; Media Computing Group, Microsoft Research Asia, Beijing, China; Media Computing Group, Microsoft Research Asia, Beijing, China; Media Computing Group, Microsoft Research Asia, Beijing, China; State University of New York at Buffalo, USA","2013 IEEE International Conference on Multimedia and Expo (ICME)","26 Sep 2013","2013","","","1","6","In this paper, we propose a layered screen video coding scheme based on existing video codecs to leverage hardware video codec for efficient screen video compression. In this scheme, the screen video compression is performed as two-layer coding: base layer coding and enhancement layer coding. The screen video is first analyzed in both frame and block levels for useful temporal and spatial information extraction to assist coding content selection in each layer. The non-skip screen frames are directly compressed by the conventional video codec in the base layer, while the screen contents sensitive to the video quality degradation are selected for improved coding in the enhancement layer. For contents to be enhanced, two intra coding modes are designed to improve the quality of the compressed text/graphics contents and suppress the artifacts introduced by chroma downsampling. The experimental results demonstrate that the screen video quality is improved objectively and subjectively by the proposed scheme with low cost on bitrate and computation complexity. Moreover, an average of 2.95dB coding gain is achieved in high bitrate.","1945-788X","978-1-4799-0015-2","10.1109/ICME.2013.6607515","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607515","Screen video compression;content analysis;layered video coding","Encoding;Graphics;Video codecs;Image coding;Transforms;Video recording;Quality assessment","computational complexity;data compression;video codecs;video coding","layered screen video coding scheme;hardware video codec;screen video compression;base layer coding;enhancement layer coding;block levels;frame levels;spatial information extraction;temporal information extraction;coding content selection;nonskip screen frames;video quality degradation;compressed text-graphics contents;chroma downsampling;screen video quality;computation complexity;bitrate cost;coding gain","","2","2","9","","26 Sep 2013","","","IEEE","IEEE Conferences"
"Enhanced Motion Vector Prediction for Video Coding","S. Wang; Z. Wang; F. Luo; S. Wang; S. Ma; W. Gao","Peking University, Institute of Digital Media, Beijing, China; Peking University, Institute of Digital Media, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Peking University, Institute of Digital Media, Beijing, China; Peking University, Institute of Digital Media, Beijing, China; Peking University, Institute of Digital Media, Beijing, China","2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)","21 Oct 2018","2018","","","1","5","Motion vector prediction (MVP) plays a crucial role in reducing the rate cost for coding motion vector (MV). However, the relative limited construction of MVP list restricts the potential efficiency. In this paper, an enhanced MVP scheme is proposed. In particular, we first modify the insertion order of MVP candidates according to the statistics. Then, the scaling process during MVP construction is modified to extend the application range. Moreover, an expanded-area motion vector prediction (EMVP) approach is adopted to further utilize the spatial correlation of local motion field. Simulation results shows that the proposed scheme can achieve better prediction for coding motion information.","","978-1-5386-5321-0","10.1109/BigMM.2018.8499100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8499100","MVP;High Efficiency Video Coding;expanded-area motion vector prediction;video coding","Encoding;Video coding;Media;Standards;Video sequences;Correlation;Indexes","image enhancement;motion estimation;vectors;video coding","enhanced motion vector prediction;video coding;rate cost;MVP list;potential efficiency;MVP candidates;MVP construction;local motion field;spatial correlation;coding motion information","","","1","9","","21 Oct 2018","","","IEEE","IEEE Conferences"
"Differential Coding Using Enhanced Inter-Layer Reference Picture for the Scalable Extension of H.265/HEVC Video Codec","A. Aminlou; J. Lainema; K. Ugur; M. M. Hannuksela; M. Gabbouj","Signal Processing Department, Tampere University of Technology, Tampere, Finland; Nokia Research Center, Tampere, Finland; AVCR Information Technologies, Istanbul, Turkey; Nokia Research Center, Tampere, Finland; Signal Processing Department, Tampere University of Technology, Tampere, Finland","IEEE Transactions on Circuits and Systems for Video Technology","28 Oct 2014","2014","24","11","1945","1956","Differential coding methods improve coding efficiency of scalable video codecs by adding the high-frequency component present in the previously coded enhancement layer (EL) pictures to the base layer (BL) picture. This paper proposes a method to enable differential coding in a scalable codec design without affecting the core coding tools, thus allowing a practical implementation to reuse single-layer hardware or software components. This is achieved by creating an additional reference picture called enhanced inter-layer reference (EILR) and inserting it to the EL decoded picture buffer and reference picture lists. An EILR picture is generated by adding differential information to the current inter-layer reference picture. The differential information is calculated using the previously decoded pictures of the BL and EL and the motion information of the BL picture. The proposed method reduces luma total bitrate on average by 2.2% and 2.8% for random access and low-delay test cases, respectively. The improvements are more significant for chroma components with the average bitrate reduction of 6.5%. The measured decoding time increase for a reference software implementation is 16% with negligible overhead on encoding time.","1558-2205","","10.1109/TCSVT.2014.2317884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799259","Differential Video Coding;scalable extension of H.265/High Efficiency Video Coding (SHVC);Scalable Video Coding (SVC)","Encoding;Video coding;Standards;Decoding;Motion compensation;Complexity theory;Video codecs","image colour analysis;software reusability;video codecs;video coding","differential coding methods;enhanced interlayer reference;H.265 video codec;HEVC video codec;coding efficiency improvement;high-frequency component;scalable codec design;single-layer hardware reuse;software component reuse;EILR;EL decoded picture buffer;reference picture lists;luma total bitrate reduction;chroma components;high efficiency video coding","","5","","36","","16 Apr 2014","","","IEEE","IEEE Journals"
"Streaming 360-Degree Videos Using Super-Resolution","M. Dasari; A. Bhattacharya; S. Vargas; P. Sahu; A. Balasubramanian; S. R. Das",Stony Brook University; KTH Royal Institute of Technology; Stony Brook University; Stony Brook University; Stony Brook University; Stony Brook University,"IEEE INFOCOM 2020 - IEEE Conference on Computer Communications","4 Aug 2020","2020","","","1977","1986","360° videos provide an immersive experience to users, but require considerably more bandwidth to stream compared to regular videos. State-of-the-art 360° video streaming systems use viewport prediction to reduce bandwidth requirement, that involves predicting which part of the video the user will view and only fetching that content. However, viewport prediction is error prone resulting in poor user Quality of Experience (QoE). We design PARSEC, a 360° video streaming system that reduces bandwidth requirement while improving video quality. PARSEC trades off bandwidth for additional client-side computation to achieve its goals. PARSEC uses an approach based on super-resolution, where the video is significantly compressed at the server and the client runs a deep learning model to enhance the video to a much higher quality. PARSEC addresses a set of challenges associated with using super-resolution for 360° video streaming: large deep learning models, slow inference rate, and variance in the quality of the enhanced videos. To this end, PAR-SEC trains small micro-models over shorter video segments, and then combines traditional video encoding with super-resolution techniques to overcome the challenges. We evaluate PARSEC on a real WiFi network, over a broadband network trace released by FCC, and over a 4G/LTE network trace. PARSEC significantly outperforms the state-of-art 360° video streaming systems while reducing the bandwidth requirement.","2641-9874","978-1-7281-6412-0","10.1109/INFOCOM41043.2020.9155477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9155477","360° Video;ABR Streaming;Super-resolution","Videos;Bandwidth;Streaming media;Spatial resolution;Computational modeling;Quality of experience","4G mobile communication;broadband networks;image resolution;learning (artificial intelligence);Long Term Evolution;quality of experience;video coding;video signal processing;video streaming","bandwidth requirement;video quality;PARSEC trades;additional client-side computation;deep learning model;PARSEC addresses;enhanced videos;PAR-SEC;shorter video segments;traditional video encoding;super-resolution techniques;state-of-art 360° video streaming;immersive experience;regular videos;state-of-the-art 360° video streaming;viewport prediction;error prone resulting;poor user Quality;design PARSEC;360° video streaming system","","5","","50","","4 Aug 2020","","","IEEE","IEEE Conferences"
"Enhanced rate-distortion optimization for stereo interleaving video coding","Qian Ma; Yongbing Zhang; Qiong Liu; Xiangyang Ji; Qionghai Dai","Department of Automation, Tsinghua University, Beijing, China; Graduate School at Shenzhen, Tsinghua University, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","2011 International Conference on 3D Imaging (IC3D)","22 Aug 2013","2011","","","1","4","Stereo interleaving format attracts more and more attention due to its back compatibility with all existing 2D video coding standards as well as its high efficiency of stereoscopic video compression. As one of the most significant coding components, rate-distortion optimization (RDO) in stereo interleaving video coding does not take into account its application scenarios where the reconstructed video needs up-sampling for displaying. To promote the efficiency of stereo interleaving video compression, we propose an enhanced RDO where the up-sampling is taken into consideration on distortion measurement. The proposed algorithm changes nothing concerning the syntax and is compatible with current decoders. Experimental results demonstrate that the proposed enhanced RDO is able to reduce the bitrates by 10%-44% on average, and achieve 0.1-0.65dB gain on PSNR compared with conventional RDO.","2379-1780","978-1-4799-1579-8","10.1109/IC3D.2011.6584364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6584364","Enhanced RDO;mode decision;stereo interleaving;video coding;up-sampling","Abstracts;Standards;Encoding;Decoding;PSNR","data compression;image reconstruction;image sampling;optimisation;stereo image processing;video coding","enhanced rate-distortion optimization;stereo interleaving video coding;stereo interleaving format;2D video coding standards;stereoscopic video compression;coding components;reconstructed video;up-sampling;stereo interleaving video compression;distortion measurement;enhanced RDO;PSNR;gain 0.1 dB to 0.65 dB","","1","","8","","22 Aug 2013","","","IEEE","IEEE Conferences"
"Enhanced sensitivity of motion detection in satellite videos using instant learning algorithms","J. F. Joe","Department of Computer Science & Engineering, Prathyusha Institute of Technology and Management Anna University, Chennai, India","IET Chennai 3rd International on Sustainable Energy and Intelligent Systems (SEISCON 2012)","23 Jan 2014","2012","","","1","6","Satellite videos are drawing attention of the frontiers in the core of video image processing. Satellite videos are in common used for weather forecast. The satellite weather videos are analyzed for various parameters in prediction of weather. The motion detection in these videos plays a vital role in the prediction of weather. By using the existing methodologies in the video motion detection, only the edges are detected as moving area and the others are left as such. There lies a need for the proposed methodology to detect the multiple motion detection for getting the satellite video processed with even more sensitivity and fidelity. This paper justifies the use of the proposed methodology using iterated training algorithm in the processing of satellite video downloaded from the NASA videos.","","978-1-84919-797-7","10.1049/cp.2012.2250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6719156","Motion Detection;Satellite Video","","geophysical image processing;image motion analysis;iterative methods;learning (artificial intelligence);video signal processing;weather forecasting","instant learning algorithms;video image processing;weather forecast;satellite weather videos;video motion detection;iterated training algorithm;NASA videos","","","","","","23 Jan 2014","","","IET","IET Conferences"
"Summarizing Long-Length Videos with GAN-Enhanced Audio/Visual Features","H. Lee; G. Lee","Seoul National University of Science and Technology, Republic of Korea; Seoul National University of Science and Technology, Republic of Korea","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","3727","3731","In this paper, we propose a novel supervised method for summarizing long-length videos. Many recent works presented successful results in video summarization. However, most videos in those works are short in duration (~5 minutes), and the methods often break down on very long videos (~30 minutes). Moreover, most works only use visual features, while audios provide useful features for the task. Based on these observations, we present a model that exploits both visual and audio features. To handle long videos, our model also refines the extracted features using adversarial networks. To demonstrate our model, we have collected a new dataset of 63 e-sports (~30 minutes) videos, each accompanied by an editorial summary video that is about 10% in length of the original video. Evaluation on this dataset suggests that our method produces quality summaries for very long videos.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022347","Summarization;GAN;Audio;Video;Multimodal","Videos;Feature extraction;Visualization;Task analysis;Mel frequency cepstral coefficient;Benchmark testing;Gallium nitride","audio signal processing;feature extraction;sport;video signal processing","long-length videos;supervised method;video summarization;audio features;editorial summary video;visual features;feature extraction;adversarial networks;e-sports","","","","16","","5 Mar 2020","","","IEEE","IEEE Conferences"
"AntMind: Enhancing error protection for video streaming in wireless networks","R. Immich; P. Borges; E. Cerqueira; M. Curado","University of Coimbra Coimbra, Portugal; University of Coimbra Coimbra, Portugal; Federal University of Para - Belem, Brazil University of California Los Angeles (UCLA) Los Angeles, USA; University of Coimbra Coimbra, Portugal","2014 International Conference on Smart Communications in Network Technologies (SaCoNeT)","31 Jul 2014","2014","","","1","6","On-line video services are becoming a large part of the daily routines of people all over the world, where most of the content is accessed through wireless networks. Therefore, it is of ever growing importance that the negative aspects of these types of error prone networks are lessened in order to ensure adequate quality of the delivered video streams. Forward Error Correction (FEC) techniques allow the stream to be protected with an amount of redundancy to preserve the video quality during transmission. Nevertheless, some FEC schemes do not make an efficient usage of the available network resources due to unnecessary use of redundancy as a result of video-unawareness. The adaptive FEC mechanism proposed in this paper uses the motion intensity characteristics of the video and the network loss state to deliver the video streaming with adequate Quality of Experience (QoE), while keeping the use of network resources to a minimum level. It does so from a combined use of a Random Neural Network (RNN) for motion intensity classification and an Ant Colony Optimization (ACO) scheme for dynamic redundancy allocation. QoE metrics are used to assess the performance of the mechanism showing its advantages over adaptive and nonadaptive protection schemes.","","978-1-4799-5196-3","10.1109/SaCoNeT.2014.6867776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6867776","Motion Vectors (MV);Forward Error Correction (FEC);Video-aware FEC;QoE;Neural networks;Unequal Error Protection (UEP);Ant Colony Optimization","Redundancy;Video sequences;Forward error correction;Streaming media;Video recording;Quality assessment;Wireless networks","ant colony optimisation;forward error correction;neural nets;radio networks;telecommunication computing;video coding;video streaming","antmind enhancing error protection;video streaming;wireless networks;on-line video services;wireless networks;error prone networks;video adequate quality;forward error correction;video transmission;video-unawareness;motion intensity characteristics;network loss state;adequate Quality of Experience;random neural network;motion intensity classification;ant colony optimization;dynamic redundancy allocation;adaptive protection schemes;nonadaptive protection schemes","","1","","19","","31 Jul 2014","","","IEEE","IEEE Conferences"
"Enhanced block prediction in stereoscopic video coding","Dong Li; Yongbing Zhang; Qiong Liu; Xiangyang Ji; Qionghai Dai","TNList and Department of Automation, Tsinghua University, Beijing 100084, China; TNList and Department of Automation, Tsinghua University, Beijing 100084, China; TNList and Department of Automation, Tsinghua University, Beijing 100084, China; TNList and Department of Automation, Tsinghua University, Beijing 100084, China; TNList and Department of Automation, Tsinghua University, Beijing 100084, China","2011 3DTV Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)","16 Jun 2011","2011","","","1","4","The coding efficiency improvement of stereoscopic video system is a hot research topic in the 3D video coding areas. To better exploit the redundancy between the two channels of stereoscopic video, a stereoscopic video coding scheme for Audio Video Coding Standard (AVS) is proposed in the paper. The superior coding performance of the proposed scheme benefits from an enhanced block prediction algorithm, which includes an improvement of AVS direct mode and motion vector prediction, as well as adaptively combining the prediction results of motion compensation and disparity compensation. The experimental results demonstrate that the proposed scheme and algorithm greatly outperforms the reference software of AVS.","2161-203X","978-1-61284-162-5","10.1109/3DTV.2011.5877163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5877163","Stereoscopic video coding;direct mode;motion vector prediction;joint prediction;AVS","Video coding;Encoding;Streaming media;Transform coding;Joints;Prediction algorithms;Three dimensional displays","motion compensation;stereo image processing;video coding","3D video coding;stereoscopic video coding;stereoscopic video system;audio video coding standard;enhanced block prediction algorithm;AVS direct mode;motion vector prediction;motion compensation;disparity compensation","","","8","9","","16 Jun 2011","","","IEEE","IEEE Conferences"
"Evaluation of adaptation methods for multi-view video","S. S. Savas; C. G. Gurler; A. M. Tekalp","College of Engineering Department, Koc University, USA; College of Engineering Department, Koc University, USA; College of Engineering Department, Koc University, USA","2012 19th IEEE International Conference on Image Processing","21 Feb 2013","2012","","","2273","2276","Multi-view video (MVV) is the next step in the evaluation of 3DTV. Using IP networks as the transport medium seems to be the most promising solution because MVV has flexible bitrate requirements that can change based on the number of views requested from the receiver. With the advanced streaming technologies like scalable video coding, the capability of video services over IP has been greatly enhanced. However, a successful MVV delivery service cannot be achieved without properly addressing the perceived quality of experience (QoE) of MVV. QoE is an important issue especially in adaptive video streaming in which the quality of the content varies to match the available channel capacity. This study evaluates the effect of different scaling methods some of which are unique to MVV and propose a novel systematic adaptation strategy in order to deliver the best QoE under diverse network conditions. Extensive subjective tests are conducted to compare different scaling methods on MVV by using high definition contents.","2381-8549","978-1-4673-2533-2","10.1109/ICIP.2012.6467349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6467349","multi-view video;3D video quality evaluation;scalable video coding;adaptation","Streaming media;Encoding;Bit rate;Video coding;Video recording;Quality assessment;Signal to noise ratio","IP networks;quality of experience;three-dimensional television;video coding","multiview video;MVV;3DTV;IP networks;flexible bitrate requirements;scalable video coding;video services;quality of experience;QoE","","5","","7","","21 Feb 2013","","","IEEE","IEEE Conferences"
"Enhanced Macroblock Features for Dynamic Background Modeling in H.264/AVC Video Encoded at Low Bitrate","B. Dey; M. K. Kundu","Center for Soft Computing Research, Indian Statistical Institute, Kolkata, India; Machine Intelligence Unit, Indian Statistical Institute, Kolkata, India","IEEE Transactions on Circuits and Systems for Video Technology","5 Mar 2018","2018","28","3","616","625","While H.264 is a well-established standard for video surveillance, its high-profile implementation, in particular, has the unique capabilities that pack more visual detail into a given bitrate. Several algorithms exist that detect moving objects from the H.264 bitstream, most of which end up incorrectly classifying the nonstationary or dynamic components (e.g., jitter camera, waving trees, ripples, etc.) of the background as foreground. Moreover, the coarse quantization schemes adopted at constrained bitrates pose new challenges for direct identification of moving objects/targets from the bitstream. This paper focuses on dynamic background modeling for the H.264 video encoded at very low bitrates. To this end, enhanced features of the fundamental coding unit, i.e., the macroblock (MB) are proposed. Based on the temporal statistics of the MB features gathered from initial frames, MBs that potentially contain parts of moving objects are selected in subsequent frames. The selected MBs constitute a coarse segmentation map of the foreground at the MB level. Finally, pixel-level segmentation of the foreground is obtained by comparing pixels constituting the selected MBs with the colocated counterparts of a background frame. Experimental results showing the comparison of bitstreams encoded at strikingly low bitrates are obtained over a diverse set of standardized surveillance sequences.","1558-2205","","10.1109/TCSVT.2016.2614984","Indian National Academy of Engineering (INAE) through an INAE Distinguished Professor Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582381","Background subtraction;H.264/Advanced Video Coding (AVC);quantization;transform coding;video surveillance","Quantization (signal);Transforms;Bit rate;Encoding;Image coding;Image color analysis;Standards","image enhancement;image motion analysis;image segmentation;image sequences;object detection;video coding;video surveillance","dynamic background modeling;H.264/AVC Video;low bitrate;video surveillance;nonstationary components;dynamic components;jitter camera;coarse quantization schemes;constrained bitrates;fundamental coding unit;MB features;coarse segmentation map;MB level;pixel-level segmentation;background frame;standardized surveillance sequences;moving objects detection;macroblock feature enhancement","","1","","25","","4 Oct 2016","","","IEEE","IEEE Journals"
"Perception-based Asymmetric Video Coding for 3D Video","N. Xu; X. Fang; W. Li; Y. An","Shanghai Jiao Tong University,Shanghai,China; Shanghai Jiao Tong University,Shanghai,China; Shanghai Jiao Tong University,Shanghai,China; Shanghai Jiao Tong University,Shanghai,China","2020 IEEE 2nd International Conference on Civil Aviation Safety and Information Technology (ICCASIT","9 Mar 2021","2020","","","492","494","Asymmetric video coding is an effective method to enhance the efficiency of encoding full-resolution stereo format 3D video. Related researches demonstrate that a suitable reduction on video quality of auxiliary view will not damage the entire perceptual video coding of 3D video, where main view is not changed. In paper, a perception-based quantization parameter adjustment scheme is proposed for asymmetric video coding. Experiments demonstrate that it can save bit significantly rate without any perceptual video quality degradation.","","978-1-7281-9948-1","10.1109/ICCASIT50869.2020.9368737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9368737","asymmetric video coding;perceptulal video coding;3D video","Video coding;Visualization;Three-dimensional displays;Quantization (signal);Sensitivity;Quality assessment;Video recording","data compression;quantisation (signal);video coding","perception-based quantization parameter adjustment scheme;perceptual video quality degradation;full-resolution stereo format 3D video;perception-based asymmetric video coding","","","","14","","9 Mar 2021","","","IEEE","IEEE Conferences"
"Video-Based Evidence Analysis and Extraction in Digital Forensic Investigation","J. Xiao; S. Li; Q. Xu","School of Computer Science and Engineering, Central South University, Hunan, Changsha, China; Department of Computer Science and Creative Technologies, University of the West of England, Bristol, U.K.; School of Computer Science and Engineering, Central South University, Hunan, Changsha, China","IEEE Access","6 May 2019","2019","7","","55432","55442","As a result of the popularity of smart mobile devices and the low cost of surveillance systems, visual data are increasingly being used in digital forensic investigation. Digital videos have been widely used as key evidence sources in evidence identification, analysis, presentation, and report. The main goal of this paper is to develop advanced forensic video analysis techniques to assist the forensic investigation. We first propose a forensic video analysis framework that employs an efficient video/image enhancing algorithm for the low quality of footage analysis. An adaptive video enhancement algorithm based on contrast limited adaptive histogram equalization (CLAHE) is introduced to improve the closed-circuit television (CCTV) footage quality for the use of digital forensic investigation. To assist the video-based forensic analysis, a deep-learning-based object detection and tracking algorithm are proposed that can detect and identify potential suspects and tools from footages.","2169-3536","","10.1109/ACCESS.2019.2913648","Planned Science and Technology Projects of Hunan Province, China(grant numbers:2016JC2010,2017NK2394); Planned Science and Technology Project of Changsha, China(grant numbers:kq1801071); Central South University(grant numbers:2017zzts717); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8700194","Forensics investigation;forensic video analysis;video/image enhancement;object detection;deep learning","Videos;Face recognition;Digital forensics;Mobile handsets;Histograms;Image recognition","closed circuit television;digital forensics;image enhancement;learning (artificial intelligence);object detection;video signal processing;video surveillance","video-based evidence analysis;digital forensic investigation;digital videos;key evidence sources;evidence identification;advanced forensic video analysis techniques;forensic video analysis framework;footage analysis;adaptive video enhancement algorithm;video-based forensic analysis;video enhancing algorithm;image enhancing algorithm;deep-learning-based object tracking algorithm;deep-learning-based object detection algorithm","","8","","44","OAPA","26 Apr 2019","","","IEEE","IEEE Journals"
"Embedding interstitial interactivity meta-data in video","K. S. Ramasubramaniam; G. Annamalai","Architecture Group, Cisco Video Technologies Pvt Ltd, Bangalore, India; Distinguished Engineer, Cisco Video Technologies Pvt Ltd, Bangalore, India","2016 Zooming Innovation in Consumer Electronics International Conference (ZINC)","18 Jul 2016","2016","","","59","61","State-of-the-art broadcast/transmission standards such as ATSC/DVB/ISDB are based on MPEG as the underlying compression standard. Various services like recommendations, advanced search etc enhance the experience of viewing the video content by maintaining a database of tags pertaining to each video. In a typical database search, there is no timestamp associated with the record item to the video content timestamp. This prohibits the progress of the next generation content-based interactivity mechanisms. This paper presents a mechanism to define and efficiently utilize the meta-data for content-based interactivity without any heavy-duty processing on the video encoding / decoding pipeline. For production house content as well as user generated content, the meta-data can be inserted into the stream. The main advantage of this approach is the portability across various platforms and decoder implementations. The overheads on the bit-stream, processing and memory due to such a change are also depicted.","","978-1-5090-2957-0","10.1109/ZINC.2016.7513655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7513655","content-based interactivity;in-source video metadata","Decision support systems","content-based retrieval;data compression;meta data;video coding;video retrieval","video interstitial interactivity meta-data embedding;broadcast standards;transmission standards;ATSC;DVB;ISDB;MPEG;compression standard;video tag database;database search;video content timestamp;content-based interactivity mechanism;video encoding-decoding pipeline;production house content;user generated content;portability","","","","5","","18 Jul 2016","","","IEEE","IEEE Conferences"
"Video quality assessment in video streaming services considering user preference for video content","D. Z. Rodríguez; R. L. Rosa; E. A. Costa; J. Abrahão; G. Bressan","School of Engineering - University of São Paulo, Brazil; School of Engineering - University of São Paulo, Brazil; University of Brescia, Italy; School of Engineering - University of São Paulo, Brazil; School of Engineering - University of São Paulo, Brazil","IEEE Transactions on Consumer Electronics","3 Nov 2014","2014","60","3","436","444","In video streaming service, the user's Quality of Experience (QoE) is not only related to video signal quality received at consumer's devices, the users' subjectivity must also be considered. In this context, a video quality assessment method that takes into account the user's preference for video content is proposed in this research. In order to perform this task, the users' profiles that include their preferences were stored in a video server. Then, subjective tests of video quality assessment were conducted, in which evaluators had different video content preferences. Results show that the evaluators' QoE is highly correlated with the user's preference for video content type. Based on these experimental results, a function named Preference Factor (PF) is defined and used to adjust the quality index values obtained by an objective video quality metric running in the end user's device. The PF function also depends on video content type and quality index score. Using the PF function, the enhanced Video streaming Quality Metric (e-VsQM) is proposed and the results of its performance evaluation demonstrate that PF improves an objective video quality metric. Furthermore, e-VsQM has low complexity and can be utilized in different video services. Thus, an application scenario is presented, in which the proposed video quality metric is implemented.","1558-4127","","10.1109/TCE.2014.6937328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6937328","Video Streaming;Video Quality Metrics;Subjective Test;QoE;MOS","Streaming media;Video recording;Quality assessment;Measurement;Indexes;Context;Multimedia communication","quality of experience;video servers;video streaming","e-VsQM;enhanced Video streaming Quality Metric;quality index score;quality index values;preference factor;video quality subjective tests conduction;video server;video signal quality;QoE;quality of experience;video content type;user preference;video streaming services;video quality assessment","","21","","46","","3 Nov 2014","","","IEEE","IEEE Journals"
"Powerful and Low Time Phase-Based Video Magnification Enhancing Technique","Z. J. Al-allaq; H. I. Shahadi; H. J. Albattat","Al-Furat Al-Awsat Technical University,Communication Techniques Eng. Dept.,Najaf,Iraq; University of Karbala,Electrical and Electronic Eng. Dept.,Karbala,Iraq; Al-Furat Al-Awsat Technical University,Communication Techniques Eng. Dept.,Najaf,Iraq","2019 4th Scientific International Conference Najaf (SICN)","2 Mar 2020","2019","","","133","138","Video processing techniques become major role with the wildly spread of smartphones and other new technologies. One of the high impact video processing is a magnification. Magnification refers to amplify a region or special bands of video frames in order to detect subtle information. The famous approach to achieve magnification is Eulerian-based. This paper proposes a developed approach for a phase-based Eulerian video magnification (PB-EVM) in order to reduce the processing time. The proposed approach reduces the processed data in PB-EVM by resizing-down the source video size with maintaining the video quality. Subsequently, the resulted video is decomposed using complex steerable pyramid. Then, the decomposed video is entered to temporal filters in order to pass only specific frequency bands, and then amplified the energy of these bands by multiplying by magnification factor. Finally, the video magnified video is reconstructed by steerable pyramid and resizing-up to come back to the original video size. The experimental results of the proposed approach show that there is a 60-65% reduction in the execution time compare to the conventional PB-EVM. Furthermore, the magnified video quality is maintained as it in the conventional PB-EVM.","","978-1-7281-4425-2","10.1109/SICN47020.2019.9019338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9019338","Motion magnification;Eulerian video magnification (EVM);Linear-Based (LB-EVM);Phase-Based (PB-EVM);video resizing","Finite impulse response filters;Pediatrics;IIR filters;Band-pass filters;Kernel;Computer aided software engineering;Image reconstruction","image sequences;video signal processing","magnified video quality;high impact video processing;video frames;phase-based Eulerian video magnification;processing time;processed data;source video size;decomposed video;magnification factor;video magnified video","","","","17","","2 Mar 2020","","","IEEE","IEEE Conferences"
"Polyview Fusion: A Strategy to Enhance Video-Denoising Algorithms","K. Zeng; Z. Wang","Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada","IEEE Transactions on Image Processing","19 Mar 2012","2012","21","4","2324","2328","We propose a simple but effective strategy that aims to enhance the performance of existing video denoising algorithms, i.e., polyview fusion (PVF). The idea is to denoise the noisy video as a 3-D volume using a given base 2-D denoising algorithm but applied from multiple views (front, top, and side views). A fusion algorithm is then designed to merge the resulting multiple denoised videos into one, so that the visual quality of the fused video is improved. Extensive tests using a variety of base video-denoising algorithms show that the proposed PVF method leads to surprisingly significant and consistent gain in terms of both peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) performance, particularly at high noise levels, where the improvement over state-of-the-art denoising algorithms is often more than 2 dB in PSNR.","1941-0042","","10.1109/TIP.2011.2170699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035781","Image fusion;polyview;video denoising;video quality enhancement","Noise reduction;Three dimensional displays;PSNR;Noise measurement;Video sequences;Complexity theory","image denoising;image enhancement;image fusion;video signal processing","video denoising;polyview fusion;3D volume;2D denoising algorithm;PVF method;peak signal-to-noise ratio;structural similarity;PSNR;video quality enhancement","Algorithms;Artifacts;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Pattern Recognition, Automated;Photography;Reproducibility of Results;Sensitivity and Specificity;Signal-To-Noise Ratio;Video Recording","5","","24","","6 Oct 2011","","","IEEE","IEEE Journals"
"Local-constrained quadtree plus binary tree block partition structure for enhanced video coding","Z. Wang; S. Wang; J. Zhang; S. Ma","Institute of Digital Media & Cooperative Medianet Innovation Center, Peking University, Beijing, China; Institute of Digital Media & Cooperative Medianet Innovation Center, Peking University, Beijing, China; Institute of Digital Media & Cooperative Medianet Innovation Center, Peking University, Beijing, China; Institute of Digital Media & Cooperative Medianet Innovation Center, Peking University, Beijing, China","2016 Visual Communications and Image Processing (VCIP)","5 Jan 2017","2016","","","1","4","In the latest Joint Video Exploration Team (JVET) development, a quadtree plus binary tree (QTBT) block partition structure is proposed for enhanced video coding. Compared to the quadtree block partition in HEVC, QTBT can achieve much better compression performance at the expense of largely increased encoding computational complexity. In this paper, a fast QTBT block partition algorithm based on the local constraint is proposed. Given the decoded information of the previous frame, the partition parameters can be dynamically derived for each coding tree unit (CTU) without any overhead. Furthermore, a constrained binary tree partition algorithm is proposed to reduce the redundancy between the quadtree partition and the binary tree partition. Experimental results have shown that the proposed scheme can speed up QTBT block partition structure by 30% on average without obvious performance decrease, which enables its practical implementations in real application scenarios.","","978-1-5090-5316-2","10.1109/VCIP.2016.7805555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7805555","Block partition;quadtree;binary tree;fast partition;local constraint","Binary trees;Encoding;Heuristic algorithms;Partitioning algorithms;Computational complexity;Video coding;Redundancy","computational complexity;trees (mathematics);video coding","enhanced video coding;joint video exploration team;quadtree plus binary tree block partition structure;HEVC;computational complexity;coding tree unit;binary tree partition algorithm","","9","","5","","5 Jan 2017","","","IEEE","IEEE Conferences"
"Adaptive Bit Allocation for Consistent Video Quality in Scalable High Efficiency Video Coding","S. Yang; P. B. Vo","National Taipei University of Technology, Taipei, Taiwan; Dalat University, Da Lat, Vietnam","IEEE Transactions on Circuits and Systems for Video Technology","30 Jun 2017","2017","27","7","1555","1567","Scalable video coding (SVC) is a coding paradigm that allows once-encoded video content to be used in diverse scenarios. SVC-coded videos can be transmitted and rendered at specified bitrates according to network bandwidth and end device requirements. In this paper, an adaptive bit allocation algorithm is proposed for the emerging scalable High Efficiency Video Coding (SHVC) standard. The bit budget at the group-of-pictures level is allocated according to buffer occupancy. Picture complexity, measured using the predicted mean absolute difference (MAD), buffer occupancy, and hierarchical level, is proposed for regulating the bitrate at the picture level. The MAD of the current picture is predicted using a novel mean prediction error (MPE) model, which is obtained from the advanced motion vector prediction, and the test zone search specified in SHVC and the associated reference software of SHVC. Moreover, MPE is used to determine the number of assigned bits at the coding-treeunit level. The bit budget of each level is incorporated with the R-λ model for computing the required quantization parameter. Experimental results reveal that the proposed method achieves accurate bitrates with enhanced and consistent visual quality and more satisfactorily controls buffer occupancy compared with the state-of-the-art approaches reported in the literature.","1558-2205","","10.1109/TCSVT.2016.2539639","Ministry of Science and Technology, Taiwan(grant numbers:MOST 103-2221-E-027-059,MOST 103-2218-E-006-MY3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7428907","Bit allocation;mean absolute difference (MAD);mean prediction error (MPE);rate control (RC);scalable High Efficiency Video Coding (SHVC);scalable video coding (SVC)","Bit rate;Encoding;Static VAr compensators;Visualization;Video sequences","image motion analysis;image resolution;quantisation (signal);trees (mathematics);video coding","adaptive bit allocation algorithm;video quality;scalable high efficiency video coding;SHVC;video content;mean absolute difference;MAD;mean prediction error model;MPE model;motion vector prediction;reference software;coding-tree unit level;R-λ model;quantization parameter","","4","","29","","8 Mar 2016","","","IEEE","IEEE Journals"
"Tagging Webcast Text in Baseball Videos by Video Segmentation and Text Alignment","C. Chiu; P. Lin; S. Li; T. Tsai; Y. Tsai","Department of Computer Science and Information Engineering, National Chiayi University, Chiayi, Taiwan; Department of Computer Science and Information Engineering, National Chiayi University, Chiayi, Taiwan; Department of Computer Science and Information Engineering, National Chiayi University, Chiayi, Taiwan; Department of Computer Science and Information Engineering, National Chiayi University, Chiayi, Taiwan; Department of Computer Science and Information Engineering, National Chiayi University, Chiayi, Taiwan","IEEE Transactions on Circuits and Systems for Video Technology","28 Jun 2012","2012","22","7","999","1013","Sports video annotation, an active research area in the field of multimedia content understanding, is an essential process in applications, such as summarization, highlight extraction, event detection, and retrieval. This paper considers the issue in relation to the annotation of baseball videos. Conventional baseball video annotation frameworks are based primarily on video content analysis, such as scoreboard recognition and machine learning techniques, which require a substantial amount of human input to collect and organize training data. The performance of such frameworks might become unstable if they encounter audiovisual patterns not included in the training data. To address the issue, we propose a novel framework for baseball video annotation that aligns high-level webcast text with low-level video content. Several cues, which are derived from the video content and webcast text, are utilized for alignment by leveraging hierarchical agglomerative clustering and genetic algorithm optimization. In addition, we develop an unsupervised method to learn the pitch segment properties of baseball videos by Markov random walk, and thereby reduce the need for human intervention substantially. Our experiments demonstrate that the proposed framework yields a robust result against a variety of video content and enhances the automaticity in baseball video annotation.","1558-2205","","10.1109/TCSVT.2012.2189478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6159070","Event detection;genetic algorithm;multimodal fusion;unsupervised learning;video annotation","Videos;Sports equipment;Games;Hidden Markov models;Event detection;Markov processes;Face","image segmentation;video retrieval","tagging Webcast text;baseball video;video segmentation;text alignment;sport video annotation;multimedia content;highlight extraction;event detection;machine learning technique;scoreboard recognition;low-level video content;genetic algorithm optimization;hierarchical agglomerative clustering;Markov random walk","","14","","34","","28 Feb 2012","","","IEEE","IEEE Journals"
"Caching-Based Scalable Video Transmission Over Cellular Networks","L. Wu; W. Zhang","Chinese Academy of Sciences, Key Laboratory of Wireless-Optical Communications; Chinese Academy of Sciences, Key Laboratory of Wireless-Optical Communications","IEEE Communications Letters","9 Jun 2016","2016","20","6","1156","1159","In this letter, caching-based scalable video transmission over cellular networks under constrained backhaul is considered, where popular videos encoded by scalable video coding are cached at base stations. Joint caching and transmission schemes are proposed considering the impacts of two intrinsic characteristics of videos, namely, popularity and quality. Moreover, mutually dependent relationship between the backhaul capacity and caching-based transmission scheme is also studied. A comprehensive analysis is conducted leveraging tools from stochastic geometry. It is shown that the proposed caching-based transmission schemes not only reduce reduplicative traffic by improving backhaul traffic offloading, but also enhance user satisfaction by providing differentiated user services and reduced video transmission delay.","1558-2558","","10.1109/LCOMM.2016.2555298","National Basic Research Program of China (973 Program) through grant(grant numbers:2013CB329004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7454673","Wireless caching;constrained backhaul;scalable video coding;stochastic geometry;Wireless caching;constrained backhaul;scalable video coding;stochastic geometry","Streaming media;High definition video;Servers;Indexes;Delays;Video coding;Geometry","cache storage;cellular radio;telecommunication traffic;video coding;video communication","caching-based scalable video transmission;cellular network;scalable video coding;base station;joint caching and transmission scheme;constrained backhaul capacity;stochastic geometry;reduplicative traffic reduction;user satisfaction enhancement;differentiated user service;video transmission delay reduction","","34","","8","","20 Apr 2016","","","IEEE","IEEE Journals"
"Study of subject agreement on stereoscopic video quality","M. Chen; D. Kwon; A. C. Bovik","Laboratory for Image and Video Engineering (LIVE), Department of Electrical & Computer Engineering, The University of Texas at Austin, USA; Systems and Applications R&D Center, Texas Instruments, 12500 TI Blvd., Dallas, 75243, USA; Laboratory for Image and Video Engineering (LIVE), Department of Electrical & Computer Engineering, The University of Texas at Austin, USA","2012 IEEE Southwest Symposium on Image Analysis and Interpretation","21 May 2012","2012","","","173","176","We describe a study that aims towards enhancing our understanding of the perception of H.264/AVC compressed stereoscopic 3D videos, in particular spatial video quality, depth quality, visual comfort and overall 3D video quality. The results of this study indicate that the human subjects have diverse opinions on depth quality scores but a high agreement on spatial video quality. Their agreement on overall 3D video quality is intermediate relative to that on spatial video quality and depth quality. Based on our analysis, we propose to use separate quality assessment models: spatial video quality models and depth quality models.","","978-1-4673-1830-3","10.1109/SSIAI.2012.6202481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6202481","3D video quality;depth quality;comfort visual;3D video database;psychometrics","Three dimensional displays;Stereo image processing;Visualization;Solid modeling;Measurement;Humans;Quality assessment","stereo image processing;video signal processing","subject agreement;stereoscopic video quality;H.264/AVC compressed stereoscopic 3D videos;spatial video quality;depth quality;visual comfort;3D video quality;spatial depth quality","","17","","8","","21 May 2012","","","IEEE","IEEE Conferences"
"Enhance learning in a video lecture archive with annotations","M. Malchow; M. Bauer; C. Meinel","Hasso Plattner Institute (HPI), University of Potsdam, Potsdam, Germany; Hasso Plattner Institute (HPI), University of Potsdam, Potsdam, Germany; Hasso Plattner Institute (HPI), University of Potsdam, Potsdam, Germany","2018 IEEE Global Engineering Education Conference (EDUCON)","24 May 2018","2018","","","849","856","When students watch learning videos online, they usually need to watch several hours of video content. In the end, not every minute of a video is relevant for the exam. Additionally, students need to add notes to clarify issues of a lecture. There are several possibilities to enhance the metadata of a video, e.g. a typical way to add user-specific information to an online video is a comment functionality, which allows users to share their thoughts and questions with the public. In contrast to common video material which can be found online, lecture videos are used for exam preparation. Due to this difference, the idea comes up to annotate lecture videos with markers and personal notes for a better understanding of the taught content. Especially, students learning for an exam use their notes to refresh their memories. To ease this learning method with lecture videos, we introduce the annotation feature in our video lecture archive. This functionality supports the students with keeping track of their thoughts by providing an intuitive interface to easily add, modify or remove their ideas. This annotation function is integrated in the video player. Hence, scrolling to a separate annotation area on the website is not necessary. Furthermore, the annotated notes can be exported together with the slide content to a PDF file, which can then be printed easily. Lecture video annotations support and motivate students to learn and watch videos from an E-Learning video archive.","2165-9567","978-1-5386-2957-4","10.1109/EDUCON.2018.8363319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8363319","E-Learning;Lecture Video Archive;Video annotations;E-Learning exam preparation","Browsers;Electronic learning;Web servers;Streaming media;Databases","computer aided instruction;image annotation;interactive video;Internet;meta data;user interfaces;video signal processing","lecture videos;video lecture archive;annotation function;video player;annotated notes;video content;online video;learning enhancement;video material;lecture video annotations;video metadata;user-specific information;comment functionality;exam preparation;annotation feature;intuitive interface;PDF file;slide content;student motivation;E-Learning video archive","","3","","11","","24 May 2018","","","IEEE","IEEE Conferences"
"Fast Enhanced DWT based Video Micro Movement Magnification","G. Fahmy; O. M. Fahmy; M. F. Fahmy","Kuwait College of Science and Technology,Electrical Engineering Dept.; Future University in Egypt,Electrical Engineering Dept.,Cairo,Egypt; Assiut University,Electrical Engineering Dept.,Assiut,Egypt","2019 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)","20 Feb 2020","2019","","","1","6","Magnifying micro movements of natural videos that are undetectable by human eye have recently received considerable interests. This is due to its impact in numerous applications. In this paper, we present a new technique to estimate accurate phase changes between subsequent video frames at different spatial locations of Complex Wavelets CWT sub bands. This estimation is more accurate compared to recent literature techniques utilizing CWT in micro movement magnification. We also propose to speed up the magnification process through only amplifying small CWT local phase error intervals for each individual frame sub band. A simple block matching technique is also proposed to assess the quality of magnification and localize magnified regions. We applied our proposed technique on both Dual Tree Complex Wavelet Transform DT-CWT as well as Real two-dimensional Dual Tree DWT. Several simulations are given to show that the proposed technique competes very well with the existing micro magnification approaches such as steerable pyramids STR and Riesz Transform based steerable pyramids RT-STR. A detailed comparison of these techniques performance in micro movement magnification is illustrated. The attached video file demonstrates the superior video quality attained by the proposed technique.","2641-5542","978-1-7281-5341-4","10.1109/ISSPIT47144.2019.9001874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9001874","Complex wavelets;Micro video magnification;local phase amplification","Continuous wavelet transforms;Discrete wavelet transforms;Image reconstruction;Band-pass filters;Pediatrics","discrete wavelet transforms;image denoising;trees (mathematics);video signal processing","RT-STR;Riesz transform based steerable pyramids;two-dimensional dual tree DWT;dual tree complex wavelet transform DT-CWT;CWT local phase error intervals;complex wavelets CWT sub bands;fast enhanced DWT based video micro movement magnification;spatial locations;video frames;human eye;natural videos;simple block matching technique;individual frame sub band","","2","","16","","20 Feb 2020","","","IEEE","IEEE Conferences"
"On the impact of the video quality assessment in 802.11e ad-hoc networks using adaptive retransmissions","R. Corrado; M. Comisso; F. Babich","Department of Engineering and Architecture University of Trieste, Via A. Valerio 10, 34127 Trieste, Italy; Department of Engineering and Architecture University of Trieste, Via A. Valerio 10, 34127 Trieste, Italy; Department of Engineering and Architecture University of Trieste, Via A. Valerio 10, 34127 Trieste, Italy","2014 13th Annual Mediterranean Ad Hoc Networking Workshop (MED-HOC-NET)","8 Jul 2014","2014","","","47","54","This paper investigates the influence of the video quality assessment at the medium access control (MAC) layer of an 802.11e distributed network in which the purpose is to adapt the retry limit of each video packet for maximizing the quality perceived by the end user. Moving from an existing Markov model of the 802.11e access scheme, the number of retransmissions associated to each video packet is evaluated to satisfy the distortion requirements inferred using two quality assessment measures: the mean squared error (MSE) and the structural similarity (SSIM) index. The aim of the proposed work is to discuss the suitability of these measures for managing the retransmission policy in the presence of contention-based mechanisms. The numerical results are obtained using a network simulator implementing the 802.11e enhanced distributed channel access (EDCA) in which the video sequences are encoded using the H.264 scalable video coding (SVC) standard.","","978-1-4799-5258-8","10.1109/MedHocNet.2014.6849104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6849104","Wireless video streaming;802.11e network;video quality assessment;retry limit adaptation","IEEE 802.11e Standard;Quality assessment;Streaming media;Video recording;Indexes;Video sequences;Estimation","access protocols;ad hoc networks;Markov processes;mean square error methods;video coding;video communication;wireless LAN","SVC standard;H.264 scalable video coding standard;EDCA;802.11e enhanced distributed channel access;network simulator;contention-based mechanisms;retransmission policy;SSIM index;structural similarity index;MSE;mean squared error;quality assessment measures;distortion requirements;802.11e access scheme;Markov model;end user;video packet;802.11e distributed network;MAC layer;medium access control layer;adaptive retransmissions;802.11e ad hoc networks;video quality assessment","","6","","24","","8 Jul 2014","","","IEEE","IEEE Conferences"
"Artificial Intelligence Based Region of Interest Enhanced Video Compression","P. Guruvareddiar; P. Prasad","Intel Corporation, USA; Intel Corporation, USA","2020 Data Compression Conference (DCC)","2 Jun 2020","2020","","","373","373","Artificial Intelligence, especially deep learning based workloads for the analysis of video data are on the rise. Examples include residential and commercial security systems where the camera data are analyzed for potential intruders, in-door retail cameras that track people movement for behavior analysis etc. In an End-to-End intelligent video solution, video analytics will be carried out at multiple places from edge IP camera to high performance cloud servers. Typically, the results of the video analytics at one stage will be sent to the next stage along with the video data for efficient processing. However, a traditional video encoder does not have an understanding of the scene and/or the priority of the objects in the frame. It tries to compress the frames with a goal to produce visually pleasing video for the human viewers by maximizing the rate-distortion performance. As a result, in the compressed video frame, the details of the regions of interest to the inferencing engine may be lost due to the artifacts introduced by the lossy compression process. We propose a novel AI based low-power architecture that utilizes the results of the machine learning at the given stage to improve the performance of the overall pipeline. The proposed method will identify the regions of interest in the frame and preserve the quality of those regions without significantly reducing the quality of the remaining regions. Simulation results show that the machine vision accuracy when inferred on the compressed video streams using the proposed method are significantly better compared to the inferencing results on the compressed video streams produced by the traditional video compression methods.","2375-0359","978-1-7281-6457-1","10.1109/DCC47342.2020.00095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9105758","","","cameras;computer vision;data compression;learning (artificial intelligence);video coding;video signal processing;video streaming;video surveillance","video analytics;video data;visually pleasing video;rate-distortion performance;compressed video frame;lossy compression process;compressed video streams;artificial Intelligence based region;enhanced video compression;deep learning;camera data;in-door retail cameras;edge IP camera;high performance cloud servers;end-to-end intelligent video solution;AI based low-power architecture;machine learning","","1","","0","","2 Jun 2020","","","IEEE","IEEE Conferences"
"Enhancing Video Encoding for Cloud Gaming Using Rendering Information","Y. Liu; S. Dey; Y. Lu","Department of Electrical and Computer EngineeringMobile Systems Design Laboratory, University of California at San Diego, La Jolla, CA, USA; Department of Electrical and Computer EngineeringMobile Systems Design Laboratory, University of California at San Diego, La Jolla, CA, USA; Department of Electrical and Computer EngineeringMobile Systems Design Laboratory, University of California at San Diego, La Jolla, CA, USA","IEEE Transactions on Circuits and Systems for Video Technology","2 Dec 2015","2015","25","12","1960","1974","Cloud gaming allows games to be rendered on the cloud server and allows the rendered videos to be encoded and streamed in real time to the player's devices. Compared with other video streaming applications, cloud gaming offers a unique opportunity to enhance the video encoding process by exploiting rendering information. In this paper, we propose two techniques to improve cloud gaming video encoding, aiming at enhancing the perceived video quality and reducing the computational complexity, respectively. First, we develop a rendering-based prioritized encoding technique to improve the perceived game video quality according to network bandwidth constraints. We first propose a technique to generate a macroblock (MB)-level saliency map for every game video frame using rendering information. Furthermore, based on such a saliency map, a prioritized rate allocation scheme is proposed to dynamically adjust the value of quantization parameter of each MB. The experimental results indicate that the perceptual quality can be greatly improved using the proposed technique. We also develop a rendering-based encoding acceleration technique that utilizes rendering information to reduce the computational complexity of video encoding. This technique mainly consists of two parts. First, we propose a method to directly calculate the motion vectors (MVs) without employing the compute intensive motion search procedure. Second, based on the computed MVs, we propose a fast mode selection algorithm to reduce the number of candidate modes of each MB. The experimental results show that the proposed technique can achieve more than 42% saving in encoding time with very limited degradation in video quality.","1558-2205","","10.1109/TCSVT.2015.2450175","InterDigital, Inc., Wilmington, DE, USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7137645","Cloud gaming;graphic rendering;prioritized encoding;MB mode selection;Cloud gaming;graphic rendering;macroblock (MB) mode selection;prioritized encoding","Games;Rendering (computer graphics);Cloud computing;Streaming media;Encoding;Video recording;Quality assessment","cloud computing;computational complexity;computer games;image enhancement;image motion analysis;image representation;quantisation (signal);rendering (computer graphics);video coding;video streaming","video encoding enhancement;cloud gaming;rendering information;cloud server;video streaming;perceived video quality;computational complexity reduction;rendering-based prioritized encoding technique;perceived game video quality;macroblock-level saliency map;game video frame;prioritized rate allocation scheme;quantization parameter;rendering-based encoding acceleration technique;motion vectors;fast mode selection algorithm","","12","1","33","","26 Jun 2015","","","IEEE","IEEE Journals"
"Enhanced the Quality of Telemedicine Real-Time Video Transmission and Distortion Minimization in Wireless Network","A. Ghimire; A. Alsadoon; P. W. C. Prasad; N. Giweli; O. D. Jerew; G. Alsadoon","School of Computing and Mathematics; Study Group Australia,Australia; School of Computing and Mathematics; Study Group Australia,Australia; School of Computing and Mathematics; Study Group Australia,Australia; School of Computing and Mathematics; Study Group Australia,Australia; Asia Pacific International College (APIC),Department of Business Management,Sydeny,Australia; AMA International University Bahrain (AMAIUB),Business Informatics Department,Zallaq,Bahrain","2020 5th International Conference on Innovative Technologies in Intelligent Systems and Industrial Applications (CITISIA)","9 Mar 2021","2020","","","1","10","Achieving good quality and minimum distortion of the video frames is one of the most challenging requirements in the telemedicine system. Transmission process for a real-time video over the wireless network is due to various real-time restrictions, such as encoding mechanism, noise, and bandwidth fluctuations. The restrictions introduce distortions and delay, hence adversely affect the reliability and quality of the video transmission system. This study aims to propose a new system which can fine-tune the encoding process dynamically. The proposed system consists of an Enhanced Video Quality and Distortion Minimization (EVQDM) algorithm to achieve guaranteed quality, minimum distortion, and the minimum delay in the transmission of the video. This system guarantees the video quality by using the adaptive video encoding technique and minimizes the distortion by considering the truncating distortion in the enhanced distortion minimization algorithm. The results of applying the proposed EVQDM algorithm and the state-of-the-art solutions are compared, and it was shown that the proposed algorithm improved the state-of-the-art solution. The video quality has been increased from 47.2 dB to 50.13 dB, the video distortion has been minimized from 0.6802 to 0.3509 and the end-to-end delay has been reduced from 123.58 ms to 112.57 ms. The proposed solution focuses on truncating distortion, to minimize the total distortion of the video. For summarization, this solution addressed the issues of achieving minimum distortion and delay while providing the guaranteed video quality within the boundaries of the real-time constraints that are imposed on the system.","","978-1-7281-9437-0","10.1109/CITISIA50690.2020.9371839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9371839","Adaptive video encoding;video quality;delay;distortion minimization;Tele-surgery","Streaming media;Distortion;Minimization;Real-time systems;Quality assessment;Delays;Video recording","minimisation;telemedicine;video coding;video communication;video streaming","wireless network;video frames;telemedicine system;encoding process;enhanced video quality;guaranteed quality;minimum delay;adaptive video encoding technique;truncating distortion;enhanced distortion minimization algorithm;EVQDM algorithm;video distortion;end-to-end delay;total distortion;guaranteed video quality;real-time constraints;telemedicine real-time video transmission","","","","25","","9 Mar 2021","","","IEEE","IEEE Conferences"
"Hierarchical Model For Long-Length Video Summarization With Adversarially Enhanced Audio/Visual Features","H. Lee; G. Lee","Seoul National University of Science and Technology,Republic of Korea; Seoul National University of Science and Technology,Republic of Korea","2020 IEEE International Conference on Image Processing (ICIP)","30 Sep 2020","2020","","","723","727","In this paper, we propose a novel supervised method for summarizing long-length videos. Many recent approaches presented promising results in video summarization. However, videos in most benchmark datasets are short in duration (<; 10 minutes), and the methods often do not work well for very long-length videos (>1 hour). Furthermore, most approaches only use visual features, while audios provide useful information for the task. Based on these observations, we present a model that exploits both audio and visual features. To handle long videos, the hierarchical structure of our model captures both the short-term and long-term temporal dependencies. Our model also refines the extracted features using adversarial networks. To demonstrate our model, we have collected a new dataset of 28 baseball (~3.5 hours) videos, accompanied by an editorial summary video that is 5% in length of the original video. Evaluation on the dataset suggests that our method produces quality summaries for very long videos.","2381-8549","978-1-7281-6395-6","10.1109/ICIP40778.2020.9190636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9190636","video summarization;multimodal features;hierarchical model;adversarial learning;long-length videos","Visualization;Feature extraction;Semantics;Benchmark testing;Task analysis;Generators","audio-visual systems;feature extraction;neural nets;sport;video signal processing","hierarchical model;long-length video summarization;novel supervised method;benchmark datasets;hierarchical structure;long-term temporal dependencies;baseball videos;editorial summary video;adversarially enhanced audio-visual features;short-term temporal dependencies;adversarial networks;feature extraction;time 10.0 min;time 1.0 hour;time 3.5 hour","","","","15","","30 Sep 2020","","","IEEE","IEEE Conferences"
