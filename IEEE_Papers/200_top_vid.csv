"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Automated In-Home Assistive Monitoring with Privacy-Enhanced Video","A. Edgcomb; F. Vahid","Dept. of Comput. Sci. & Eng., Univ. of California, Riverside, Riverside, CA, USA; Dept. of Comput. Sci. & Eng., Univ. of California, Riverside, Riverside, CA, USA","2013 IEEE International Conference on Healthcare Informatics","12 Dec 2013","2013","","","192","198","A privacy-enhanced video obscures the appearance of a person in the video. We consider four privacy enhancements: person blurred, person silhouetted, person covered with a bounding-oval, and person covered by a bounding-box. We demonstrate that privacy-enhanced video can be as accurate as raw video for eight in-home assistive monitoring goals: energy expenditure estimation, in room too long, leave but not return at night, arisen in morning, not arisen in morning, in region too long, abnormally inactive during day, and fall detection. Each monitoring goal's solution was trained using one actor and tested using two different actors. The privacy enhancements of silhouette, bounding-oval, and bounding-box, did not degrade achievement of the eight assistive monitoring goals. Raw video had a fidelity of 0.994 for the goal of energy expenditure estimation, while silhouette had 0.995, bounding-oval had 0.994, and bounding-box had 0.997. The fall detection algorithm yielded the same sensitivity of 0.91 and specificity of 0.92 for raw and bounding-oval video, while silhouette had a sensitivity of 0.91 and specificity of 0.75, and bounding-box had a sensitivity of 0.82 and specificity of 0.92. The other 6 goals yielded perfect sensitivity and specificity for raw and privacy-enhanced video, with the exception of blur video's sensitivity of 0.5 in region too long.","","978-0-7695-5089-3","10.1109/ICHI.2013.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680478","Assistive monitoring;smart homes;privacy-enhanced video;telehealth;embedded systems;ubiquitous systems","Cameras;Monitoring;Privacy;Streaming media;Estimation;Detectors;Sensitivity and specificity","assisted living;data privacy;object detection;video signal processing","automated in-home assistive monitoring;privacy-enhanced video;privacy enhancements;blurred person;silhouetted person;bounding-oval covered person;bounding-box covered person;energy expenditure estimation;fall detection algorithm;bounding-oval video;raw video;blur video sensitivity","","2","","26","","12 Dec 2013","","","IEEE","IEEE Conferences"
"Enhanced spatio-temporal video copy detection by combining trajectory and spatial consistency","S. Özkan; E. Esen; G. B. Akar","Image Processing Group, TUBITAK UZAY, Ankara, Turkey; Image Processing Group, TUBITAK UZAY, Ankara, Turkey; Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey","2014 IEEE International Conference on Image Processing (ICIP)","29 Jan 2015","2014","","","2527","2531","The recent improvements on internet technologies and video coding techniques cause an increase in copyright infringements especially for video. Frequently, image-based approaches appear as an essential solution due to the fact that joint usage of quantization-based indexing and weak geometric consistency stages give a capability to compare duplicate videos quickly. However, exploiting purely spatial content ignores the temporal variation of video. In this work, we propose a system that combines the state-of-the-art quantization-based indexing scheme with a novel trajectory-based geometric consistency on spatio-temporal features. This combination improves duplicate video matching task significantly. Briefly, spatial mean and variance of the trajectories are incorporated to establish a weak geometric consistency among pair of frames. To show the success of the proposed method, content-based video copy detection field is selected and TRECVID 2009 dataset is utilized. The experimental results show that constituting trajectory-based consistency on corresponding feature pairs outperforms the performances of merely utilizing spatiotemporal signature and visual signature with enhanced weak geometric consistency.","2381-8549","978-1-4799-5751-4","10.1109/ICIP.2014.7025511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7025511","spatio-temporal feature;trajecotory-based consistency;duplicate video search;video copy detection","Trajectory;Visualization;Feature extraction;Indexing;Estimation;Histograms;Vectors","image matching;indexing;object detection;video coding;video databases","spatiotemporal video copy detection enhancement;spatial consistency;trajectory consistency;Internet technologies;video coding techniques;weak geometric consistency stages;quantization-based indexing scheme;duplicate video matching;spatial mean;spatial variance;content-based video copy detection field;TRECVID 2009 dataset;spatiotemporal signature;visual signature","","","","19","","29 Jan 2015","","","IEEE","IEEE Conferences"
"Design and ARM-Embedded Implementation of a Chaotic Map-Based Real-Time Secure Video Communication System","Z. Lin; S. Yu; J. Lü; S. Cai; G. Chen","College of Automation, Guangdong University of Technology, Guangzhou, China; College of Automation, Guangdong University of Technology, Guangzhou, China; Academy of Mathematics and Systems Sciences, Chinese Academy of Sciences, Beijing, China; College of Automation, Guangdong University of Technology, Guangzhou, China; Department of Electronic Engineering, City University of Hong Kong, Hong Kong","IEEE Transactions on Circuits and Systems for Video Technology","30 Jun 2015","2015","25","7","1203","1216","A systematic methodology is proposed for a chaotic map-based real-time video encryption and decryption system with advanced Reduced Instruction Set Computer machine (ARM)-embedded hardware implementation. According to the anticontrol principle of dynamical systems, first, an 8-D discretetime chaotic map-based system is constructed, which possesses the required property of 1-1 surjection in the integer range [0, N - 1], where N is the number of frame pixels, suitable for position scrambling of each video frame. Then, an 8-D discretetime hyperchaotic system is designed for encryption-decryption of red, green, and blue (RGB) tricolor pixel values. Using the ARM-embedded platform super4412 model with Cortex-A9 processor, together with the standard QT cross-platform, an integrated chaotic map-based real-time secure video communication system is designed, implemented, and evaluated. In addition, the security performance of the designed system is tested using criteria from the National Institute of Standards and Technology statistical test suite. The main feature of this method is that, both scrambling-antiscrambling of RGB tricolor pixel positions and encryption-decryption of pixel values are realized simultaneously for enhancing the security. As is well known, compared with numerical simulations, hardware implementation for such a secure video communication system is very difficult to achieve, but we successfully implemented and tested in a real-world network environment. Both theoretical analysis and experimental results validate the feasibility and real-time performance of the new secure video communication system.","1558-2205","","10.1109/TCSVT.2014.2369711","National Science and Technology Major Project of China(grant numbers:2014ZX10004-001-014); 973 Project(grant numbers:2014CB845302); National Natural Science Foundation of China(grant numbers:61025017,61172023,61201392,11472290,61203148); Specialized Research Fund for the Doctoral Program of Higher Education through the Ministry of Education(grant numbers:20114420110003); General Research Fund through the Research Grants Council, Hong Kong(grant numbers:CityU 11201414); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6954438","Chaotic map;real-time secure video communication;ARM-embedded implementation;Advanced Reduced Instruction Set Computer (RISC) machine (ARM)-embedded implementation;chaotic map;real-time secure video communication","Streaming media;Chaotic communication;Encryption;Real-time systems;Hardware","chaotic communication;cryptography;image colour analysis;image resolution;microprocessor chips;real-time systems;statistical testing;telecommunication security;video communication;video signal processing","ARM-embedded hardware implementation;chaotic map-based real-time secure video communication system;chaotic map-based real-time video encryption system;chaotic map-based real-time video decryption system;advanced reduced instruction set computer machine;anti control principle;8D discrete time chaotic map-based system;1-1 surjection property;frame pixels;integer range;video frame;8D discrete time hyperchaotic system;red-green-and-blue tricolor pixel value;super4412 model;Cortex-A9 processor;QT cross-platform;National Institute of Standards and Technology statistical test suite","","75","","21","","12 Nov 2014","","","IEEE","IEEE Journals"
"Fast Palette Mode Decision Methods for Coding Game Videos With HEVC-SCC","Y. Liu; C. Fang; J. Sun; X. Huang","School of Microelectronics, Tianjin University, Tianjin, China; School of Microelectronics, Tianjin University, Tianjin, China; School of Microelectronics, Tianjin University, Tianjin, China; School of Electrical and Information School, Tianjin University, Tianjin, China","IEEE Transactions on Circuits and Systems for Video Technology","2 Oct 2019","2019","29","10","3061","3067","The live broadcasting of game playing is becoming more and more popular. The game video is one of the typical video data which should be coded with high efficiency video coding-screen content coding (HEVC-SCC). In HEVC-SCC, the palette mode is an important technique which can effectively enhance the coding performance. However, this technique also demands high-coding computation. In this paper, we propose a fast palette mode pre-decision method based on the analysis of the color complexity of the video data. Considering only the coding computation of palette mode of HEVC-SCC, our algorithm saves about 74.24% computation in the experiments of game videos. Whereas, the BDBR only increases by about 0.36% and the BDPSNR decreases by about 0.03 dB on average.","1558-2205","","10.1109/TCSVT.2018.2873700","National Natural Science Foundation of China(grant numbers:61771338,61671012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8481685","Game videos;palette mode;HEVC-SCC","Encoding;Videos;Image color analysis;Games;Indexes;Complexity theory;Standards","computer games;image colour analysis;video coding;video streaming","typical video data;high efficiency video coding-screen content coding;HEVC-SCC;coding performance;high-coding computation;fast palette mode pre-decision method;game video;fast palette mode decision methods;coding game videos;game playing","","1","","21","","4 Oct 2018","","","IEEE","IEEE Journals"
"Relay-Assisted Multiuser Video Streaming in Cognitive Radio Networks","Y. Xu; D. Hu; S. Mao","Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA; AT&T Laboratories, Inc., San Ramon, CA, USA; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA","IEEE Transactions on Circuits and Systems for Video Technology","30 Sep 2014","2014","24","10","1758","1770","Due to the drastic increase in wireless video traffic, the capacity of the existing and future wireless networks will be greatly stressed, while interference will become the dominant capacity-limiting factor. In this paper, we investigate relay-assisted downlink multiuser video streaming in a cognitive radio (CR) cellular network. We incorporate zero-forcing precoding to allow transmitters collaboratively send encoded (mixed) signals to all CR users, such that undesired signals will be canceled and the desired signal can be decoded at each CR user. We present a stochastic programming formulation of the problem, as well as a problem reformulation that greatly reduces computational complexity. In the cases of a single licensed channel and multiple licensed channels with channel bonding, we develop an optimal distributed algorithm with proven convergence and convergence speed. In the case of multiple channels without channel bonding, we develop a greedy algorithm with a proven performance bound. The algorithms are evaluated with simulations and are shown to achieve considerable gains over two heuristic schemes.","1558-2205","","10.1109/TCSVT.2014.2313898","National Science Foundation(grant numbers:CNS-0953513,CNS-1247955); NSF Broadband Wireless Access and Applications Center, Auburn University, Auburn, AL, USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6778761","Cognitive radio (CR);optimization;relay-enhanced cellular network;video streaming;zero forcing","Streaming media;Vectors;Sensors;Transmitters;Relays;Bonding;Wireless communication","cellular radio;cognitive radio;distributed algorithms;greedy algorithms;multiuser channels;precoding;relay networks (telecommunication);stochastic programming;telecommunication traffic;video coding;video streaming","relay-assisted multiuser video streaming;cognitive radio networks;wireless video traffic;downlink multiuser video streaming;cellular network;zero-forcing precoding;encoded signals;stochastic programming formulation;computational complexity;licensed channels;channel bonding;optimal distributed algorithm;convergence speed;greedy algorithm;heuristic schemes","","20","","40","","26 Mar 2014","","","IEEE","IEEE Journals"
"Segmentation-based motion compensation for enhanced video coding","S. Milani; G. Calvagno","Department of Information Engineering, University of Padova; Department of Information Engineering, University of Padova","2011 18th IEEE International Conference on Image Processing","29 Dec 2011","2011","","","1649","1652","Traditional video compression is based on block-based motion estimation and compensation. However, experimental results and the latest standards have proved that adapting the size and the shape of the motion estimation area to the objects in the scene significantly improves the performance of the overall coding process. The current paper presents a new segmentation-based coding strategy that partitions the input frame into irregularly-shaped regions. These segments are then used to estimate the motion-compensated prediction in the previous frames. The scheme outperforms the rate-distortion performance of H.264/AVC of 2 dB with a reasonable increment of the encoding complexity due to segmentation.","2381-8549","978-1-4577-1303-3","10.1109/ICIP.2011.6115769","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6115769","segmentation;video coding;object-oriented coding;motion estimation;H.264/AVC","Motion segmentation;Image segmentation;Image coding;Motion estimation;PSNR;Encoding;Vectors","data compression;image segmentation;motion compensation;video coding","segmentation-based motion compensation;enhanced video coding;video compression;block-based motion estimation;size;segmentation-based coding;irregularly-shaped region;motion-compensated prediction","","7","","9","","29 Dec 2011","","","IEEE","IEEE Conferences"
"Enhancing the Accessibility of Hearing Impaired to Video Content through Fully Automatic Dynamic Captioning","B. Mocanu; R. Tapu; T. Zaharia","Institute Mines - Télécom/Télécom SudParis,ARTEMIS Department,Evry,France; Institute Mines - Télécom/Télécom SudParis,ARTEMIS Department,Evry,France; Institute Mines - Télécom/Télécom SudParis,ARTEMIS Department,Evry,France","2019 E-Health and Bioengineering Conference (EHB)","28 Jan 2020","2019","","","1","4","In this paper we introduce an automatic subtitle positioning approach designed to enhance the video accessibility of deaf and hearing impaired people to multimedia documents. By using a dynamic subtitle and captioning approach, which exploits various computer vision techniques, including face detection, tracking and recognition, video temporal segmentation into shots and scenes and active speaker recognition, we are able to position each video subtitle segment in the near vicinity of the active speaker. The experimental evaluation performed on 30 video elements validates our approach with average F1-scores superior to 92%.","2575-5145","978-1-7281-2603-6","10.1109/EHB47216.2019.8970038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8970038","dynamic subtitle;active speaker recognition;deaf and hearing impaired people","","computer vision;face recognition;handicapped aids;image segmentation;speaker recognition;video signal processing","video content;fully automatic dynamic captioning;automatic subtitle positioning approach;video accessibility;impaired people;multimedia documents;dynamic subtitle;captioning approach;computer vision techniques;face detection;video temporal segmentation;active speaker recognition;video subtitle segment","","1","","19","","28 Jan 2020","","","IEEE","IEEE Conferences"
"Collaboratively Replicating Encoded Content on RSUs to Enhance Video Services for Vehicles","Y. Zhou; J. Chen; G. Ye; D. Wu; J. H. Wang; M. Chen","Department of Computing, Faculty of Science and Engineering, Macquarie University, Sydney, NSW, Australia; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Mobile Computing","4 Feb 2021","2021","20","3","877","892","With the development of smart cities, Internet services will be pervasively accessible for moving vehicles. It is envisioned that the video content demand of vehicles will explode in the near future. However, the strategy to efficiently distribute video content in large-scale vehicular networks is still absent due to challenges arising from the huge video population, heavy bandwidth consumption, heterogeneous user devices, and vehicles' mobility. In this work, we propose to collaboratively replicate video content on Roadside Units (RSUs) to enhance video distribution services based on the fact that the contact period between moving vehicles and a single RSU is not long enough to complete video downloading. In our design, a video file is split into multiple chunks. Each RSU replicates a small number of original chunks and chunks encoded by network coding. Replicating encoded chunks can reduce redundancy of chunks on different RSUs so that RSUs can complement each other better, whereas original chunks can be transrated to chunks with lower bitrates flexibly to fit in users' devices. Therefore, we replicate both original and encoded chunks on RSUs to take advantages of both sides. Stochastic models are employed to analyze chunk download processes and a convex optimization problem is formulated to determine the optimal partition of space allocated to each kind of chunks. Furthermore, we extend our strategy to support video streaming services and empirically prove that the influence caused by limitations of network coding is moderate. In the end, we conduct extensive simulations which not only validate the accuracy of our models but also demonstrate that our strategy can effectively boost video distribution services.","1558-0660","","10.1109/TMC.2019.2960022","ARC(grant numbers:DE180100950); National Natural Science Foundation of China(grant numbers:U1911201,U1705261); Guangdong Special Support Program(grant numbers:2017TX04X148); Fundamental Research Funds for the Central Universities(grant numbers:19LGZD37,19LGYJS57,19LGYJS58); Director Fund of WNLO; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8933474","Roadside unit;video file downloading;encoded chunks;vehicular networks","Streaming media;Network coding;Bit rate;Encoding;Mobile computing;Stochastic processes;Real-time systems","convex programming;Internet;network coding;smart cities;stochastic programming;vehicular ad hoc networks;video coding;video on demand;video streaming","video downloading;video file;network coding;video streaming services;video distribution services;Internet services;video content demand;large-scale vehicular networks;huge video population;heavy bandwidth consumption;heterogeneous user devices;roadside unit;stochastic models;convex optimization problem","","1","","43","IEEE","16 Dec 2019","","","IEEE","IEEE Journals"
"Inter-layer error concealment for scalable video coding based on motion vector averaging and slice interleaving","B. Zhao; E. J. Delp","Video and Image Processing Laboratory, School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana, USA; Video and Image Processing Laboratory, School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana, USA","2013 IEEE International Conference on Multimedia and Expo (ICME)","26 Sep 2013","2013","","","1","6","Scalable video coding (SVC) is desirable for video applications in heterogeneous environments where users have a variety of terminals. Because motion-compensated prediction is used in SVC codec, when SVC video is transmitted over loss-prone networks, the decompressed video can suffer severe visual degradation across multiple frames. In order to overcome this, one scheme utilized at the decoder is error concealment to enhance the visual quality. This paper assumes that the packet delivery of the base layer is loss-prone the same way as the enhancement layer. Three inter-layer error concealment methods are proposed using two new approaches. (1) Motion vector averaging using adaptively averaging over multiple types of motion vectors in different layers for the recovery of lost motion vectors. (2) Slice interleaving utilizing an optimum ordering technique to make the average distance between two contiguous slices as far as possible. A two-layer spatial-temporal scalable video coding system is developed to evaluate the existing and proposed error concealment methods. The impact of burst packet losses and error propagation on the frames in both layers is investigated. Experimental results verified that the proposed error concealment methods outperform two existing methods.","1945-788X","978-1-4799-0015-2","10.1109/ICME.2013.6607539","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607539","Error concealment;error resilience;scalable video coding;video compression","Vectors;Packet loss;Static VAr compensators;Video coding;Decoding;Visualization","motion compensation;video coding","interlayer error concealment method;two-layer spatial-temporal scalable video coding system;motion vector averaging;slice interleaving;SVC codec;motion-compensated prediction;SVC video;visual quality;packet delivery;base layer;enhancement layer;optimum ordering technique;contiguous slices;burst packet loss;error propagation","","4","","24","","26 Sep 2013","","","IEEE","IEEE Conferences"
"Effective Inter Transform Method Based on QTBT Structure for Future Video Coding","L. Wang; B. Niu; Y. He","Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China","2018 Picture Coding Symposium (PCS)","6 Sep 2018","2018","","","184","188","Transform, a crucial module for hybrid video coding framework, has been selecting Discrete Cosine Transform (DCT) for several decades. Recently, Singular Value Decomposition (SVD) and Enhanced Multiple Transform (EMT) are proposed to improve transform efficiency. However, the perspectives of SVD and EMT are different. SVD enhances transform efficiency by utilizing the similarity of prediction block and inter residual block. EMT adopts some new sinusoidal transform cores to accommodate the larger prediction errors closer to the boundary of prediction unit. In this paper, the proposed method mainly has two key contributions. First, SVD and EMT are combined skillfully. Second, non-square SVD is newly introduced to the original algorithm. By extensive experiments, averages 1.07%, 1.06% and 0.65% BD-rate saving for Y, U and V are achieved compared to JEM5.0.1 with some coding tools off, up to 5.87%, 4.28% and 4.47%.","2472-7822","978-1-5386-4160-6","10.1109/PCS.2018.8456266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8456266","SVD;EMT;RDO;Non-square transform;Video coding","Encoding;Indexes;Video coding;Discrete cosine transforms;Tools;Syntactics","discrete cosine transforms;singular value decomposition;video coding","inter transform method;video coding;QTBT structure;sinusoidal transform cores;Enhanced Multiple Transform;Singular Value Decomposition;Discrete Cosine Transform;hybrid video coding framework","","","","16","","6 Sep 2018","","","IEEE","IEEE Conferences"
"HDR Video Coding based on a temporally constrained Tone Mapping Operator","C. Ozcinar; P. Lauga; G. Valenzise; F. Dufaux","LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay, 75013, France; LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay, 75013, France; LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay, 75013, France; LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay, 75013, France","2016 Digital Media Industry & Academic Forum (DMIAF)","26 Sep 2016","2016","","","43","47","Given its potential for more realistic rendering and enhanced user experience, High Dynamic Range (HDR) imaging is raising a lot of interest both in industry and academia. In this context, efficient representation and coding techniques are needed, as HDR video entails significantly higher raw data rate. In this paper, we present a temporally constrained content-adaptive Tone Mapping Operator (TMO) in order to convert the input HDR video into a reduced bit depth video sequence which is then encoded using High Efficiency Video Coding (HEVC). As the proposed TMO simultaneously takes into account the statistical characteristics of the input frame while better preserving temporal coherence of the tone mapped video sequence, it leads to improved coding efficiency. Experimental results show that the proposed technique compares favorably with existing methods in terms of rate-distortion when using the HDR-VDP-2.2.1 quality metric.","","978-1-5090-1000-4","10.1109/DMIAF.2016.7574900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574900","High dynamic range;video coding;tone mapping;convex optimization","Video coding;Encoding;Bit rate;Video sequences;Coherence;Rate-distortion;Convex functions","image representation;image sequences;video coding","high dynamic range imaging;HDR video coding;content adaptive tone mapping operator;TMO;user experience;image representation;image coding techniques;tone mapped video sequence;high efficiency video coding;HEVC;HDR-VDP-2.2.1","","5","","14","","26 Sep 2016","","","IEEE","IEEE Conferences"
"High Definition Video Packet Scheduling Algorithms for IEEE802.11ac Networks to Enhance QoE","S. Nosheen; J. Y. Khan","The University of Newcastle,School of Electrical Engineering and Computing,Callaghan,NSW,Australia,2280; The University of Newcastle,School of Electrical Engineering and Computing,Callaghan,NSW,Australia,2280","2020 IEEE 91st Vehicular Technology Conference (VTC2020-Spring)","30 Jun 2020","2020","","","1","5","The prevalence of video traffic services over IEEE 802.11 networks increased the incentive for service providers to improve the video quality of experience (QoE). Delivering high quality video on a random access network is a challenging task due to the use of non-deterministic resource allocation techniques. High definition (HD) video services require enhanced viewing quality with similar QoE for all video users in a network. In this paper, we propose two video packet scheduling algorithms for the 802.11 MAC protocol which can offer high QoS for all video streams as well as maintaining fairness among these video streams. We propose two algorithms to achieve above objectives. The first algorithm named FRA-TXOP allocates appropriate network resources to video terminals by identifying congested terminals. The second algorithm is the EAS that works in combination with the FRA-TXOP to offer inter-user fairness in a networks. A NS-3 simulation model was developed to evaluate the performance of proposed algorithms, and compare proposed algorithms performance with the Bi-level algorithm and the standard IEEE 802.11ac MAC protocol. Simulation results demonstrate that the proposed algorithms can improve the perceived video quality and inter-user fairness for all video users in a network.","2577-2465","978-1-7281-5207-3","10.1109/VTC2020-Spring48590.2020.9128873","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9128873","IEEE802.11ac;HD video stream;TXOP;Resource Allocation;Inter-service fairness","Streaming media;Delays;Quality of experience;Quality of service;Throughput;Resource management;Standards","access protocols;high definition video;quality of experience;resource allocation;telecommunication scheduling;telecommunication traffic;video communication;video streaming;wireless LAN","random access network;nondeterministic resource allocation techniques;high definition video services;video streams;FRA-TXOP;Bi-level algorithm;IEEE 802.11ac MAC protocol;high definition video packet scheduling algorithms;QoE;video traffic services;NS-3 simulation model;quality of experience","","1","","12","","30 Jun 2020","","","IEEE","IEEE Conferences"
"Retina-enhanced SURF descriptors for semantic concept detection in videos","S. T. Strat; A. Benoit; P. Lambert; A. Caplier","LISTIC - Université de Savoie, Annecy Le Vieux, France; LISTIC - Université de Savoie, Annecy Le Vieux, France; LISTIC - Université de Savoie, Annecy Le Vieux, France; Gipsa-Lab - Université de Grenoble, St Martin d'Hères, France","2012 3rd International Conference on Image Processing Theory, Tools and Applications (IPTA)","25 Feb 2013","2012","","","319","324","This paper proposes to investigate the potential benefit of the use of low-level human vision behaviors in the context of high-level semantic concept detection. A large part of the current approaches relies on the Bag-of-Words (BoW) model, which has proven itself to be a good choice especially for object recognition in images. Its extension from static images to video sequences exhibits some new problems to cope with, mainly the way to use the added temporal dimension for detecting the target concepts (swimming, drinking...). In this study, we propose to apply a human retina model to preprocess video sequences, before constructing a State-Of-The-Art BoW analysis. This preprocessing, designed in a way that enhances the appearance especially of static image elements, increases the performance by introducing robustness to traditional image and video problems, such as luminance variation, shadows, compression artifacts and noise. These approaches are evaluated on the TrecVid 2010 Semantic Indexing task datasets, containing 130 high-level semantic concepts. We consider the well-known SURF descriptor as the entry point of the BoW system, but this work could be extended to any other local gradient based descriptor.","2154-512X","978-1-4673-2584-4","10.1109/IPTA.2012.6469557","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6469557","Bag of words;Retina analysis;Retina preprocessing;Semantics;SURF;Video content;Video indexation","Retina;Videos;Feature extraction;Semantics;Vocabulary;Visualization;Noise","computer vision;data compression;eye;gradient methods;image sequences;object detection;object recognition;video signal processing","retina-enhanced SURF descriptor;low-level human vision behavior;high-level semantic concept detection;BoW model;object recognition;static image;video sequence;temporal dimension;human retina model;image problem;video problem;luminance variation;shadow;compression artifact;noise;TrecVid 2010 semantic indexing task dataset;local gradient based descriptor;bag-of-words model","","1","","17","","25 Feb 2013","","","IEEE","IEEE Conferences"
"Influence of block size on motion vector estimation error in enhancement of video temporal resolution","D. Vranješ; S. Rimac-Drlje; M. Vranješ","Faculty of Electrical Engineering, Computer Science and Information Technology Osijek, Josip Juraj Strossmayer University of Osijek, Osijek, Croatia; Faculty of Electrical Engineering, Computer Science and Information Technology Osijek, Josip Juraj Strossmayer University of Osijek, Osijek, Croatia; Faculty of Electrical Engineering, Computer Science and Information Technology Osijek, Josip Juraj Strossmayer University of Osijek, Osijek, Croatia","2017 International Conference on Smart Systems and Technologies (SST)","14 Dec 2017","2017","","","263","267","Modern video recording devices record high resolution video material, and for efficient storage and transmission the video file size should be reduced by encoding. Depending on the content, decreasing the temporal or spatial resolution of a video signal before the encoding and increasing after the decoding can ensure higher sequence quality than compression of the material with original resolution. Before displaying the video material with the decreased resolution on a modern high resolution displays the resolution of the transmitted video signal should be enhanced in order to satisfy the users' perceived quality. Temporal resolution can be increased by interpolating of new frames between the existing frames in the downscaled sequence. The best quality of the interpolated frames can be achieved by using motion compensated frame interpolation (MCFI). The first step in MCFI is motion estimation (ME). In ME process the existing frames are divided into blocks, and motion vectors (MV) are calculated for each block. These MV are base for the estimation of the motion vectors between existing and frames that will be interpolated. The influence of the block size in the ME process on the estimated motion vector error is analyzed in this paper. The results show that in the interpolation process MV estimation error rises as the block dimensions decrease.","","978-1-5386-2101-1","10.1109/SST.2017.8188707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8188707","temporal resolution increasing;frame interpolation;motion estimation;motion vector probability error;block size","Spatial resolution;Signal resolution;Estimation error;Interpolation;Complexity theory;Video sequences","data compression;image resolution;interpolation;motion compensation;motion estimation;video coding;video recording;video signal processing","spatial resolution;encoding;modern high resolution;transmitted video signal;users;interpolated frames;motion compensated frame interpolation;block size;estimated motion vector error;motion vector estimation error;video temporal resolution;modern video recording devices record high resolution video material;block dimensions;sequence quality;video file size","","","","17","","14 Dec 2017","","","IEEE","IEEE Conferences"
"Robust candidate frame detection in videos using semantic content modeling","T. Manonmani; K. Mala","Department of Computer Science & Engineering, Kamaraj College of Engineering & Technology, Virudhunagar, India; Department of Computer Science & Engineering, Mepco Shlenk Engineering College, Sivakasi, India","2014 International Conference on Communication and Network Technologies","19 Mar 2015","2014","","","281","285","Videos are of the most popular rich media formats carrying large amount of visual, audio and textual information. In recent years people all over the world show great interest in video mining to extract meaningful patterns and knowledge to enhance the smart level of video applications. In this work Speeded Up Robust Features (SURF) are used to detect the candidate frames among the set of key frames extracted from a video content. By eliminating the presence of duplicate key frames the computational and time complexity of processing a large number of frames is reduced. From the identified candidate frames semantic objects with meaningful content are extracted which improves the efficiency of video mining applications like Video recommendation systems, Video concept detection etc. Experimental results show that the proposed approach eliminates the duplicate frames without a prior knowledge of the video content.","","978-1-4799-6266-2","10.1109/CNT.2014.7062770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7062770","Object discovery;Candidate frames;Semantic objects;Video recommendation","Videos;Semantics;Feature extraction;Histograms;Image color analysis;Robustness;Visualization","computational complexity;data mining;feature extraction;object detection;recommender systems;video signal processing","robust candidate frame detection;semantic content modeling;textual information;audio information;visual information;pattern extraction;knowledge extraction;video applications;speeded up robust features;SURF;key frame extraction;video content;time complexity;video mining applications;video recommendation systems;video concept detection","","","","15","","19 Mar 2015","","","IEEE","IEEE Conferences"
"Enhancing Table-Tennis Learning Outcome Using Video-Based Self-Recording During Play and Practice","T. Sookhanaphibarn; K. Sookhanaphibarn","Faculty of Applied Arts, King Mongkut’s University of Technology North Bangkok, Bangkok, Thailand; BU-Multimedia Intelligent Technology Laboratory, School of Information Technology and Innovation Bangkok University, Thailand","2018 IEEE 7th Global Conference on Consumer Electronics (GCCE)","13 Dec 2018","2018","","","647","648","This paper presented the learning experience supplementary program using video clips to teach table tennis for enhancing learning outcome of undergraduate students. The video clips were taken by students during their practices. The study type is an experimental research, and the purpose of this study was to investigate the patterns of video clips that are required to learn the table tennis course and compare the effectiveness of two table tennis instruction methods-conventional method and using video clips. The findings from the study indicate, the most important table tennis skill is bounce the ball, knock board are basic skills and fore hand, back hand rally with partner are higher skills before developing advanced skills. The necessary format of table tennis video clips skills includes: 1) bounce the ball fore and back switching, 2) knock board rally, 3) fore hand rally with partner and 4) back hand rally with partner.","2378-8143","978-1-5386-6309-7","10.1109/GCCE.2018.8574664","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8574664","Selfie;Self-recording;Self-observation;Video Clips;Physical Education;Coaching","Sports;Education;Switches;Art;Information technology;Technological innovation","computer aided instruction;educational courses;further education;sport;teaching;video recording","table-tennis learning outcome;video-based self-recording;table tennis instruction methods;table tennis skill;table tennis video clips;table tennis course;undergraduate students;learning experience supplementary program","","","","4","","13 Dec 2018","","","IEEE","IEEE Conferences"
"Enhancing video concept detection with the use of tomographs","P. Sidiropoulos; V. Mezaris; I. Kompatsiaris","Information Technologies Institute/Centre for Research and Technology Hellas 6th Km Charilaou-Thermi Road, P.O. BOX 60361, Thermi 57001, Greece; Information Technologies Institute/Centre for Research and Technology Hellas 6th Km Charilaou-Thermi Road, P.O. BOX 60361, Thermi 57001, Greece; Information Technologies Institute/Centre for Research and Technology Hellas 6th Km Charilaou-Thermi Road, P.O. BOX 60361, Thermi 57001, Greece","2013 IEEE International Conference on Image Processing","13 Feb 2014","2013","","","3991","3995","In this work we deal with the problem of video concept detection, for the purpose of using the detection results towards more effective concept-based video retrieval. In order to handle this task, we propose using spatio-temporal video slices, called video tomographs, in the same way that visual keyframes are typically used in traditional keyframe-based video concept detection schemes. Video tomographs capture in a compact way motion patterns that are present in the video, and are used in this work for training a number of base detectors. The latter augment the set of keyframe-based base detectors that can be trained on different image representations. Combining the keyframe-based and tomograph-based detectors, improved concept detection accuracy can be achieved. The proposed approach is evaluated on a dataset that is extensive both in terms of video duration and concept variation. The experimental results manifest the merit of the proposed approach.","2381-8549","978-1-4799-2341-0","10.1109/ICIP.2013.6738822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6738822","video analysis;supervised learning;support vector machines;video tomograph;concept detection","","image enhancement;image motion analysis;image representation;object detection;video retrieval","video concept detection enhancement;concept-based video retrieval;spatio-temporal video slices;video tomographs;visual keyframes;keyframe-based video concept detection schemes;compact way motion patterns;base detectors;image representations;tomograph-based detectors;video duration;video concept variation","","8","","25","","13 Feb 2014","","","IEEE","IEEE Conferences"
"ES&IS: Enhanced Spread and Iterative Search hardware-friendly motion estimation algorithm for the HEVC Standard","G. Sanchez; B. Zatt; M. Porto; L. Agostini","Group of Architectures and Integrated Circuits - GACI, Federal University of Pelotas - UFPel, Pelotas, Brazil; Group of Architectures and Integrated Circuits - GACI, Federal University of Pelotas - UFPel, Pelotas, Brazil; Group of Architectures and Integrated Circuits - GACI, Federal University of Pelotas - UFPel, Pelotas, Brazil; Group of Architectures and Integrated Circuits - GACI, Federal University of Pelotas - UFPel, Pelotas, Brazil","2013 IEEE 20th International Conference on Electronics, Circuits, and Systems (ICECS)","15 May 2014","2013","","","941","944","In this work the hardware-friendly Enhanced Spread and Iterative Search (ES&IS) motion estimation (ME) algorithm is presented and evaluated for the High Efficiency Video Coding Standard (HEVC). The ES&IS is divided in two steps: a central evaluation and a spread evaluation with a refinement. The ES&IS was compared with Enhanced Predictive Zonal Search (EPZS) using the HEVC reference software. Software evaluations showed that, in average, the ES&IS presents a 1.07% increase in bitrate and 0.05% decrease in PSNR when compared to EPZS. However, the main advantage of the ES&IS is its hardware-friendly aspect.","","978-1-4799-2452-3","10.1109/ICECS.2013.6815567","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6815567","Motion Estimation Algorithm;HEVC;Video Coding;Block Matching Algorithm","Algorithm design and analysis;Vectors;Prediction algorithms;Computer architecture;Software algorithms;Video coding;Bit rate","code standards;iterative methods;motion estimation;search problems;video coding","HEVC standard;enhanced spread and iterative search;ES&IS hardware friendly motion estimation algorithm;high efficiency video coding;central evaluation;spread evaluation;software evaluation","","1","3","8","","15 May 2014","","","IEEE","IEEE Conferences"
"Trunk-Branch Ensemble Convolutional Neural Networks for Video-Based Face Recognition","C. Ding; D. Tao","School of Electronic and Information Engineering, South China University of Technology, Tianhe District, Guangzhou, P.R. China; UBTech Sydney Artificial Intelligence Institute and the School of Information Technologies in the Faculty of Engineering and Information Technologies, The University of Sydney, Darlington, NSW, Australia","IEEE Transactions on Pattern Analysis and Machine Intelligence","5 Mar 2018","2018","40","4","1002","1014","Human faces in surveillance videos often suffer from severe image blur, dramatic pose variations, and occlusion. In this paper, we propose a comprehensive framework based on Convolutional Neural Networks (CNN) to overcome challenges in video-based face recognition (VFR). First, to learn blur-robust face representations, we artificially blur training data composed of clear still images to account for a shortfall in real-world video training data. Using training data composed of both still images and artificially blurred data, CNN is encouraged to learn blur-insensitive features automatically. Second, to enhance robustness of CNN features to pose variations and occlusion, we propose a Trunk-Branch Ensemble CNN model (TBE-CNN), which extracts complementary information from holistic face images and patches cropped around facial components. TBE-CNN is an end-to-end model that extracts features efficiently by sharing the low- and middle-level convolutional layers between the trunk and branch networks. Third, to further promote the discriminative power of the representations learnt by TBE-CNN, we propose an improved triplet loss function. Systematic experiments justify the effectiveness of the proposed techniques. Most impressively, TBE-CNN achieves state-of-the-art performance on three popular video face databases: PaSC, COX Face, and YouTube Faces. With the proposed techniques, we also obtain the first place in the BTAS 2016 Video Person Recognition Evaluation.","1939-3539","","10.1109/TPAMI.2017.2700390","Australian Research Council(grant numbers:FT-130101457,DP-140102164); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917252","Video-based face recognition;video surveillance;blur- and pose-robust representations;convolutional neural networks","Videos;Face;Face recognition;Feature extraction;Training data;Training;Surveillance","face recognition;feature extraction;image representation;learning (artificial intelligence);neural nets;video signal processing","face recognition;human faces;surveillance videos;comprehensive framework;blur-robust face representations;real-world video training data;artificially blurred data;blur-insensitive features;CNN features;holistic face images;end-to-end model;middle-level convolutional layers;branch networks;popular video face databases;YouTube faces;feature extraction;PaSC;COX face;BTAS 2016 video person recognition evaluation;trunk-branch ensemble convolutional neural networks;trunk-branch ensemble CNN model;low-level convolutional layers;TBE-CNN","Algorithms;Biometric Identification;Databases, Factual;Face;Humans;Image Processing, Computer-Assisted;Neural Networks, Computer;Pattern Recognition, Automated;Video Recording","67","","66","","2 May 2017","","","IEEE","IEEE Journals"
"Factors Affecting Enhanced Video Quality Preferences","P. N. Satgunam; R. L. Woods; P. M. Bronstad; E. Peli","Hyderabad Eye Research Foundation, L. V. Prasad Eye Institute, Hyderabad, India; Harvard Medical School, Schepens Eye Research Institute, Boston, MA, USA; Harvard Medical School, Schepens Eye Research Institute, Boston, MA, USA; Harvard Medical School, Schepens Eye Research Institute, Boston, MA, USA","IEEE Transactions on Image Processing","4 Oct 2013","2013","22","12","5146","5157","The development of video quality metrics requires methods for measuring perceived video quality. Most of these metrics are designed and tested using databases of images degraded by compression and scored using opinion ratings. We studied video quality preferences for enhanced images of normally-sighted participants using the method of paired comparisons with a thorough statistical analysis. Participants (n=40) made pair-wise comparisons of high definition video clips enhanced at four different levels using a commercially available enhancement device. Perceptual scales were computed with binary logistic regression to estimate preferences for each level and to provide statistical inference of the differences among levels and the impact of other variables. While moderate preference for enhanced videos was found, two unexpected effects were also uncovered: 1) participants could be broadly classified into two groups: a) those who preferred enhancement (“Sharp”) and b) those who disliked enhancement (“Smooth”) and 2) enhancement preferences depended on video content, particularly for human faces to be enhanced less. The results suggest that algorithms to evaluate image quality (at least for enhancement) may need to be adjusted or applied differentially based on video content and viewer preferences. The possible impact of similar effects on image quality of compressed video needs to be evaluated.","1941-0042","","10.1109/TIP.2013.2282120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605552","Television watching;image enhancement;image quality","Image quality;HDTV;Measurement;Logistics;Video recording;Quality assessment;Image coding","data compression;high definition video;image enhancement;regression analysis;statistical analysis;video databases;visual perception","image enhancement;video compression;image quality evaluation;statistical inference;binary logistic regression;perceptual scale;high definition video clip;statistical analysis;image compression;opinion rating;image database;video quality measurement;video quality metrics;video quality preference enhancement","","10","","56","","20 Sep 2013","","","IEEE","IEEE Journals"
"Joint antenna allocation and rate adaption for video transmission in massive MIMO systems","B. Liu; H. Zhang; H. Ji; X. Li; K. Wang","Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, 100876, China; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, 100876, China; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, 100876, China; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, 100876, China; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, 100876, China","2016 Digital Media Industry & Academic Forum (DMIAF)","26 Sep 2016","2016","","","77","82","Massive multi-input-multi-output (MIMO) networks could achieve higher data transmission rate benefited from the advantages of space diversity and multiplexing. In recent years, large amounts of research about different service adopted in massive MIMO network have been proposed. In this paper, we investigate instant video communication services requested by users in massive MIMO networks. After defining a detailed system model for video streaming in massive MIMO networks, we jointly consider the problem of antenna allocation and time-average video streaming scheduling. Since the problem is NP-hard, we reformulate it by decomposing the problem into two sub-problems that are antennas allocation and video packets queuing so that some fast common algorithms can be employed. To solve the two sub-problems, Enhanced Hungarian algorithm (EHA) and Enhanced Kuhn-Munkras algorithm (EKM) are designed for antenna allocation, and High Quality Fair Queuing (HQFQ) algorithm is proposed for video streaming scheduling. Consequently, numerical solution can be calculated in the time scale of real-life video streaming sessions. Various results demonstrate that our approach performs well in balance of quality of service and fairness to video streaming users.","","978-1-5090-1000-4","10.1109/DMIAF.2016.7574906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574906","Massive multi-input-multi-output (Massive MIMO) network;Video communication;Quality of service (QoS);Antenna allocation;Queuing algorithm","Streaming media;MIMO;Resource management;Antennas;Quality of service;Video recording;Quality assessment","antenna arrays;computational complexity;MIMO communication;optimisation;quality of service;queueing theory;space division multiplexing;telecommunication scheduling;video communication;video streaming","massive MIMO systems;joint antenna allocation;rate adaption;video transmission;massive multiinput-multioutput networks;MIMO networks;space diversity;space multiplexing;instant video communication services;time-average video streaming scheduling;NP-hard problem;video packets queuing;enhanced Hungarian algorithm;EHA;enhanced Kuhn-Munkras algorithm;EKM;high quality fair queuing algorithm;HQFQ algorithm;quality of service","","1","","13","","26 Sep 2016","","","IEEE","IEEE Conferences"
"Quality of Service Enhancement for Multimedia Applications Using Scalable Video Coding","M. Vandana; H. D. Kallinatha","Siddaganga Institute of Technology, Department of Computer Science, Tumkur, India; Siddaganga Institute of Technology, Department of Computer Science, Tumkur, India","2018 Second International Conference on Intelligent Computing and Control Systems (ICICCS)","10 Mar 2019","2018","","","394","399","Streaming of media data is the most rapid-growth network service. On the other end there are vast numbers of application users to access the service. Supporting end-to-end Quality of Service (QoS) is the real-time issue for the application administrators. For an application, safeguarding the QoS in streaming the video records from source to destination in a network is must. Representing client needs and reaching their desire for video service through application is essential as the clients expect greater administration quality. Henceforth the applications ought to be constantly reachable and responded at a quicker rate. This paper presents an approach for enhancing Quality of Service (QoS) over the network for streaming the media data in two phases: Firstly, Considering the type of resolution of the device to which the application user is connected. Secondly, the transmission capacity the client is associated. To preserve the Quality of Experience (QoE), the proposed solution aims at streaming only the file format the client device supports, taking into account the type of resolution the client device is having (this prevents the client to manually select the different file formats eg: Youtube). To achieve the streaming of different file formats Scalable Video Coding (SVC) technique is used which increases the QoS by preventing the packet loss and delay in the service. SVC is an expansion of (MPEG-4 AVC) video compression standard for video encoding. It is a standard that packs the video information which has the subsets (different formats) of video, this comprises of video designs (of various pixels). These are piled up in a single document and can be placed in the single server. Here we are decreasing the server overhead (different formats on different servers) by placing it on a single server and delivering the requested video recordings to the client at a faster rate. The experimental results shows that the application enables multiple devices to access the service at faster rate and accomplish with high quality video service without giving up the execution.","","978-1-5386-2842-3","10.1109/ICCONS.2018.8663244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663244","Quality of Service (QoS);Video Coding;Scalable Video Coding(SVC)","Streaming media;Quality of service;Video coding;Servers;Bandwidth;Static VAr compensators;Media","data compression;quality of service;video coding;video recording","QoS;video encoding;video information;video designs;high quality video service;multimedia applications;video records;scalable video coding technique;video recordings;quality of service enhancement;SVC technique;MPEG-4 AVC video compression standard","","1","","13","","10 Mar 2019","","","IEEE","IEEE Conferences"
"Design and realization of embedded network video surveillance system based on the DM365","H. Li; B. Xue; C. Li; D. Chen; W. Song","Jiangsu Teachers University of Technology, Changzhou, China; Jiangsu Teachers University of Technology Changzhou, China; Jiangsu Teachers University of Technology Changzhou, China; Jiangsu Teachers University of Technology Changzhou, China; Jiangsu Teachers University of Technology Changzhou, China","2012 7th International Conference on Computer Science & Education (ICCSE)","6 Sep 2012","2012","","","700","703","The paper adopts the DM365 video processing chip to realize an embedded network video surveillance system, and selects highly efficient video coding standard - H.264 compression algorithm to enhance the efficiency of coding data. The video monitoring system software includes video monitoring server module and client-side monitoring module. The video surveillance server module running on DM365 platform captures video frame and encodes it using H.264 encoder, and writes the encoded frame to the hard disk and sends it to the client-side surveillance using the RTP/RTCP protocol. The client-side monitoring program running on the PC real-time controls and monitors the scene.","","978-1-4673-0242-5","10.1109/ICCSE.2012.6295171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6295171","DM365;video surveillance system;H.264;streaming media","Streaming media;Video surveillance;Servers;Instruction sets;Real time systems;Encoding","digital signal processing chips;embedded systems;video coding;video surveillance","embedded network video surveillance system;DM365 video processing chip;video coding standard;H.264 compression algorithm;data coding;video monitoring system software;client side monitoring module;video monitoring server module;hard disk;RTCP protocol","","2","","7","","6 Sep 2012","","","IEEE","IEEE Conferences"
"Real Time Pedestrian Detection Using Robust Enhanced Tiny-YOLOv3","C. B. Murthy; M. Farukh Hashmi","National Institute of Technology, Warangal,Electronics & Communication Engineering,Telangana,India; National Institute of Technology, Warangal,Electronics & Communication Engineering,Telangana,India","2020 IEEE 17th India Council International Conference (INDICON)","5 Feb 2021","2020","","","1","5","One of the key components in developing smart cities in countries like India, autonomous pedestrian detection plays a very crucial role in Computer Vision tasks such as smart video surveillance and intelligent traffic monitoring system. In self-driving cars, real time performance is required without scarifying the detection accuracy, while detecting smaller pedestrians. In the proposed paper, a robust Enhanced Tiny-Yolov3 Network is designed by introducing an anti-residual module which helps to improve network's feature extraction ability. Second, the loss function is improved which reduces bounding box loss error and optimizes the network. Third one prediction scale of size 26 x 26 is removed from the Tiny-Yolov3 network so the computational complexity of the network is reduced. The proposed network is trained on the extracted pedestrian images from Pascal Voc-2007 dataset. Experimental results show this network improves the detection accuracy while detecting smaller pedestrians and it still meets the real-time requirements.","2325-9418","978-1-7281-6916-3","10.1109/INDICON49873.2020.9342082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9342082","Anti-residual Module;Computer Vision (CV);Enhanced Tiny-Yolov3 Network;Pedestrian Detection","Training;Smart cities;Feature extraction;Video surveillance;Real-time systems;Task analysis;Monitoring","computer vision;feature extraction;image classification;object detection;pedestrians;traffic engineering computing;video surveillance","time pedestrian detection;smart cities;autonomous pedestrian detection;Computer Vision tasks;smart video surveillance;intelligent traffic monitoring system;self-driving cars;detection accuracy;smaller pedestrians;robust Enhanced Tiny-Yolov3 Network;anti-residual module;feature extraction ability;loss function;box loss error;computational complexity;extracted pedestrian images;real-time requirements","","","","16","","5 Feb 2021","","","IEEE","IEEE Conferences"
"Joint Coding-Transmission Optimization for a Video Surveillance System With Multiple Cameras","M. Wang; B. Cheng; C. Yuen","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Engineering Product Development Pillar, Singapore University of Technology and Design, Singapore","IEEE Transactions on Multimedia","14 Feb 2018","2018","20","3","620","633","Surveillance video has become an important multi-media application in recent years, with development trends toward wide viewing angles and high definition. At the same time, transmission of surveillance video over the Internet and storage in the cloud are becoming increasingly popular. However, it is extremely challenging to transfer surveillance video with both a wide viewing angle and a high resolution over the Internet because of 1) the tradeoff between a wide viewing angle and high-surveillance object resolution for a single camera, 2) the large throughput requirement, and 3) the fluctuating delay and bandwidth of the Internet. In this paper, we present a reference-frame-cache-based surveillance video transmission system (RSVTS), which delivers wide-viewing-angle and high-definition surveillance video over the Internet in real time using multiple rotatable cameras. First, we develop an efficient video mosaicing method for merging two (or more) surveillance videos together. Second, novel reference frame selection and update algorithms are proposed for the RSVTS encoder to reduce the amount of encoded data. Third, we implement a reference frame cache on both the sender and receiver sides to increase video quality by increasing the probability of video decoding. We conduct a performance evaluation using a simulation system that integrates RSVTS and ns-3. Compared with the H.264/SVC and H.264/AVC schemes, RSVTS achieves appreciable improvements in enhancing the video peak signal-to-noise ratio and bandwidth conservation.","1941-0077","","10.1109/TMM.2017.2748459","National High-tech Research and Development Program of China (863 Program)(grant numbers:2013AA102301); National Research Foundation Singapore; Interactive Digital Media (IDM) Strategic Research Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8024017","High definition;real time;surveillance video transmission system;throughput;video mosaicing;wide viewing angle","Streaming media;Encoding;Surveillance;Internet;Real-time systems;Cameras;Video coding","cache storage;cameras;object detection;optimisation;probability;video coding;video surveillance","joint coding-transmission optimization;video surveillance system;wide viewing angle;high-surveillance object resolution;high-definition surveillance video;efficient video mosaicing method;video quality;video decoding;video peak signal-to-noise ratio;multimedia application;Internet;large throughput requirement;fluctuating delay;fluctuating bandwidth;reference-frame-cache-based surveillance video transmission system;multiple rotatable cameras;RSVTS encoder;bandwidth conservation;video decoding probability;reference frame update algorithm;reference frame selection algorithm","","8","","47","","1 Sep 2017","","","IEEE","IEEE Journals"
"Video Multicast: Integrating Scalability of Soft Video Delivery Systems Into NOMA","J. Wu; B. Tan; J. Wu; M. Wang","College of Electronics and Information Engineering, Tongji University, Shanghai, China; College of Electronics and Information Engineering, Jinggangshan University, Ji’an, China; Key Laboratory of Ministry of Education in Embedded System and Service Computing, College of Electronics and Information Engineering, Tongji University, Shanghai, China; School of Mathematics and Computer Science, Gannan Normal University, Ganzhou, China","IEEE Wireless Communications Letters","8 Dec 2019","2019","8","6","1722","1726","Non-orthogonal Multiple Access (NOMA) has aroused research interest for its potential use in future wireless video multicast. By exploring the power domain, multiple video layers are superposed to achieve video delivery scalability and efficiency in NOMA. However, the scalability of this approach is still limited despite adopting medium-grain quality scalable (MGS) video coding. In this letter, we propose a scalable multicast design inspired by soft video delivery systems. Soft video delivery systems cannot be directly applied in NOMA networks to improve the scalability because they cannot achieve errorless decoding when utilizing linear least square estimation (LLSE) at the decoder. If there is decoding error in the former layers, the successive interference cancellation (SIC) in NOMA will give rise to error propagation and huge performance loss. To solve this problem, we propose a design with digitalized base layer and soft encoded enhanced layer. The power allocation is formulated as a distortion minimization problem. The experimental results demonstrate that the proposed design has significant performance gain over SoftCast, SupCast and the traditional digital system.","2162-2345","","10.1109/LWC.2019.2939129","National Natural Science Foundation of China(grant numbers:61762053,61601128,61831018,61631017); Guangdong Province Key Research and Development Program Major Science and Technology Projects(grant numbers:2018B010115002); Tongji University(grant numbers:ESSCKF2018-06); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823962","Soft video delivery;NOMA;wireless video transmission","NOMA;Decoding;Streaming media;Silicon carbide;Distortion;Encoding;Resource management","interference suppression;least squares approximations;multi-access systems;multicast communication;video coding","soft encoded enhanced layer;soft video delivery systems;NOMA networks;multiple video layers;video delivery scalability;medium-grain quality scalable video coding;scalable multicast design;wireless video multicast;nonorthogonal multiple access;MGS video coding;errorless decoding;successive interference cancellation;error propagation;digitalized base layer;power allocation;distortion minimization problem;SoftCast;SupCast;digital system","","2","","12","IEEE","4 Sep 2019","","","IEEE","IEEE Journals"
"Predictive Enhanced Modified Orthogonal Search algorithm for wireless video sensor network","Zhuge Yan; S. Welsen","Department of Electrical and Electronic Engineering, University of Nottingham, Ningbo, China; Department of Electrical and Electronic Engineering, University of Nottingham, Ningbo, China","2016 Asia Pacific Conference on Multimedia and Broadcasting (APMediaCast)","16 Mar 2017","2016","","","60","65","A wireless multimedia sensor network (WMSN) requires a video encoding system that should be energy-efficient because of its special characteristics: limited power source but long service life without maintenance. Motion estimation plays an important role in predictive coding, which is the key part of video encoding and requires large amount of calculation. In order to reduce the computational complexity of motion estimation, the block-matching search algorithm must be able to find the motion vectors using fewer search points. This paper introduces a search algorithm designed for WMSNs. It adds predictive search technique to the enhanced modified orthogonal search (EMOS) algorithm, which improves the efficiency of block-matching search algorithms, and has two self-adapting search patterns for large and small movements. The proposed algorithm requires less search points to work out the movement of blocks and provides acceptable image quality.","","978-1-4673-9791-9","10.1109/APMediaCast.2016.7878172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7878172","block-matching;video processing;compression;wireless multimedia sensor network","Prediction algorithms;Multimedia communication;Algorithm design and analysis;Motion estimation;Streaming media;Wireless sensor networks;Image quality","motion estimation;search problems;video coding;wireless sensor networks","wireless multimedia sensor network;WMSN;video encoding system;motion estimation;predictive coding;computational complexity;block-matching search algorithm;motion vectors;predictive search technique;enhanced modified orthogonal search algorithm;EMOS algorithm;self-adapting search patterns;image quality","","","","16","","16 Mar 2017","","","IEEE","IEEE Conferences"
"Cluster based V2V communications for enhanced QoS of SVC video streaming over vehicular networks","E. Yaacoub; F. Filali","Qatar Mobility Innovations Center (QMIC), Qatar Science and Technology Park, Doha, Qatar; Qatar Mobility Innovations Center (QMIC), Qatar Science and Technology Park, Doha, Qatar","2014 International Wireless Communications and Mobile Computing Conference (IWCMC)","25 Sep 2014","2014","","","678","683","Cooperative vehicle-to-vehicle (V2V) communications for real-time video streaming of scalable video coded (SVC) videos is investigated, and a novel clustering method is proposed. In the proposed method, moving vehicles are grouped into cooperative clusters to enhance the distribution of the video to the cluster members. Within each cluster, vehicles communicate over IEEE 802.11p, whereas the long term evolution (LTE) system is used to send the data over long range cellular links. The simulation results show that the collaborative V2V video streaming method significantly outperforms the non collaborative case.","2376-6506","978-1-4799-0959-9","10.1109/IWCMC.2014.6906437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906437","Video streaming;scalable video coding;vehicle-to-vehicle communications;LTE;IEEE 802.11p","Vehicles;Streaming media;Static VAr compensators;Shadow mapping;Wireless communication;Road transportation;Fading","cooperative communication;Long Term Evolution;pattern clustering;quality of service;vehicular ad hoc networks;video coding;video streaming","cooperative vehicle-to-vehicle communications;cooperative V2V communications;real-time video streaming;scalable video coded videos;SVC videos;novel clustering method;moving vehicles;cooperative clusters;IEEE 802.11p;long term evolution system;LTE system;long range cellular links;collaborative V2V video streaming method","","1","","16","","25 Sep 2014","","","IEEE","IEEE Conferences"
"An enhanced HeRO scheme for VoD with heterogeneous receivers in wireless networks","Z. Zhong; Linghao Xiao; X. Wang; J. Liu","Shenzhen Key Lab of Information Security and Digital Content Protection Technology, Division of Information Science and Technology, Graduate School at Shenzhen, Tsinghua University, China; Shenzhen Key Lab of Information Security and Digital Content Protection Technology, Division of Information Science and Technology, Graduate School at Shenzhen, Tsinghua University, China; Shenzhen Key Lab of Information Security and Digital Content Protection Technology, Division of Information Science and Technology, Graduate School at Shenzhen, Tsinghua University, China; Shenzhen Key Lab of Information Security and Digital Content Protection Technology, Division of Information Science and Technology, Graduate School at Shenzhen, Tsinghua University, China","2015 Advances in Wireless and Optical Communications (RTUWO)","28 Dec 2015","2015","","","119","122","In Video-on-Demand system, periodic broadcasting is the most bandwidth-efficient method to provide clients with popular videos. Many broadcasting schemes have been proposed to reduce server's bandwidth as well as client's waiting time. However, most of the schemes assume clients to have enough reception bandwidth, which is impractical since client's reception bandwidth varies, especially in wireless network environment. In order to solve this heterogeneous receiving problem, Reception Bandwidth Adaptable (RBA) schemes and Reception Bandwidth Limited (RBL) schemes are presented. In this paper, we propose an Enhanced Heterogeneous Receiver-Oriented scheme (HeRO-3) to solve this problem. By revising the channel design of the FB scheme part in HeRO scheme, clients with 3 or more reception channels can watch the video with much lower waiting time. Also, similar to HeRO scheme, it maintains a low buffer requirement of about 25% of video size. The performance result of HeRO-3 scheme is also presented and compared with other heterogeneous receiving schemes in this paper.","","978-1-4673-7431-6","10.1109/RTUWO.2015.7365733","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7365733","broadcasting scheme;multimedia communication;multimedia system;heterogeneous receivers;video-on-demand;low service latency;wireless network","Bandwidth;Videos;Servers;Broadcasting;Multimedia communication;Wireless networks;Silicon","radio broadcasting;radio networks;radio receivers;radio reception;video on demand;wireless channels","heterogeneous receiver;enhanced HeRO scheme;video-on-demand system;VoD system;periodic broadcasting;bandwidth-efficient method;server bandwidth reduction;wireless network environment;reception bandwidth adaptable scheme;reception bandwidth limited scheme;enhanced heterogeneous receiver-oriented scheme;reception channel design;video size low buffer requirement","","1","","11","","28 Dec 2015","","","IEEE","IEEE Conferences"
"Smart caching for live 360° video streaming in mobile networks","P. Maniotis; N. Thomos","University of Essex,CSEE Department,Colchester,United Kingdom; University of Essex,CSEE Department,Colchester,United Kingdom","2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP)","16 Dec 2020","2020","","","1","6","Despite the advances of 5G systems, the delivery of 360° video content in mobile networks remains challenging because of the size of 360° video files. Recently, edge caching has been shown to bring large performance gains to 360° Video on Demand (VoD) delivery systems, however existing systems cannot be straightforwardly applied to live 360° video streaming. To address this issue, we investigate edge cache-assisted live 360° video streaming. As videos' and tiles' popularities vary with time, our framework employs a Long Short-Term Memory (LSTM) network to determine the optimal cache placement/evictions strategies that optimize the quality of the videos rendered by the users. To further enhance the delivered video quality, users located in the overlap of the coverage areas of multiple SBSs are allowed to receive their data from any of these SBSs. We evaluate and compare the performance of our method with that of state-of-the-art systems. The results show the superiority of the proposed method against its counterparts, and make clear the benefits of accurate tiles' popularity prediction by the LSTM networks and users association with multiple SBSs in terms of the delivered quality.","2473-3628","978-1-7281-9320-5","10.1109/MMSP48831.2020.9287059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9287059","Tile-encoding;live 360° video streaming;edge-caching;LSTM networks","Video on demand;Prefetching;Streaming media;Signal processing;Performance gain;Quality assessment;Video recording","5G mobile communication;cache storage;mobile computing;video on demand;video streaming","smart caching;mobile networks;360° video content;360° video files;edge cache-assisted live 360° video streaming;long short-term memory network;video quality;LSTM networks;360° video on demand delivery systems;5G systems;VoD delivery systems;optimal cache placement","","","","15","","16 Dec 2020","","","IEEE","IEEE Conferences"
"Enhanced Model-Free Deep-Q Network based PTZ Camera Control Method","D. Kim; S. Park","Information & Media Research Center, Korea Electronics Technology Institute, Seoul, Korea; Information & Media Research Center, Korea Electronics Technology Institute, Seoul, Korea","2019 Eleventh International Conference on Ubiquitous and Future Networks (ICUFN)","22 Aug 2019","2019","","","251","253","Recently, the algorithm based on reinforcement learning for controlling PTZ(Pan/Tilt/Zoom) of a camera has been significantly increasing. To improve the accuracy of video analysis in a video surveillance environment, it is essential to get good video sources from a camera such as an appropriate object size or an object not covered by the other object. In this paper, we propose a model-free reinforcement learning based PTZ control method to support improving the accuracy of video analysis. The proposed method fine-tunes the DQN (Deep-Q Network) for smoothly controlling PTZ camera. The simulation results show that the proposed method can automatically and seamlessly manage the PTZ camera according to moving objects.","2165-8536","978-1-7281-1340-1","10.1109/ICUFN.2019.8806045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8806045","Deep-Q network;reinforcement learning;PTZ camera;video surveillance system","Cameras;Reinforcement learning;Computational modeling;Video surveillance;Analytical models;Simulation;Security","cameras;learning (artificial intelligence);target tracking;video surveillance","moving objects;enhanced model-free Deep-Q Network;PTZ camera control method;video analysis;video surveillance environment;video sources;model-free reinforcement learning;PTZ control method","","","","6","","22 Aug 2019","","","IEEE","IEEE Conferences"
"Enhanced multi-view dancing videos synchronisation","X. Lin; V. Kitanovski; Q. Zhang; E. Izquierdo","Multimedia and Vision Group, School of Electronic Engineering and Computer Science, Queen Mary, University of London, UK; Multimedia and Vision Group, School of Electronic Engineering and Computer Science, Queen Mary, University of London, UK; Multimedia and Vision Group, School of Electronic Engineering and Computer Science, Queen Mary, University of London, UK; Multimedia and Vision Group, School of Electronic Engineering and Computer Science, Queen Mary, University of London, UK","2012 13th International Workshop on Image Analysis for Multimedia Interactive Services","28 Jun 2012","2012","","","1","4","This paper describes a system for automatically synchronising multi-view video sequences of Salsa dancing recorded with multimodal capturing platform. The multimodal capturing setup consists of audiovisual streams along with depth maps and inertial measurements. Part of the dataset was video sequences captured from machine vision cameras and Microsoft Kinect sensor that were not temporal synchronised during the capturing stage. As an essential step, we proposed efficient solutions for synchronisation of these data based on co-occurrence appearance changes. In order to improve the accuracy, the proposed system employed state-of-art body detection and tracking algorithm to obtain Region of Interest, within which the appearance changes are analysed. The accurately synchronised video set can then be further analysed and augmented for visualisation and evaluation of dancing performance.","2158-5881","978-1-4673-0790-1","10.1109/WIAMIS.2012.6226773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226773","","Videos;Cameras;Synchronization;Video sequences;Feature extraction;Accuracy;Correlation","computer vision;image enhancement;image sensors;image sequences;object detection;target tracking;video signal processing","multiview dancing videos synchronisation enhancement;video sequences;Salsa dancing;multimodal capturing platform;audiovisual streams;depth maps;inertial measurements;machine vision cameras;Microsoft Kinect sensor;co-occurrence appearance changes;body detection;tracking algorithm;region of interest;dancing performance evaluation;dancing performance visualisation","","5","","13","","28 Jun 2012","","","IEEE","IEEE Conferences"
"Detection of unusual events in low resolution video for enhancing ATM security and prevention of thefts","B. K. Prajwal; M. S. Sreeharsha; R. Raghulan; S. S. Babu; M. Kiran","Computer Science and Engineering, REVA Institute of Technology and Management, Bengaluru, India; Computer Science and Engineering, REVA Institute of Technology and Management, Bengaluru, India; Computer Science and Engineering, REVA Institute of Technology and Management, Bengaluru, India; Computer Science and Engineering, REVA Institute of Technology and Management, Bengaluru, India; Computer Science and Engineering, REVA Institute of Technology and Management, Bengaluru, India","2017 International Conference On Smart Technologies For Smart Nation (SmartTechCon)","14 May 2018","2017","","","1139","1143","In present-day applications, tracking object in low resolution video is a stimulating task due to loss of distinctive aspect in the visual outward look of moving target object. The present-day approaches are typically involved on the enhancement of Low Resolution (LR) video by super resolution techniques. However these approaches need massive computational cost. This cost again rises if we are handling with event detection. Recently, an algorithm has been developed which is capable of detecting unusual events and for enhancement of ATMs security where simple low resolution cameras are used because of their lower cost. Our state-of-the-art technique is able to detect the existence of unusual events like face masking, camera masking, fight or overcrowding in the low resolution video merely by means of standard deviation, statistical method of moving target objects. It is sufficient because it processes low resolution frames and also sends alert messages to the concerned authorities in investigation (surveillance) system for enhancing the ATMs security and prevention of thefts using conventional camera of low resolution.","","978-1-5386-0569-1","10.1109/SmartTechCon.2017.8358547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8358547","Face Detection;Detection of Unusual Event;Video Surveillance;ATMs security;Alert Messages","Conferences","cameras;image enhancement;image recognition;image resolution;image sensors;security;statistical analysis;video signal processing;video surveillance","unusual events;target object;super resolution techniques;event detection;simple low resolution cameras;low-resolution video;ATM security;low-resolution frames;moving target object","","","","9","","14 May 2018","","","IEEE","IEEE Conferences"
"Automatic Lecture Video Indexing Using Video OCR Technology","H. Yang; M. Siebert; P. Luhne; H. Sack; C. Meinel","Hasso Planner Inst. (HPI), Univ. of Potsdam, Potsdam, Germany; Hasso Planner Inst. (HPI), Univ. of Potsdam, Potsdam, Germany; Hasso Planner Inst. (HPI), Univ. of Potsdam, Potsdam, Germany; Hasso Planner Inst. (HPI), Univ. of Potsdam, Potsdam, Germany; Hasso Planner Inst. (HPI), Univ. of Potsdam, Potsdam, Germany","2011 IEEE International Symposium on Multimedia","9 Jan 2012","2011","","","111","116","During the last years, digital lecture libraries and lecture video portals have become more and more popular. However, finding efficient methods for indexing multimedia still remains a challenging task. Since the text displayed in a lecture video is closely related to the lecture content, it provides a valuable source for indexing and retrieving lecture contents. In this paper, we present an approach for automatic lecture video indexing based on video OCR technology. We have developed a novel video segmenter for automated slide video structure analysis and a weighted DCT (discrete cosines transformation) based text detector. A dynamic image constrast/brightness adaption serves the purpose of enhancing the text image quality to make it processible by existing common OCR software. Time-based text occurence information as well as the analyzed text content are further used for indexing. We prove the accuracy of the proposed approach by evaluation.","","978-1-4577-2015-4","10.1109/ISM.2011.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123333","video OCR;video segmentation;e-learning;multimedia retrieval;recorded lecture videos","Discrete cosine transforms;Streaming media;Optical character recognition software;Image segmentation;Image edge detection;Indexing;Brightness","computer aided instruction;digital libraries;discrete cosine transforms;image enhancement;indexing;multimedia computing;optical character recognition;portals;text analysis;video retrieval","automatic lecture video indexing;video OCR technology;digital lecture libraries;lecture video portals;multimedia indexing;lecture content retrieval;video segmenter;automated slide video structure analysis;weighted DCT based text detector;discrete cosines transformation;dynamic image constrast adaption;text image quality enhancement;OCR software;time-based text occurrence information;text content analysis","","28","","11","","9 Jan 2012","","","IEEE","IEEE Conferences"
"A QoE and Visual Attention Evaluation on the Influence of Audio in 360° Videos","A. Hirway; Y. Qiao; N. Murray","Athlone Institute of Technology, Ireland; Athlone Institute of Technology, Ireland; Athlone Institute of Technology, Ireland","2020 IEEE 21st International Symposium on "A World of Wireless, Mobile and Multimedia Networks" (WoWMoM)","9 Oct 2020","2020","","","191","193","360° video, also known as immersive video, is the recording of video content which simultaneously captures scene information in every direction, using an omnidirectional camera. Due to their immersive nature, the popularity of 360° videos has grown significantly. Understanding user Visual Attention when watching 360° videos is very important. This knowledge can help develop effective techniques for processing, encoding, distributing, and rendering 360° content. Whilst major efforts have concentrated on the visual element of immersive experiences, recently there has been growing interest in different forms of audio and in particular high-quality spatial audio. Spatial audio allows listeners to experience sound in all directions. Ambisonics or 3D audio is one such technique which offers a complete 360° soundscape. Although several models of visual and audio-visual attention have been proposed, very few have investigated the role of spatial audio in guiding attention in 360° videos. This demo shows our dataset and our methodological approach to understanding the user's audio-visual attention and QoE when experiencing 360° videos enhanced with spatial sound (first and third order ambisonic). Our research focus is to understand how audio affects Visual Attention in 360° videos and to evaluate its impact on the user's Quality of Experience (QoE).","","978-1-7281-7374-0","10.1109/WoWMoM49955.2020.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9217677","360° Video;Spatial Audio;Ambisonics;QoE;Audio-Visual Attention","Videos;Visualization;Quality of experience;Virtual reality;Streaming media;Media;Resists","audio signal processing;audio-visual systems;quality of experience;video recording;video signal processing","visual element;immersive experiences;high-quality spatial audio;audio-visual attention;QoE;spatial sound;visual attention evaluation;360° videos;immersive video;360° soundscape;video content recording;omnidirectional camera;ambisonics;3D audio;quality of experience","","","","19","","9 Oct 2020","","","IEEE","IEEE Conferences"
"Enhanced Inter-Prediction Via Shifting Transformation in the H.264/AVC","S. G. Blasi; E. Peixoto; E. Izquierdo","School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.","IEEE Transactions on Circuits and Systems for Video Technology","1 Apr 2013","2013","23","4","735","740","Inter-prediction based on block-based motion estimation (ME) is used in most video codecs. The closer the prediction to the target block, the lower the residual, and thus more efficient compression can be achieved. In this paper, a new technique called enhanced inter-prediction (EIP) is proposed to improve the prediction candidates using an additional transformation acting while performing ME. A parametric transformation acts within the coding loop of each block to modify the prediction for each motion vector candidate. The EIP is validated in the particular case of a single-parameter shifting transformation. This paper presents an efficient algorithm to compute the best shift for each prediction candidate and a model to select the optimal prediction based on minimum cost integrating the approach with existing rate-distortion optimization techniques in the H.264/AVC video codec. Results show significant improvements with an average of 6% bit-rate reduction compared to the original H.264/AVC.","1558-2205","","10.1109/TCSVT.2012.2214931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6279460","H.264/AVC;inter-prediction;video coding","Video coding;Standards;Gain;Encoding;Decoding;Vectors;PSNR","data compression;error statistics;motion estimation;prediction theory;rate distortion theory;video codecs;video coding","enhanced interprediction;block-based motion estimation;ME;target block prediction;compression;enhanced inter-prediction;EIP;parametric transformation;coding loop;motion vector;single-parameter shifting transformation;optimal prediction;rate-distortion optimization technique;H.264/AVC video codec;bit-rate reduction","","3","1","13","","22 Aug 2012","","","IEEE","IEEE Journals"
"Lossy video compression using limited set of mathematical functions and reference values","L. O. Bensaid; M. Omari","LDDI Laboratory, Mathematics and Computer Science Department, University of Adrar, Adrar 01000, Algeria; LDDI Laboratory, Mathematics and Computer Science Department, University of Adrar, Adrar 01000, Algeria","2017 International Conference on Mathematics and Information Technology (ICMIT)","18 Jan 2018","2017","","","19","23","Nowadays the number of digital video applications is rapidly increasing. The amount of raw video data is tremendous which makes storing, processing, and transmitting video sequences very complex tasks. Furthermore, while the demand for enhanced user experience is growing, the sizes of devices capable of performing video processing operations are getting smaller. In this paper, we propose a new lossy video compression technique using a set of well known mathematical functions and a limited set of reference values. For comparison purposes, we conducted several compression experiments of our technique as well as MPEG-2, H.264, and Motion JPEG. Experimental results show a promising of our method regarding the quality and the compression ratio.","","978-1-5386-3269-7","10.1109/MATHIT.2017.8259690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8259690","Lossy video compression;mathematical function compression;MPEG-2;H.264;Motion JPEG","Transform coding;Image coding;Streaming media;Video compression;Decoding;Standards;Discrete cosine transforms","data compression;image sequences;video coding","reference values;digital video applications;raw video data;video processing operations;lossy video compression technique;compression ratio;user experience;transmitting video sequences","","","","12","","18 Jan 2018","","","IEEE","IEEE Conferences"
"P.1203 evaluation of real OTT video services","S. Satti; C. Schmidmer; M. Obermann; R. Bitto; L. Agarwal; M. Keyhl","OPTICOM Dipl.-Ing. M. Keyhl GmbH, Erlangen, Germany; OPTICOM Dipl.-Ing. M. Keyhl GmbH, Erlangen, Germany; OPTICOM Dipl.-Ing. M. Keyhl GmbH, Erlangen, Germany; OPTICOM Dipl.-Ing. M. Keyhl GmbH, Erlangen, Germany; OPTICOM Dipl.-Ing. M. Keyhl GmbH, Erlangen, Germany; OPTICOM Dipl.-Ing. M. Keyhl GmbH, Erlangen, Germany","2017 Ninth International Conference on Quality of Multimedia Experience (QoMEX)","3 Jul 2017","2017","","","1","3","Quality prediction performance of recently standardized parametric P.1203 models for real-streaming services: YouTube, Vimeo, Amazon Instant Video and a proprietary DASH-based streaming framework, is analyzed. In particular, a validation database comprising of bitstream traces from aforementioned services is used to evaluate the performance of P.1203 (mode 0 and mode 1) models. It is understood from the analysis that P.1203 models do capture the diverse encoding optimization strategies used in today's streaming applications to yield accurate prediction of long-term audio-visual quality, and the overall performance of P.1203 can be further enhanced using short-term Full Reference (FR) video-only models.","2472-7814","978-1-5386-4024-1","10.1109/QoMEX.2017.7965682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965682","P.1203;QoE;Streaming Services;Video Quality","Streaming media;Encoding;Predictive models;Databases;Quality assessment;Video recording;Analytical models","optimisation;social networking (online);video coding;video databases;video streaming","P.1203 evaluation;real OTT video services;quality prediction performance;standardized parametric P.1203 models;real-streaming services;YouTube;Vimeo;Amazon Instant Video;proprietary DASH-based streaming framework;validation database;encoding optimization strategies;long-term audio-visual quality;short-term full reference video-only models;FR","","5","","8","","3 Jul 2017","","","IEEE","IEEE Conferences"
"A Perception-Based Hybrid Model for Video Quality Assessment","F. Zhang; D. R. Bull","Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K.; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K.","IEEE Transactions on Circuits and Systems for Video Technology","2 Jun 2016","2016","26","6","1017","1028","It is known that the human visual system (HVS) employs independent processes (distortion detection and artifact perception-also often referred to as near-threshold and suprathreshold distortion perception) to assess video quality for various distortion levels. Visual masking effects also play an important role in video distortion perception, especially within spatial and temporal textures. In this paper, a novel perception-based hybrid model for video quality assessment is presented. This simulates the HVS perception process by adaptively combining noticeable distortion and blurring artifacts using an enhanced nonlinear model. Noticeable distortion is defined by thresholding absolute differences using spatial and temporal tolerance maps that characterize texture masking effects, and this makes a significant contribution to quality assessment when the quality of the distorted video is similar to that of the original video. Characterization of blurring artifacts, estimated by computing high frequency energy variations and weighted with motion speed, is found to further improve metric performance. This is especially true for low quality cases. All stages of our model exploit the orientation selectivity and shift invariance properties of the dual-tree complex wavelet transform. This not only helps to improve the performance but also offers the potential for new low complexity in-loop application. Our approach is evaluated on both the Video Quality Experts Group (VQEG) full reference television Phase I and the Laboratory for Image and Video Engineering (LIVE) video databases. The resulting overall performance is superior to the existing metrics, exhibiting statistically better or equivalent performance with significantly lower complexity.","1558-2205","","10.1109/TCSVT.2015.2428551","Overseas Research Students Awards Scheme; Engineering and Physical Sciences Research Council(grant numbers:EP/J019291/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7100892","Video metrics;quality assessment;visual masking;blurring detection;Blurring detection;quality assessment;video metrics;visual masking","Quality assessment;Nonlinear distortion;Visualization;Databases;Video recording;Sensitivity","image restoration;image segmentation;image texture;video signal processing;wavelet transforms","hybrid model perception;video quality assessment;human visual system;HVS;artifact perception;distortion detection;suprathreshold distortion perception;visual masking effects;video distortion perception;temporal textures;spatial textures;enhanced nonlinear model;texture masking effects;blurring artifacts;shift invariance properties;dual-tree complex wavelet transform;video quality experts group;VQEG","","27","","59","","1 May 2015","","","IEEE","IEEE Journals"
"A survey: Heterogeneous video transcoder for H.264/AVC to HEVC","S. J. Kaware; S. K. Jagtap","Department of E&TC, Smt. Kashibai Navale College of Engineering, Vadgaon (Bk), Pune-41, India; Department of E&TC, Smt. Kashibai Navale College of Engineering, Vadgaon (Bk), Pune-41, India","2015 International Conference on Pervasive Computing (ICPC)","16 Apr 2015","2015","","","1","3","High Efficiency Video Coding (HEVC) is invented as new video coding standard for improvement of the H.264/AVC coding standard. HEVC currently used to enhance the video compression and quality of video. However, there are a lot of limitations in H.264/AVC encoded video. So the new efficient method is proposed for transcoding the H.264 encoded video into HEVC format. In this paper, different methods for video transcoding are explained which include mode mapping, machine learning, complexity scalable and background modeling.","","978-1-4799-6272-3","10.1109/PERVASIVE.2015.7087098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7087098","background modeling;complexity scalability;High Efficiency Video Coding (HEVC);machine learning;mode mapping","Transcoding;Video coding;Complexity theory;Standards;Codecs;Rate-distortion","video coding","heterogeneous video transcoder;H.264;HEVC;high efficiency video coding;video compression;mode mapping;machine learning;background modeling","","2","","6","","16 Apr 2015","","","IEEE","IEEE Conferences"
"Transaction clues based video summarization using by speeded up robust features","N. Brindha; T. Amitha","Department of CSE, Dhanalakshmi College of Engineering, Chennai-601301, India; Department of CSE Dhanalakshmi College of Engineering, Chennai-601301, India","International Conference on Information Communication and Embedded Systems (ICICES2014)","9 Feb 2015","2014","","","1","6","A novel video composition system ""Video Puzzle"" which generates aesthetically enhanced long-shot videos from short video clips. Here is to automatically composite several related single shots into a virtual long-take video with spatial and temporal consistency. We propose a novel framework to compose descriptive long-take video with content-consistent shots retrieved from a video pool. For each video, frame-by-frame search is performed over the entire pool to find start-end content correspondences through a coarse-to-fine partial matching process. The content correspondence here is general and can refer to the matched person and objects, The content consistency of these correspondences enables us to design several shot transition schemes to seamlessly stitch one shot to another in a spatially and temporally consistent manner. Then we using the SURF refer the object to calculate the maximum number of object in the process. The proposed system can also provide an efficient video browsing mode. Experiments are conducted on multiple video albums and the results demonstrate the effectiveness and the Usefulness of the proposed scheme.","","978-1-4799-3834-6","10.1109/ICICES.2014.7034060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7034060","Image retrieval;one-shot video;video authoring;video transition","Face;Streaming media;Robustness;Educational institutions;Detectors;Training;Support vector machines","image matching;video signal processing","transaction clues based video summarization;speeded up robust features;video composition system;video puzzle;short video clips;virtual long-take video;spatial consistency;temporal consistency;content consistent shots;video pool;coarse-to-fine partial matching process;content correspondence;SURF;video browsing mode;multiple video albums","","","","10","","9 Feb 2015","","","IEEE","IEEE Conferences"
"Video retargeting based frame-compatible stereo video coding","S. Chen; M. Tsai; J. Chiang","Department of Electrical Engineering, National Chung Cheng University, Taiwan; Department of Electrical Engineering, National Chung Cheng University, Taiwan; Department of Electrical Engineering, National Chung Cheng University, Taiwan","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","1854","1858","Due to more and more request of visual entertainment with improved perceptual realism, stereo video offering enhanced realism and interactivity is highly desired. Usually, the stereo video is packed into a single video and each view preserves only half resolution to achieve an efficient delivery. In this paper, a novel frame-compatible stereo video coding technique based on content-aware video retargeting is presented. Despite of combing the stereo video into a single channel by uniform downsampling, we propose to sub-sample the stereo video in an unequal way, by taking the saliency map of the video into considering. During the reconstruction of the stereo video with original resolution, the regions with higher saliency attention are less distorted, relying on a smaller downsampling factor, instead of 2. The experimental results reveal that the proposed technique achieves up to 43% bitrate saving as compared to the most popular frame-compatible stereo formats.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6637974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637974","Frame compatible;stereo video;video retargeting;content-aware","Abstracts;Indexes;Cameras;Stereo image processing;Image reconstruction;Decoding;Standards","image enhancement;image reconstruction;image resolution;image sampling;video coding","stereo video enhancement;frame-compatible stereo video coding technique;content-aware video retargeting;uniform downsampling;video saliency map;stereo video reconstruction;stereo video resolution;frame-compatible stereo format","","1","","12","","21 Oct 2013","","","IEEE","IEEE Conferences"
"Video stabilization using classification-based homography estimation for consumer video camera","I. Yoon; S. Jeon; S. Jeong; J. Paik","Image Processing and Intelligent Systems Laboratory, Graduate School of Advanced Imaging Science, Multimedia, and Film, Chung-Ang University, Seoul, Korea; Image Processing and Intelligent Systems Laboratory, Graduate School of Advanced Imaging Science, Multimedia, and Film, Chung-Ang University, Seoul, Korea; Image Processing and Intelligent Systems Laboratory, Graduate School of Advanced Imaging Science, Multimedia, and Film, Chung-Ang University, Seoul, Korea; Image Processing and Intelligent Systems Laboratory, Graduate School of Advanced Imaging Science, Multimedia, and Film, Chung-Ang University, Seoul, Korea","2015 IEEE 5th International Conference on Consumer Electronics - Berlin (ICCE-Berlin)","28 Jan 2016","2015","","","116","117","Handheld camera such as a mobile phone, action camera, and portable camcorder suffer from video instability due to unintentional shakes. This paper presents a video stabilization approach to enhance a shaky video acquired by consumer handheld camera. The proposed algorithm consists of three steps: i) generation of the binary image based on vertical class, ii) estimation of homography, and iii) frame warping for enhancing a shaky video. As a result, the proposed algorithm can remove undesired motions using optimal camera motion trajectory based on feature detection in the background region. The proposed video stabilization algorithm is suitable for consumer video devices including advanced driver assistance systems (ADAS), video surveillance systems, and aerial platforms for video enhancement.","","978-1-4799-8748-1","10.1109/ICCE-Berlin.2015.7391209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391209","feature detection;moving object;Kalman filter;video stabilization","Cameras;Trajectory;Streaming media;Feature extraction;Estimation;Prediction algorithms;Kalman filters","feature extraction;image classification;image enhancement;motion estimation;video cameras;video surveillance","video stabilization;classification-based homography estimation;consumer video camera;mobile phone;action camera;portable camcorder;video instability;unintentional shakes;shaky video;consumer handheld camera;binary image generation;vertical class;frame warping;undesired motion removal;optimal camera motion trajectory;feature detection;advanced driver assistance systems;ADAS;video surveillance systems;aerial platforms;video enhancement","","","","5","","28 Jan 2016","","","IEEE","IEEE Conferences"
"Doctoral Colloquium—Exploring the Benefits of Using 360<sup>0</sup> Video Immersion to Enhance Motivation and Engagement in System Modelling Education","J. Carlos Muñoz-Carpio; M. Cowling; J. Birt","CQUniversity,School of Engineering & Technology,Brisbane,Queensland,Australia; CQUniversity,School of Engineering & Technology,Brisbane,Queensland,Australia; Bond University,Faculty of Society & Design,Gold Coast,Queensland,Australia","2020 6th International Conference of the Immersive Learning Research Network (iLRN)","4 Aug 2020","2020","","","403","406","This paper describes the use of a 360°-video case study to enhance experiential learning in an ICT systems analysis class. We hypothesize that the use of a visual case study combined with virtual instructions can lead to learning motivation and enhanced learner engagement. This examination follows the conceptualization phase of a methodology to investigate the practical uses of the intervention deriving in learning engagement. A sample of 24 participants from an Australian University was considered. The findings of the study reveal a positive impact on the measures provoking learning engagement and motivation among the participants.","","978-1-7348995-0-4","10.23919/iLRN47897.2020.9155100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9155100","systems analysis and design;education;immersion;learning motivation and active engagement;experiential learning","Visualization;Task analysis;Tools;Solid modeling;Media;Training","computer aided instruction;educational institutions;human factors;video signal processing;virtual reality","doctoral colloquium;enhance motivation;system modelling education;360°-video case study;experiential learning;ICT systems analysis class;visual case study;virtual instructions;learning motivation;enhanced learner engagement;conceptualization phase;learning engagement;3600 video immersion;motivation enhancement","","","","20","","4 Aug 2020","","","IEEE","IEEE Conferences"
"Enhancing the MST-CSS Representation Using Robust Geometric Features, for Efficient Content Based Video Retrieval (CBVR)","C. Chattopadhyay; S. Das","Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Madras, Chennai, India; Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Madras, Chennai, India","2012 IEEE International Symposium on Multimedia","31 Jan 2013","2012","","","352","355","Multi-Spectro-Temporal Curvature Scale Space (MST-CSS) had been proposed as a video content descriptor in an earlier work, where the peak and saddle points were used for feature points. But these are inadequate to capture the salient features of the MST-CSS surface, producing poor retrieval results. To overcome these, we propose EMST-CSS (Enhanced MST-CSS) as a better feature representation with an improved matching method for CBVR (Content Based Video Retrieval). Comparative study with the existing MST-CSS representation and two state-of-the-art methods for CBVR shows enhanced performance on one synthetic and two real-world datasets.","","978-1-4673-4370-1","10.1109/ISM.2012.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424685","EMST-CSS;CBVR;Curvature;Peak;Ridge;STV;VOB;Matching;Precision-Recall","Feature extraction;Shape;Trajectory;Surface treatment;Cameras;Robustness;Humans","content-based retrieval;feature extraction;pattern matching;video retrieval;video signal processing","MST-CSS representation;robust geometric features;content based video retrieval;CBVR;multispectro-temporal curvature scale space;video content descriptor;saddle points;feature points;salient features;MST-CSS surface;Enhanced MST-CSS;feature representation;matching method","","5","","26","","31 Jan 2013","","","IEEE","IEEE Conferences"
"A Multi-Objective Video Crowdsourcing Method in Mobile Environment","C. Yan; Y. Chen; F. Wang; Y. Wen; S. Fu; W. Huang","School of Information Science and Engineering, Qufu Normal University, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Information Science and Engineering, Qufu Normal University, China; Key Laboratory of Knowledge Processing and Networked Manufacture, Hunan University of Science and Technology, Xiangtan, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Information Science and Engineering, Qufu Normal University, China","IEEE Access","25 Sep 2019","2019","7","","133787","133798","With the rapid development of mobile video services, HD and UHD videos are attractive for mobile users due to the realistic visual enjoyment and the accurate representation. However, the limited transmission bit rate in 4G communication network affects the experience of the users for watching videos. Crowdsourcing is considered as a reasonable and effective solution to alleviate the resource limitation. Through employing the crowdsourcing participants to download and transmit video segments, mobile users can get enhanced video services. However, it is still a significant challenge that how to avoid excessive payment and energy consumption when the crowdsourcing participants download the video segments for the mobile users. To address this challenge, a multi-objective video crowdsourcing method in mobile environment is proposed in this paper. Technically, the crowdsourcing participants apply device-to-device (D2D) communication technique rather than the cellular network or bluetooth transmission to transmit video segments to the mobile users. Here, we divide our problem into two situations, the single participant case and the multi-participants case. In the single participant case, we apply the improved dynamic programming algorithm to find strategies with more enhanced video service time that the crowdsourcing participants provide for the mobile users. Then Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) and Multiple Criteria Decision Making (MCDM) techniques are applied to find a balanced strategy to maximize the enhanced video service time and minimize the payment and the energy consumption. In the multi-participants case, through DBSCAN clustering, the problem with multi-participants is divided into several problems with single participant. Finally, extensive experimental evaluations are conducted to demonstrate the effectiveness and efficiency of our proposed method.","2169-3536","","10.1109/ACCESS.2019.2940955","National Key Research and Development Program of China(grant numbers:2017YFB1400600); National Natural Science Foundation of China(grant numbers:61872219); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835905","Mobile video;D2D;crowdsourcing;DBSCAN","Crowdsourcing;Energy consumption;Streaming media;Device-to-device communication;Visualization;Information science","4G mobile communication;crowdsourcing;decision making;dynamic programming;pattern clustering;TOPSIS;video signal processing;video streaming","multiobjective video crowdsourcing method;mobile environment;mobile video services;UHD videos;mobile users;watching videos;video segments;enhanced video services;crowdsourcing participants download;device-to-device communication technique;single participant case;enhanced video service time;multiparticipant case;device-to-device video segement;technique for order preference by similarity by ideal solution;TOPSIS;multiple criteria decision making","","2","","49","CCBY","13 Sep 2019","","","IEEE","IEEE Journals"
"Learning-Based JND-Directed HDR Video Preprocessing for Perceptually Lossless Compression With HEVC","S. Ki; J. Do; M. Kim","Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Access","30 Dec 2020","2020","8","","228605","228618","The final consumer of videos is mostly human. Therefore, if videos can be compressed by fully utilizing the perception characteristics of human visual systems (HVS), the bitrates of the compressed videos can be significantly reduced with subjective visual quality degradation as little as possible. Based on this, we newly propose a learning-based Just Noticeable Distortion (JND)-directed preprocessing scheme for perceptual video compression, especially for 10-bit High Dynamic Range (HDR) videos, which is called the HDR-JNDNet. Our HDR-JNDNet effectively suppresses the perceptual redundancy of 10-bit HDR video signals so that the compression efficiency can be significantly enhanced for the HEVC main10 profile encoder. To our best knowledge, our work is the first approach to training a CNN-based model to directly generate the JND-directed suppressed frames of 10-bit HDR video with the negligible perceptual quality difference between the decoded frames for the original HDR video input with and without the preprocessing by our HDR-JNDNet. Via intensive experiments, when the HDR-JNDNet is applied as preprocessing for the HDR video input before compression, it allows to remarkably save the required bitrates up to the maximum (average) 40.66% (18.37%) for 4K-UHD/HDR test videos, with little subjective video quality degradation without increasing the computational complexity.","2169-3536","","10.1109/ACCESS.2020.3046194","Institute for Information and Communications Technology Promotion (IITP) Grant; Ministry for Science and ICT (Information and Communication Technology) of the Korean Government, Intelligent High Realistic Visual Processing for Smart Broadcasting Media(grant numbers:2017-0-00419); BK21 (Brain Korea 21) Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301307","High dynamic range (HDR) video;video compression;just noticeable distortion (JND);perceptual video coding (PVC)","Videos;Encoding;Data preprocessing;Distortion;Quantization (signal);Redundancy;High efficiency video coding","computational complexity;convolutional neural nets;data compression;image enhancement;video coding","JND-directed HDR video preprocessing;lossless compression;human visual systems;visual quality degradation;just noticeable distortion;perceptual video compression;high dynamic range videos;HDR-JNDNet;CNN-based model;JND-directed suppressed frames;subjective video quality degradation;HDR video signals;computational complexity","","","","33","CCBY","21 Dec 2020","","","IEEE","IEEE Journals"
"Enhancing video lectures with digital footnotes","V. Kumar","Department of Information Science, PES University, Bangalore, India","2014 IEEE Frontiers in Education Conference (FIE) Proceedings","19 Feb 2015","2014","","","1","3","This paper describes an ongoing research project to develop micro-notes: digital footnotes created by a community of instructors and learners to enhance the utility of educational videos. Specifically, we demonstrate that a suitably motivated group of undergraduates were able to create over 1,400 annotations for over 30 hours of video lectures in just ten days. Furthermore, over 57% of these micro-notes were rated by at least two peers in two more days. The quality of these micro-notes was manually assessed in two ways: (1) an independent group of undergraduates evaluated a large sample (35%) of micro-notes that had been peer-rated above average by the original group, and (2) the author, who was also the instructor for a similar course, evaluated a smaller sample of the top 6% of peer-rated notes. Our results indicate that peer-ratings are a useful (but not infallible) indication of the quality of micro-notes, and can be used to effectively generate useful annotations of video lectures for beginner students.","2377-634X","978-1-4799-3922-0","10.1109/FIE.2014.7044212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044212","video lectures;crowd-sourced annotations;micro-notes","Communities;Education;Internet;Message systems;Statistical analysis;YouTube;Collaboration","computer aided instruction;further education;interactive video","video lectures;digital footnotes;micronotes;educational videos;undergraduates;peer-rated notes;beginner students","","1","","8","","19 Feb 2015","","","IEEE","IEEE Conferences"
"Automated Security Testing Framework for Validating Content Rights On Video Streaming Devices","D. Das","Video Assurance Research Lab, u, Tata Consultancy Services Ltd.,Bhubaneswar,India","TENCON 2019 - 2019 IEEE Region 10 Conference (TENCON)","12 Dec 2019","2019","","","516","521","Proliferation of high speed internet services and mass adoption of smart phones and internet enabled smart devices has led to a meteoritic rise in streaming video services around the globe. As streaming audio-video content takes center-stage in this digital era, “Cut-the-Cord” phenomenon has gained significant momentum all over the globe. New streaming models like Over-the-Top (OTT) services have disrupted this industry globally. OTT platforms like Netflix, Amazon Prime, YouTube, Sony Crackle, BBC iPlayer have gained lot of market share in the recent years. In India alone, there are more than 40 video streaming services [1] like Hotstar, AirtelTV, Voot, Sony Liv, JioTV and the list is growing by the day. However, streaming video applications, face constant challenges with respect to security of content rights across the streaming value chain. A leading global research firm KPMG, in a recent report suggests that, global streaming players could suffer losses of over 50 billion dollars (USD) due to pirated content in the next few years. Similarly a recent report released by FICCI highlights the fact that, upto 30% of overall film sector revenue in India has been lost due to piracy alone. As millions of internet videos get streamed on various consumer devices, risks of security breaches, online piracy and video hacking, pose considerable challenge for the entire video streaming industry. Therefore, it becomes absolutely essential for the video content creators, aggregators and distributors to adopt best-in-class secure development methodologies along with cutting edge security testing techniques to protect digital video assets. This requires special focus beyond traditional quality assurance (QA) and conventional security testing techniques like Pen Testing, Vulnerability Assessment etc. In this paper we propose a unique automated security testing framework focusing on critical aspects of the video streaming domain. In-order to provide a holistic model for enhancing cyber security aspects of streaming video systems, we expanded our earlier research work [3], [4] to cover latest content security concepts like Digital Rights Management (DRM), High-bandwidth Digital Content Protection (HDCP). Our proposed security testing methodology uses latest automation techniques and proven statistical methods to yield more than 90% better efficiencies with accuracy as high as 97%.","2159-3450","978-1-7281-1895-6","10.1109/TENCON.2019.8929472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8929472","Video Cyber Security;Digital Video;Security Testing","Streaming media;Testing;Servers;Computer security;Automation;Tools","computer crime;computer network security;digital rights management;Internet;quality assurance;video streaming","digital rights management;security testing methodology;latest automation techniques;automated security testing framework;video streaming devices;mass adoption;smart phones;smart devices;meteoritic rise;video services;audio-video content;digital era;OTT platforms;video systems;cyber security aspects;video streaming domain;unique automated security;pen testing;conventional security;digital video assets;edge security;best-in-class secure development methodologies;distributors;entire video streaming industry;video hacking;security breaches;consumer devices;Internet videos;pirated content;global streaming players;recent report;streaming value chain;streaming video applications;Sony Liv;video streaming services;India;Sony Crackle;content security concepts;high-bandwidth digital content protection;content rights;high speed Internet services;cut-the-cord phenomenon;KPMG","","","","26","","12 Dec 2019","","","IEEE","IEEE Conferences"
"An Enhanced Video Super Resolution System Using Group-Based Optimized Filter-Set with Shallow Convolutional Neural Network","S. Kim; J. Nang","Dept. of Comput. Sci. & Eng., Sogang Univ., Seoul, South Korea; Dept. of Comput. Sci. & Eng., Sogang Univ., Seoul, South Korea","2016 12th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)","24 Apr 2017","2016","","","323","326","Scaling up video resolution has conventionally been achieved via linear interpolation, however this method occasionally introduces blurring to the output. Super-resolution (SR), an approach to preserve image quality in enlarged still images, has been exploited as a substitute for linear interpolation, however, the output at times exhibits image qualities worse than what linear interpolation produces primarily because the initial goal of SR is preservation of image quality when a still image is enlarged. In this context, this paper proposes a fast-performance adaptive system for scaling-up other resolutions like X2 using X3 model or X3 using X2 model by (1) first grouping frames that would use similar filter sets (2) then conducting fine-tuning of shallow CNN for SR on each frame group. Filter sets fine-tuned for each group resulted in significantly improved PSNR over either linear interpolation or conventional SR in our experiment. In the fine-tuning stage for each group, 0.5K to 2.5K iterations were sufficient to improve PSNR by 10%. By fine-tuning instead of performing full training, the number of sufficient iterations was reduced from 3,000K to mere 0.5K to 2.5K.","","978-1-5090-5698-9","10.1109/SITIS.2016.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7907486","video resolution Scaling up;convolutional neural network;shot change detection;gradual transition detection;deep learning;fine tuning;super resolution;CNN","Training;Image resolution;Neural networks;Interpolation;Image quality;Optical filters;Maximum likelihood detection","image filtering;image resolution;interpolation;iterative methods;neural nets;optimisation;video signal processing","enhanced video super resolution system;group-based optimized filter-set;shallow convolutional neural network;linear interpolation;SR;image qualities;fast-performance adaptive system;X3 model;X2 model;shallow CNN;improved PSNR;fine-tuning stage","","","","11","","24 Apr 2017","","","IEEE","IEEE Conferences"
"Robust video denoising for better subjective evaluation","A. Acharya; S. Meher","Electronics and Communication Engineering Department, National Institute of Technology Rourkela, 769008, Odisha, India; Electronics and Communication Engineering Department, National Institute of Technology Rourkela, 769008, Odisha, India","2011 International Conference on Image Information Processing","22 Dec 2011","2011","","","1","5","Video denoising is a pre-processing step of many high level video processing applications since it can considerably enhance the perceived video quality, increase compression effectiveness, facilitate transmission bandwidth reduction and improve the performance of the subsequent higher level task such as feature extraction, object detection, prediction, coding and object tracking. In this paper, a new scheme for video denoising based on spatial filtering is proposed which synthesizes the blurring caused by averaging filter and sharpening due to high boost filter for effective objective and subjective video quality enhancement. The main drawback of averaging filter is that it gives a blurring effect and de-emphasizes the fine details from an image such as edges and texture. This blurring artefact is compensated through highboost filtering operation by selecting an appropriate value of the highboost weight factor intended for better video quality and superior HVS performance. Experimental results show that the proposed method achieves better performance of noise removal with much reduced blurring and fine detail preservation in terms of improved objective and subjective video quality.","","978-1-61284-861-7","10.1109/ICIIP.2011.6108967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6108967","Image and video processing;Video denoising;spatial averaging filter;highboost filter;human visual system (HVS);subjective evaluation;additive white Gaussian noise","Noise reduction;Video sequences;Gaussian noise;Information filters;Information processing","image denoising;image enhancement;image restoration;spatial filters;video coding","robust video denoising;high level video processing;video preprocessing;compression effectiveness;transmission bandwidth reduction;spatial filtering;blurring synthesis;averaging filter;video quality enhancement;image sharpening;highboost weight factor;highboost filtering;HVS performance;noise removal;fine detail preservation","","1","","14","","22 Dec 2011","","","IEEE","IEEE Conferences"
"Efficient Semantic Enrichment Process for Human Trajectories in Surveillance Videos","F. Bao; X. Sun; W. Luo; X. Liu; G. Ji; B. Zhao","School of Computer Science and Technology, Nanjing Normal University,Nanjing,China; School of Computer Science and Technology, Nanjing Normal University,Nanjing,China; School of Computer Science and Technology, Nanjing University of Posts and Telecommunications,Nanjing,China; Land Surverying and Geo-Informatics, The Hong Kong Polytechnic University,Hong Kong,China; School of Computer Science and Technology, Nanjing Normal University,Nanjing,China; School of Computer Science and Technology, Nanjing Normal University,Nanjing,China","2019 6th International Conference on Behavioral, Economic and Socio-Cultural Computing (BESC)","20 Jan 2020","2019","","","1","4","Nowadays, it becomes very convenient to collect large-scale videos that record trajectories of human mobility behavior in various situations in cities, due to the increasing availability of surveillance camera. Obviously, surveillance videos became a new data source of spatiotemporal trajectories. However, a typical trajectory semantic enrichment process receives as input spatiotemporal trajectories. The process methods cannot be applied to video data directly. In this paper, we propose a semantic enrichment process framework for human trajectories in surveillance videos. It includes trajectory identification in videos, trajectory transformation, sub-traj ectory segmentation, segment annotation. We can derive semantic trajectories from surveillance videos through the four phases. Having observed the common occurrence of the similarities between individual trajectories, we propose a grid index-based method to search similar pre-annotated sub-trajectory segments in pixel space for retrieving semantic trajectories in order to enhance the performance of this approach. Finally, we demonstrate the effectiveness and efficiency of our proposed approach by using a real world data set.","","978-1-7281-4762-8","10.1109/BESC48373.2019.8963491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8963491","video;semantic trajectory;annotation","Trajectory;Semantics;Videos;Spatiotemporal phenomena;Surveillance;Annotations;Transforms","data mining;image motion analysis;image segmentation;query processing;video signal processing;video surveillance;visual databases","human trajectories;surveillance videos;large-scale videos;human mobility behavior;surveillance camera;input spatiotemporal trajectories;video data;trajectory identification;trajectory transformation;semantic trajectories;trajectory semantic enrichment process;grid index-based method;pre-annotated sub-trajectory segments","","","","5","","20 Jan 2020","","","IEEE","IEEE Conferences"
"A multi-mode high-speed video data capture system based on DSP + FPGA","Wang Yuanpeng; Jiang Hongxu; Yu Huirong","Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, 100191, China; Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, 100191, China; Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, 100191, China","2011 International Conference on Multimedia Technology","25 Aug 2011","2011","","","5230","5233","Based on actual needs, we design a DSP and FPGA-based video capture system, achieve the capture of standard PAL video and digital video which is transmitted through a standard Camera Link interface. By building a high-speed data path which is composed of Camera Link, EMIF, and PCI interface, we guarantee a high-speed and correct transmission of the captured data. Further more, processing like de-interlacing, digital image stabilization and video enhancing is done to the captured video data. These provide the basis for follow-up process like video compression and intelligent analysis, outstanding result is received and the system reliability is verified.","","978-1-61284-774-0","10.1109/ICMT.2011.6002328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6002328","video data capture;Camera Link;EMIF;PCI","Digital signal processing;Field programmable gate arrays;Clocks;Data communication;Streaming media;Digital images;Cameras","digital signal processing chips;field programmable gate arrays;video signal processing","multimode high-speed video data capture system;DSP;FPGA;video capture system;standard PAL video;digital video;standard camera link interface;high-speed data path;EMIF;PCI interface;digital image stabilization;video enhancing;video compression;intelligent analysis;system reliability","","5","","7","","25 Aug 2011","","","IEEE","IEEE Conferences"
"The impact of YouTube videos on the student's learning","Y. Chtouki; H. Harroud; M. Khalidi; S. Bennani","Al Akhawayn University in Ifrane, P.O Box 104, Ave Hassan II, Ifrane 53000, Morocco; Al Akhawayn University in Ifrane, P.O Box 104, Ave Hassan II, Ifrane 53000, Morocco; RIME Laboratory, Ecole Mohammadia d'Ingénieurs Ave Ibn Sina, Rabat, Morocco; RIME Laboratory, Ecole Mohammadia d'Ingénieurs Ave Ibn Sina, Rabat, Morocco","2012 International Conference on Information Technology Based Higher Education and Training (ITHET)","23 Jul 2012","2012","","","1","4","An important part of education is student's learning. Good quality education is based mainly on how well student attain the knowledge. One way to achieve that is to simplify the content and make it as intuitive as possible. This can be challenging especially for introductory computer science courses for non-computer science students. Such courses are supposed to cover a wide range of complex computer concepts such as networking, computer internal hardware, databases, operating systems and others. In this paper we are presenting the results of a study done on the use of YouTube videos to enhance students' learning. We have evaluated the student's performance in an introduction to computers course for non-computer science students by comparing two groups of students, The first one is a test group in which we have supplied the students with a set of videos from YouTube to illustrate different concepts such as multiple core versus single core processor, hard disk internal components, using fiber optic cables to connect continents under water. ect. The second is a control group in which we have only used the traditional resources, such as the textbook, in class lectures and handouts. The results of the study have shown that students understand and can remember the complex concepts much better when they are exposed to a visual explanation video. We found that most of the students if not all watched the short videos, which is not the case with textual content. One of the main advantages of YouTube is that it is a free web based service that contains short contents about specific concepts taught in schools. Educators can easily search and review videos related to a specific concept or knowledge, and then provide the students with the link. In our case the videos were downloaded using RealPlayer plugin, which allowed us to download any video streaming content on the web. Then we have uploaded the videos in our LMS (learning management system). We have opted to include the videos in the LMS so that we can track the number of students who have downloaded the videos and keep track of the number of downloads. In this study we have found that using YouTube videos encouraged students to look for similar videos, and get a habit of using YouTube as an educational resource. The only challenge is the evaluation of the reliability of the content, for that reason content selection has to done by the instructor. Lastly, YouTube videos have been a useful source of educational content, it is a free web based tool, and the impact has been important based on our study on students' performance. Educators have used YouTube videos in other fields such as nursing in [1] and have proven to be an effective tool to enhance students learning and engagement.","","978-1-4673-2334-5","10.1109/ITHET.2012.6246045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6246045","Blended Learning;E-learning;Web Services;YouTube Videos","Videos;YouTube;Education;Databases;Computers;Least squares approximation;Hardware","computer aided instruction;computer science education;educational administrative data processing;educational aids;educational courses;Internet;social networking (online);video streaming","YouTube videos;student learning;quality education;introductory computer science courses;noncomputer science students;complex computer concepts;student performance;control group;visual explanation video;textual content;free Web based service;RealPlayer plugin;video streaming content;World Wide Web;LMS;learning management system;educational resource;content reliability;content selection;educational content;free Web based tool;students learning;student engagement","","11","","8","","23 Jul 2012","","","IEEE","IEEE Conferences"
"MANET routing protocol performance for video streaming","N. Rathod; N. Dongre","Department of Information Technology, Ramrao Adik Institute of Technology, Nerul Navi Mumbai, India; Department of Information Technology, Ramrao Adik Institute of Technology, Nerul Navi Mumbai, India","2017 International Conference on Nascent Technologies in Engineering (ICNTE)","15 Jun 2017","2017","","","1","5","MANET is a self-sorting out, decentralized, framework less, multi hop, remote system of cell phones. routing protocols assume a crucial part in transmission of information over the network. Streaming video is content sent in packed frame over the Internet and showed by the viewer continuously. Mobile Ad hoc Networks are considered for some applications. Routing protocols are the most imperative component of MANET and media streaming is a very requesting assignment over MANET. Examination of directing convention which is more dependable for video streaming is specified in this paper. Some well-known routing protocols in particular Ad-hoc On-request Distance Vector (AODV), Ad-hoc On-request multipath Distance Vector (AOMDV), Enhanced Video Streaming in MANET (EVSM) have been considered and on the premise of throughput, normal network delay, packet delivery ratio these protocols are tasted in this paper.","","978-1-5090-2794-1","10.1109/ICNTE.2017.7947974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7947974","MANET;AODV;AOMDV;Enhanced Video Streaming In MANET;MDC;Cross Layer Technique;Multipath Routing Technique","Streaming media;Routing protocols;Mobile ad hoc networks;Routing;Throughput;Cross layer design","mobile ad hoc networks;routing protocols;video streaming","MANET routing protocol performance;video streaming;self-sorting out;multi hop;remote system;cell phones;network information;packed frame;Internet;mobile ad hoc networks;media streaming;routing protocols;ad-hoc on-request distance vector;AODV;ad-hoc on-request multipath distance vector;AOMDV;enhanced video streaming","","7","","10","","15 Jun 2017","","","IEEE","IEEE Conferences"
"Enhancing Mobile Video Capacity and Quality Using Rate Adaptation, RAN Caching and Processing","H. A. Pedersen; S. Dey","Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA, USA; Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA, USA","IEEE/ACM Transactions on Networking","14 Apr 2016","2016","24","2","996","1010","Adaptive Bit Rate (ABR) streaming has become a popular video delivery technique, credited with improving Quality of Experience (QoE) of videos delivered on wireless networks. Recent independent research reveals video caching in the Radio Access Network (RAN) holds promise for increasing the network capacity and improving video QoE. In this paper, we investigate opportunities and challenges of combining the advantages of ABR and RAN caching to increase the video capacity and QoE of the wireless networks. While each ABR video is divided into multiple chunks that can be requested at different bit rates, a cache hit requires the presence of a specific chunk at a desired bit rate, making ABR-aware RAN caching challenging. To address this without having to cache all bit rate versions of a video, we propose adding limited processing capacity to each RAN cache. This enables transrating a higher rate version that may be available in the cache, to satisfy a request for a lower rate version, and joint caching and processing policies that leverage the backhaul, caching, and processing resources most effectively, thereby maximizing video capacity of the network. We also propose a novel rate adaptation algorithm that uses video characteristics to simultaneously change the video encoding and transmission rate. The results of extensive statistical simulations demonstrate the effectiveness of our approaches in achieving significant capacity gain over ABR or RAN caching alone, as well as other ways of enabling ABR-aware RAN caching, while improving video QoE.","1558-2566","","10.1109/TNET.2015.2410298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7111389","Adaptive bit rate (ABR) algorithm;video processing and caching;video quality of experience;wireless network capacity","Streaming media;Bit rate;Radio access networks;Wireless communication;Joints;Delays;Mobile communication","quality of experience;radio access networks;statistical analysis;video coding;video streaming","mobile video capacity;mobile video quality;rate adaptation;RAN processing;adaptive bit rate streaming;video delivery technique;quality of experience;QoE;wireless networks;radio access network;wireless networks;backhaul;processing resources;video capacity;rate adaptation algorithm;video characteristics;video encoding;transmission rate;statistical simulations;ABR-aware RAN caching","","58","","27","","21 May 2015","","","IEEE","IEEE Journals"
"Enhancing the performance of Wireless Mesh Network (WMN) for video transmission in context with IEEE 802.11","S. T. Mankar; S. M. Koli","E&TC Department, SKNCOE, Pune, India; E&TC Department, SKNCOE, Pune, India","2015 International Conference on Energy Systems and Applications","4 Jul 2016","2015","","","34","39","In recent years, multimedia streaming has increased dramatically in wireless networks. Transmission of streaming video over Wireless Mesh Networks (WMNs) faces some challenges such as bandwidth guarantee. The packet loss and delay increases due to high data rate flow over a shared medium in a multi-hop manner. Real-time live surveillance applications usually use Forward Error Control (FEC) technique for reliable transmission of video signals. Sending the video over WMN causes degradation in Quality of Service (QoS) parameters such as Packet Delivery Ratio (PDR), end-to-end delay, throughput, which determine the quality of delivered video. This paper proposes an improved Medium Access Control (MAC) based on medium reservations called Enhanced Mesh Deterministic Access (EMDA). It is a per-node reservation of a block of contiguous transmission opportunities. EMDA significantly improves the optional MDA MAC to provide a much higher video call capacity and much less MDA signalling overhead. Extensive simulations are carried out with the proposed scheme using Network Simulator 2 (NS-2). The results of simulation demonstrate that the proposed scheme significantly improves the end-to-end delay performance of real-time traffic, Packet Delivery Ratio (PDR), jitter and system throughput, as compared with Delivery Traffic Indication Message (DTIM) scheme.","","978-1-4673-6817-9","10.1109/ICESA.2015.7503309","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7503309","WMN;MAC;Video transmission;EMDA;DTIM;NS-2","Streaming media;Logic gates;Routing;Delays;Throughput;Bandwidth;Topology","access protocols;jitter;multimedia communication;quality of service;synchronisation;telecommunication control;telecommunication signalling;telecommunication traffic;video streaming;video surveillance;wireless LAN;wireless mesh networks","wireless mesh network;WMN;video transmission;IEEE 802.11;multimedia streaming;streaming video;packet loss;data rate flow;live surveillance applications;forward error control;FEC technique;video signals;quality of service;QoS parameters;packet delivery ratio;PDR;medium access control;MDA MAC;medium reservations;enhanced mesh deterministic access;EMDA;per-node reservation;video call capacity;MDA signalling overhead;Network Simulator 2;NS-2;end-to-end delay performance;delivery traffic indication message;DTIM scheme","","1","","11","","4 Jul 2016","","","IEEE","IEEE Conferences"
"Enhancing video viewing-experience in opportunistic networks based on SVC, an experimental study","M. Klaghstan; D. Coquil; N. Bennani; H. Kosch; L. Brunie","University of Passau, Department of Distributed and Multimedia Information Systems; University of Passau, Department of Distributed and Multimedia Information Systems; University of Lyon, CNRS, INSA-Lyon, LIRIS; University of Passau, Department of Distributed and Multimedia Information Systems; University of Lyon, CNRS, INSA-Lyon, LIRIS","2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","25 Nov 2013","2013","","","3563","3567","Opportunistic networks are generally characterized by a low performance due to user mobility and non end-to-end communications, which makes it challenging to use them in scenarios involving the transmission of large data, such as video transmission. This article presents simulated experiments of transmitting different video sequences in opportunistic networks, with the goal of enhancing user video viewing experience. We consider this subjective concept objectively as a trade-off between playout delay and the delivered quality level. To obtain the best trade-off, we propose using Scalable Video Coding, which divides a video into multiple unequally important layers, providing different levels of quality. This property is exploited to transmit video layers with unequal degrees of redundancy proportionally to their degree of importance. Experimental results show that this approach enables the video to become more quickly playable at the destination with a low starting quality but progressively being enhanced, providing a better viewing-experience.","2166-9589","978-1-4673-6235-1","10.1109/PIMRC.2013.6666767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666767","Scalable Video Coding;Opportunistic Networks;Video Quality;Latency","Streaming media;Static VAr compensators;Routing;Redundancy;Mobile computing;Mobile communication;Delays","image sequences;video coding","video viewing-experience enhancement;opportunistic networks;SVC;user mobility;nonend-to-end communications;video data transmission;video sequences;playout delay;delivered quality level;scalable video coding;video layers","","5","","15","","25 Nov 2013","","","IEEE","IEEE Conferences"
"An Immersive Video Experience with Real-Time View Synthesis Leveraging the Upcoming MIV Distribution Standard","J. Fleureau; B. Chupeau; F. Thudor; G. Briand; T. Tapie; R. Doré","Interdigital R&D,France; Interdigital R&D,France; Interdigital R&D,France; Interdigital R&D,France; Interdigital R&D,France; Interdigital R&D,France","2020 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","9 Jun 2020","2020","","","1","2","The upcoming MPEG Immersive Video (MIV) standard will enable storage and distribution of immersive video content over existing and future networks, for playback with 6 full or partial degrees of freedom of view position and orientation. The demo showcases a VOD server streaming MIV encoded immersive video contents up to a decoding client where an HMD is connected. The user can perceive the parallax as he moves his head (translation, rotation) when seated. The core contribution of the demo is a real-time GPU implementation of the virtual view synthesis at decoder side. An enhanced quality of rendered views is attained through a weighting strategy of source views contributions which leverages standardized metadata conveying information on the pruning decisions at encoder side.","","978-1-7281-1485-9","10.1109/ICMEW46912.2020.9105948","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9105948","immersive video;MPEG-I;MIV standard;view synthesis","Streaming media;Decoding;Cameras;Transform coding;Standards;Metadata;Synthesizers","graphics processing units;meta data;rendering (computer graphics);video coding;video on demand;video streaming","upcoming MPEG Immersive Video standard;upcoming MIV distribution standard;real-time view synthesis;Immersive Video experience;source views contributions;rendered views;virtual view synthesis;real-time GPU implementation;immersive video content;view position","","1","","4","","9 Jun 2020","","","IEEE","IEEE Conferences"
"A simplified approach to perceptual quality adaptation of multi-dimensional scalable video","G. Maddodi; B. Lee; Y. Qiao","Software Research Institute (SRI), Athlone Institute of Technology, (AIT), Co. Westmeath, Ireland; Software Research Institute (SRI), Athlone Institute of Technology, (AIT), Co. Westmeath, Ireland; Software Research Institute (SRI), Athlone Institute of Technology, (AIT), Co. Westmeath, Ireland","24th IET Irish Signals and Systems Conference (ISSC 2013)","7 Oct 2013","2013","","","1","7","Content Aware Networking (CAN) can enhance the distribution of media through awareness in the network of the content it is carrying. In this way functions such as content adaptation, content routing and resource allocation can be improved. Our current research investigates `in-network' content adaptation of scalable video coded streams (SVC) via a media-aware network element or MANE. We propose a fuzzy logic adaptation approach based on the user perceived quality of the video stream as well as a policy management based MANE architecture. The paper describes work in progress to define the concept and approach-with particular focus on the approach to codifying QoE/QoS relationships.","","978-1-84919-754-0","10.1049/ic.2013.0045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621231","Scalable Video Coding;Software Defined Networking;Media Aware Network Elements;Scalable Video Adaptation;Policy-based Network Management","","fuzzy logic;quality of service;resource allocation;telecommunication network routing;video coding;video streaming","perceptual quality adaptation;multidimensional scalable video;content aware networking;CAN;media distribution;in-network content adaptation;scalable video coded streams;SVC;media-aware network element;fuzzy logic adaptation approach;video stream;policy management based MANE architecture;QoE relationship;QoS relationship;content adaptation;content routing;resource allocation;software defined networking","","","","","","7 Oct 2013","","","IET","IET Conferences"
"Enhancing software-defined RAN with collaborative caching and scalable video coding","R. Yu; S. Qin; M. Bennis; X. Chen; G. Feng; Z. Han; G. Xue","Arizona State University, Tempe, AZ 85287, USA; UESTC, Chengdu 611731, China; University of Oulu, Oulu 90014, Finland; VTT Technical Research Centre of Finland; UESTC, Chengdu 611731, China; University of Houston, Houston, TX 77004, USA; Arizona State University, Tempe, AZ 85287, USA","2016 IEEE International Conference on Communications (ICC)","14 Jul 2016","2016","","","1","6","The ever increasing video demands from mobile users have posed great challenges to cellular networks. To address this issue, video caching in radio access networks (RANs) has been recognized as one of the enabling technologies in future 5G mobile networks, which brings contents near the end-users, reducing the transmission cost of duplicate contents, meanwhile increasing the Quality-of-Experience (QoE) of users. Inspired by the emerging software-defined networking technology, recent proposals have employed centralized collaborative caching among cells to further increase the caching capacity of the RAN. In this paper, we explore a new dimension in video caching in software-defined RANs to expand its capacity. We enable the controller with the capability to adaptively select the bitrates of videos received by users, in order to maximize the number and quality of video requests that can be served, meanwhile minimizing the transmission cost. To achieve this, we further incorporate Scalable Video Coding (SVC), which enables caching and serving sliced video layers that can serve different bitrates. We formulate the problem of joint video caching and scheduling as a reward maximization (cost minimization) problem. Based on the formulation, we further propose a 2-stage rounding-based algorithm to address the problem efficiently. Simulation results show that using SVC with collaborative caching greatly improves the cache capacity and the QoE of users.","1938-1883","978-1-4799-6664-6","10.1109/ICC.2016.7511029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7511029","Software-defined radio access network;collaborative video caching;Scalable Video Coding;5G mobile networks","Bit rate;Static VAr compensators;Streaming media;Mobile communication;Collaboration;Delays;Mobile computing","quality of experience;radio access networks;software defined networking;video coding;video communication","software-defined RAN;collaborative caching;scalable video coding;video demands;cellular networks;radio access networks;5G mobile networks;transmission cost;quality-of-experience;QoE;software-defined networking technology;transmission cost;video caching;scheduling;reward maximization;cost minimization problem;2-stage rounding-based algorithm","","27","","21","","14 Jul 2016","","","IEEE","IEEE Conferences"
"Overview of Screen Content Video Coding: Technologies, Standards, and Beyond","W. Peng; F. G. Walls; R. A. Cohen; J. Xu; J. Ostermann; A. MacInnis; T. Lin","National Chiao Tung University, Hsinchu, Taiwan; Broadcom Corporation, Irvine, CA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Microsoft Research Asia, Beijing, China; Leibniz Universitat Hannover, Hannover, Germany; Broadcom Corporation, Santa Clara, CA, USA; College of Electronics and Information Engineering, Tongji University, Shanghai, China","IEEE Journal on Emerging and Selected Topics in Circuits and Systems","12 Dec 2016","2016","6","4","393","408","This paper presents recent advances in screen content video coding, with an emphasis on two state-of-the-art standards: HEVC/H.265 Screen Content Coding Extensions (HEVC-SCC) by ISO/IEC Moving Picture Experts Group and ITU-T Video Coding Experts Group, and Display Stream Compression (DSC) by Video Electronics Standards Association. The HEVC-SCC enhances the capabilities of HEVC in coding screen content, while DSC provides lightweight compression for display links. Although targeting different application domains, they share some design principles and are expected to become the leading formats in the marketplace in the coming years. This paper provides a brief account of their background, key elements, performance, and complexity characteristics, according to their final specifications. As we survey these standards, we also summarize prior arts in the last decade and explore future research opportunities and standards developments in order to give a comprehensive overview of this field.","2156-3365","","10.1109/JETCAS.2016.2608971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7769190","Display Stream Compression (DSC);High Efficiency Video Coding (HEVC/H.265);Joint Collaborative Team on Video Coding (JCT-VC);Screen Content Video Coding (SCC);Video Electronics Standards Association (VESA)","High efficiency video coding;Standards;Video coding;Streaming media;Transforms;ISO Standards;ITU Standards","data compression;video coding","screen content video coding;HEVC-H.265 screen content coding extension;HEVC-SCC;ISO/IEC moving picture expert group;ITU-T video coding expert group;display stream compression;DSC;video electronic standard association;display link;lightweight compression;leading format;complexity characteristics","","34","","118","","5 Dec 2016","","","IEEE","IEEE Journals"
"A Decision-Tree-Based Perceptual Video Quality Prediction Model and Its Application in FEC for Wireless Multimedia Communications","A. Hameed; R. Dai; B. Balas","Department of Computer Science, COMSATS Institute of Information Technology, Islamabad, Pakistan; Department of Electrical Engineering and Computing Systems, University of Cincinnati, Cincinnati, OH, USA; Department of Psychology, North Dakota State University, Fargo, ND, USA","IEEE Transactions on Multimedia","15 Mar 2016","2016","18","4","764","774","With the exponential growth of video traffic over wireless networked and embedded devices, mechanisms are needed to predict and control perceptual video quality to meet the quality of experience (QoE) requirements in an energy-efficient way. This paper proposes an energy-efficient QoE support framework for wireless video communications. It consists of two components: 1) a perceptual video quality model that allows the prediction of video quality in real-time and with low complexity, and 2) an application layer energy-efficient and content-aware forward error correction (FEC) scheme for preventing quality degradation caused by network packet losses. The perceptual video quality model characterizes factors related to video content as well as distortion caused by compression and transmission. Prediction of perceptual quality is achieved through a decision tree using a set of observable features from the compressed bitstream and the network. The proposed model can achieve prediction accuracy of 88.9% and 90.5% on two distinct testing sets. Based on the proposed quality model, a novel FEC scheme is introduced to protect video packets from losses during transmission. Given a user-defined perceptual quality requirement, the FEC scheme adjusts the level of protection for different components in a video stream to minimize network overhead. Simulation results show that the proposed FEC scheme can enhance the perceptual quality of videos. Compared to conventional FEC methods for video communications, the proposed FEC scheme can reduce network overhead by 41% on average.","1941-0077","","10.1109/TMM.2016.2525862","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7399416","Perceptual video quality;quality of experience;video coding;forward error correction;wireless multimedia communications;energy-efficiency;Energy-efficiency;forward error correction (FEC);perceptual video quality;quality of experience (QoE);video coding;wireless multimedia communications","Forward error correction;Video recording;Quality assessment;Predictive models;Streaming media;Packet loss","decision trees;forward error correction;quality of experience;video coding;wireless channels","video traffic;wireless networked devices;embedded devices;quality of experience requirements;QoE requirements;energy-efficient QoE support framework;wireless video communications;perceptual video quality model;video quality prediction;application layer energy-efficient FEC scheme;content-aware forward error correction scheme;quality degradation;decision tree;compressed bitstream;video packets;user-defined perceptual quality requirement","","35","","31","","4 Feb 2016","","","IEEE","IEEE Journals"
"Dispersive Video Frame Importance Driven Probabilistic Packet Mapping for 802.11e Based Video Transmission","W. Lai; E. Liou; W. Fu","Dept. of Commun. Eng., Yuan Ze Univ., Taoyuan, Taiwan; Dept. of Commun. Eng., Yuan Ze Univ., Taoyuan, Taiwan; Dept. of Commun. Eng., Yuan Ze Univ., Taoyuan, Taiwan","2011 Third International Conference on Intelligent Networking and Collaborative Systems","19 Jan 2012","2011","","","424","429","The FIFO characteristic of the access category queue (AC_VI) reserved for video in the 802.11e MAC limits its performance for high quality video transmission during traffic congestions. To enhance video transmission quality, further service differentiation among the video packets is preferred. To achieve this, this paper presents a novel cross-layer design by proposing a dispersive video frame importance (DVFI) scheme from the Application layer and a dispersive random early detection (DRED) algorithm for video packet mapping among ACs. DVFI is calculated from a single frame loss induced video quality impairment, including the effects of imperfect concealment for the lost video frame itself and its error propagation to the surrounding video frames. Whereas DRED can dynamically determine the packet mapping probabilities among ACs based on the dispersive values of packet importance. Our results show the superiority of the proposed (DVFI+DRED) framework over the existing ones, including EDCA and (I/P/B+AM), and it can reach a largest gain of 3.2 dB over EDCA, and 2.6 dB over (I/P/B+AM).","","978-1-4577-1908-0","10.1109/INCoS.2011.77","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132843","cross layer design;dispersive video frame importance;dispersive random early detection;802.11e;quality of service","Streaming media;Dispersion;IEEE 802.11e Standard;Indexes;PSNR;Encoding;Video sequences","multimedia communication;video communication;wireless LAN","dispersive video frame importance driven probabilistic packet mapping;802.11e based video transmission;FIFO characteristic;access category queue;802.11e MAC;high quality video transmission;traffic congestions;video transmission quality;video packet mapping;video quality impairment","","3","","11","","19 Jan 2012","","","IEEE","IEEE Conferences"
"High-Performance Low-Area Video Up-Scaling Architecture for 4-K UHD Video","J. Lee; I. Park","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Circuits and Systems II: Express Briefs","19 May 2017","2017","64","4","437","441","A new algorithm and its hardware architecture are presented to up-scale high-definition (HD) and full-HD video streams to 4-K ultra-HD video streams in real time. The Lagrange interpolation is employed, as it provides high estimation accuracy and hardware-friendly properties. To enhance the accuracy further, the pixels at the edge regions are specially processed by employing an image-sharpening technique. Experimental results show that the proposed architecture provides the best visual quality at the cost of reasonable hardware resources.","1558-3791","","10.1109/TCSII.2016.2563818","Center for Integrated Smart Sensors; Ministry of Science, ICT & Future Planning(grant numbers:CISS-2011-0031860); IC Design Education Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7465786","Lagrange interpolation;Laplacian edge detection;sharpening filter;ultra high-definition (UHD) video;video up-scaling","Interpolation;Image edge detection;Streaming media;Computer architecture;Hardware;Laplace equations;Kernel","high definition video;image processing;video streaming","ultra-HD video streams;high-performance video up-scaling architecture;low-area video up-scaling architecture;hardware architecture;up-scale high-definition video streams;Lagrange interpolation;estimation accuracy;hardware-friendly properties;image sharpening","","11","","14","","5 May 2016","","","IEEE","IEEE Journals"
"Joint Communication and Computational Resource Allocation for QoE-driven Point Cloud Video Streaming","J. Li; C. Zhang; Z. Liu; W. Sun; Q. Li","Hefei University of Technology,School of Computer and Information,China; Hefei University of Technology,School of Computer and Information,China; Shizuoka University,Department of Mathematical and Systems Engineering,Japan; Hefei University of Technology,School of Electrical Engineering and Automation,Hefei,China; Hefei University of Technology,School of Electrical Engineering and Automation,Hefei,China","ICC 2020 - 2020 IEEE International Conference on Communications (ICC)","27 Jul 2020","2020","","","1","6","Point cloud video is the most popular representation of hologram, which is the medium to precedent natural content in VR/AR/MR and is expected to be the next generation video. Point cloud video system provides users immersive viewing experience with six degrees of freedom (6DoF) and has wide applications in many fields such as online education and entertainment. To further enhance these applications, point cloud video streaming is in critical demand. The inherent challenges lie in the large size by the necessity of recording the three-dimensional coordinates besides color information, and the associated high computation complexity of encoding/decoding. To this end, this paper proposes a communication and computational resource allocation scheme for QoE-driven point cloud video streaming. In particular, with the goal to maximize the defined QoE by selecting proper quality levels (uncompressed tiles at different quality levels are also considered) for each partitioned point cloud video tile, we formulate this into an optimization problem under the limited communication and computational resources constraints and propose a scheme to solve it. Extensive simulations are conducted and the simulation results show the superior performance of the proposed scheme over the existing schemes.","1938-1883","978-1-7281-5089-5","10.1109/ICC40277.2020.9148922","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9148922","point cloud video;hologram video;QoE;video streaming;immersive video;6DoF;resource allocation","Three-dimensional displays;Streaming media;Decoding;Quality of experience;Bandwidth;Optical fibers;Resource management","computational complexity;optimisation;quality of experience;resource allocation;video signal processing;video streaming","QoE-driven point cloud video streaming;next-generation video;point cloud video system;computation complexity;computational resource allocation scheme;point cloud video tile;quality levels;encoding-decoding;six degrees of freedom;6DoF;hologram","","2","","17","","27 Jul 2020","","","IEEE","IEEE Conferences"
"Adaptive Transmission for Delay-Constrained Wireless Video","C. Gong; X. Wang","Qualcomm Research, San Diego, CA; Electrical Engineering Department, Columbia University, New York, NY 10027","IEEE Transactions on Wireless Communications","24 Jan 2014","2014","13","1","49","61","We consider a point-to-point delay-stringent video communication system. To reduce the fluctuation of the coded data packet sizes for efficient wireless transmission, we adopt the intra-refreshment and adaptive slice partitioning for video encoding. We model the delay-constrained video transmission with the proposed video coding and slicing scheme using an MDP model, and compute the optimal coding and modulation scheme corresponding to each channel state and the encoding and transmission delay using the value iteration algorithm. Extensive simulations based on real videos show that the MDP-based transmission scheme significantly reduces the transmission cost and enhances the quality of the reconstructed video sequences compared with the non-adaptive transmission scheme, which transmits constant number of transmitted symbols for all slices. Simulation results also show that the intra-refreshment and adaptive slice partitioning significantly enhances the quality of the reconstructed video sequences compared with the video coding without intra-refreshment.","1558-2248","","10.1109/TWC.2013.112613.121033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6678674","H.264;wireless video transmission;stringent delay constraint;adaptive coded modulation;Markov decision process","Encoding;Delays;Streaming media;Wireless communication;Video coding;PSNR;Video sequences","iterative methods;modulation coding;video coding;video communication","delay-constrained wireless video;point-to-point communication system;delay-stringent communication system;video communication system;coded data packet sizes;wireless transmission;intra-refreshment partitioning;adaptive slice partitioning;delay-constrained video transmission;video coding;slicing scheme;MDP model;optimal coding;modulation scheme;channel state;transmission delay;value iteration;video sequences;nonadaptive transmission","","18","","23","","5 Dec 2013","","","IEEE","IEEE Journals"
"Indexing and keyword search to ease navigation in lecture videos","T. Tuna; J. Subhlok; S. Shah","University of Houston, Computer Science Department, TX 77204-3010, USA; University of Houston, Computer Science Department, TX 77204-3010, USA; University of Houston, Computer Science Department, TX 77204-3010, USA","2011 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)","3 Apr 2012","2011","","","1","8","Lecture videos have been commonly used to supplement in-class teaching and for distance learning. Videos recorded during in-class teaching and made accessible online are a versatile resource on par with a textbook and the classroom itself. Nonetheless, the adoption of lecture videos has been limited, in large part due to the difficulty of quickly accessing the content of interest in a long video lecture. In this work, we present “video indexing” and “keyword search” that facilitate access to video content and enhances user experience. Video indexing divides a video lecture into segments indicating different topics by identifying scene changes based on the analysis of the difference image from a pair of video frames. We propose an efficient indexing algorithm that leverages the unique features of lecture videos. Binary search with frame sampling is employed to efficiently analyze long videos. Keyword search identifies video segments that match a particular keyword. Since text in a video frame often contains a diversity of colors, font sizes and backgrounds, our text detection approach requires specialized preprocessing followed by the use of off-the-shelf OCR engines, which are designed primarily for scanned documents. We present image enhancements: text segmentation and inversion, to increase detection accuracy of OCR tools. Experimental results on a suite of diverse video lectures were used to validate the methods developed in this work. Average processing time for a one-hour lecture is around 14 minutes on a typical desktop. Search accuracy of three distinct OCR engines - Tesseract, GOCR and MODI increased significantly with our preprocessing transformations, yielding an overall combined accuracy of 97%. The work presented here is part of a video streaming framework deployed at multiple campuses serving hundreds of lecture videos.","2332-5615","978-1-4673-0216-6","10.1109/AIPR.2011.6176364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176364","video search;optical character recognition;video indexing;lecture videos;distance learning","Videos;Optical character recognition software;Indexing;Image segmentation;Image edge detection;Engines","computer aided instruction;distance learning;image enhancement;image segmentation;indexing;object detection;optical character recognition;teaching;text analysis;video signal processing","video indexing;keyword search;lecture video;in-class teaching;distance learning;user experience;video content;scene change;difference image;video frame;binary search;frame sampling;text detection approach;OCR engine;optical character recognition;image enhancement;text segmentation;text inversion;Tesseract engine;GOCR engine;MODI engine;video streaming framework","","12","","14","","3 Apr 2012","","","IEEE","IEEE Conferences"
"Removing glint with video processing to enhance underwater target detection","V. Scholl; A. Gerace","Digital Image Remote Sensing Laboratory Carlson Center for Imaging Science Rochester Institute of Technology Rochester, NY; Digital Image Remote Sensing Laboratory Carlson Center for Imaging Science Rochester Institute of Technology Rochester, NY","2013 IEEE Western New York Image Processing Workshop (WNYIPW)","8 Sep 2014","2013","","","18","21","Remotely sensed imagery of large bodies of water is often dappled with bright patches known as glint. Solar glint is light originating from the sun that reflects off the water surface directly into a camera's sensor. Glint reduces the ability to observe the water at depth, making complicated problems such as in-water parameter retrieval, benthic mapping, and submerged target detection especially difficult. The purpose of this research is two-fold. First, it is hypothesized that the latency between spectral bands on typical pushbroom imaging systems can be utilized to remove glint. The experimental concept of using video and basic image processing techniques is explored using a monochrome camera. Secondly, ongoing efforts are focused on characterizing the key features of glint (size, shape, intensity, and duration) to provide insight for improved glint removal algorithms.","","978-1-4799-3026-5","10.1109/WNYIPW.2013.6890982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890982","Glint;remote sensing;water;video","Object detection;Sun;Cameras;Remote sensing;Surface waves;Shape","image sensors;object detection;remote sensing;video cameras;video signal processing","video processing;underwater target detection;remote sensing;solar glint features;pushbroom imaging systems;image processing techniques;monochrome camera sensor;parameter retrieval;benthic mapping;spectral bands","","1","","5","","8 Sep 2014","","","IEEE","IEEE Conferences"
"Gradual view refresh in depth-enhanced multiview video","M. M. Hannuksela; L. Chen; D. Rusanovskyy; H. Li","Nokia Research Center Tampere, Finland; University of Science and Technology of China Hefei, China; Nokia Research Center Tampere, Finland; University of Science and Technology of China Hefei, China","2012 Picture Coding Symposium","7 Jun 2012","2012","","","141","144","Depth-enhanced multiview video, such as the multiview video plus depth (MVD) format, can be used to provide displaying-time view adjustment capability through depth-image-based rendering (DIBR) and additional compression improvement compared to the Multiview Video Coding standard. In this paper a gradual view refresh (GVR) method is presented to code random access points and provide fast startup in streaming for MVD bitstreams. When decoding is started from a GVR point, a subset of the views can be accurately decoded, while the remaining views can be approximately reconstructed using DIBR. Perfect reconstruction of all views can be reached at a subsequent random access point. The GVR coding was found to be effective with up to 10% bitrate reduction for sequences with static camera arrangement. The use of GVR for fast startup in video streaming was found to be clearly superior to transmitting a MVD bitstream conventionally from rate-distortion point of view. It has been found in earlier studies that there seems to be a delay from stimulus onset until depth is fully perceived, hence giving a reason to believe that accurate reconstruction of all views might not be necessary immediately after starting decoding. Furthermore, it was observed in this paper that the objective picture quality reduction during GVR was only moderate, verifying the applicability of the presented GVR method.","","978-1-4577-2049-9","10.1109/PCS.2012.6213306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6213306","3D video;random access;MVC;MVD;streaming;GVR","Encoding;Decoding;Bit rate;Streaming media;Stereo image processing;PSNR;Transform coding","data compression;image enhancement;image reconstruction;image sensors;video coding;video streaming","gradual view refresh method;depth-enhanced multiview video;multiview video plus depth format;MVD format;displaying-time view adjustment;depth-image-based rendering;DIBR;multiview video coding standard;MVD bitstreams;decoding;GVR point;GVR coding;bitrate reduction;static camera arrangement;video streaming;objective picture quality reduction","","3","3","10","","7 Jun 2012","","","IEEE","IEEE Conferences"
"A training data cleaning framework for video distortion diagnosis","M. Yang; L. Chen; J. Tian","School of Computer Science of Technology, Wuhan University of Science of Technology, Wuhan China, 430065. Hubei Province Key Laboratory of Intelligent Information Processing and Real-time Industrial System, Wuhan University of Science and Technology, Wuhan China, 430065; School of Computer Science of Technology, Wuhan University of Science of Technology, Wuhan China, 430065. Hubei Province Key Laboratory of Intelligent Information Processing and Real-time Industrial System, Wuhan University of Science and Technology, Wuhan China, 430065; School of Computer Science of Technology, Wuhan University of Science of Technology, Wuhan China, 430065. Hubei Province Key Laboratory of Intelligent Information Processing and Real-time Industrial System, Wuhan University of Science and Technology, Wuhan China, 430065","2017 12th IEEE Conference on Industrial Electronics and Applications (ICIEA)","8 Feb 2018","2017","","","214","217","Intelligent video classification aims to exploit machine learning and artificial intelligence techniques to classify videos and detect abnormal video sequences. However, the training data used in video classification is manually selected, which is vulnerable to be influenced by human. To tackle this issue, automatic training data cleaning framework is required to improve the efficiency of data cleaning and is potential to enhancing the robustness of video classification algorithms. The proposed framework contains two significant contributions. First, the conventional categories of video distortions are studied and integrated into a simplified set. Second, a new decision strategy is proposed in this paper to handle the case that the video is labeled as different type of distortions by different assessors. The proposed framework is evaluated in real-world video surveillance database to demonstrate that it achieves larger robustness and accuracy than conventional framework.","2158-2297","978-1-5090-6161-7","10.1109/ICIEA.2017.8282844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8282844","video diagnosis algorithm;data cleaning;video diagnostic classification","Distortion;Cleaning;Image color analysis;Training data;Classification algorithms;Streaming media;Interference","data handling;image classification;image sequences;learning (artificial intelligence);object detection;pattern classification;video signal processing;video surveillance","automatic training data cleaning framework;real-world video surveillance database;conventional framework;video distortion diagnosis;intelligent video classification;machine learning;artificial intelligence techniques;abnormal video sequence detection;video classification","","","","9","","8 Feb 2018","","","IEEE","IEEE Conferences"
"Deep Video Prediction Network-Based Inter-Frame Coding in HEVC","J. Lee; N. Kim; S. Cho; J. Kang","Department of Electronic and Electrical Engineering, Ewha Womans University, Seoul, South Korea; Department of Electronic and Electrical Engineering, Ewha Womans University, Seoul, South Korea; Department of Information and Communication Engineering, Kyungnam University, Changwon, South Korea; Department of Electronic and Electrical Engineering, Ewha Womans University, Seoul, South Korea","IEEE Access","1 Jun 2020","2020","8","","95906","95917","In this paper, we propose a novel Convolutional Neural Network (CNN) based video coding technique using a video prediction network (VPN) to support enhanced motion prediction in High Efficiency Video Coding (HEVC). Specifically, we design a CNN VPN to generate a virtual reference frame (VRF), which is synthesized using previously coded frames, to improve coding efficiency. The proposed VPN uses two sub-VPN architectures in cascade to predict the current frame in the same time instance. The VRF is expected to have higher temporal correlation than a conventional reference frame, and, thus it is substituted for a conventional reference frame. The proposed technique is incorporated into the HEVC inter-coding framework. Particularly, the VRF is managed in a HEVC reference picture list, so that each prediction unit (PU) can choose a better prediction signal through Rate-Distortion optimization without any additional side information. Furthermore, we modify the HEVC inter-prediction mechanisms of Advanced Motion Vector Prediction and Merge modes adaptively when the current PU uses the VRF as a reference frame. In this manner, the proposed technique can exploit the PU-wise multi-hypothesis prediction techniques in HEVC. Since the proposed VPN can perform both the video interpolation and extrapolation, it can be used for Random Access (RA) and Low Delay B (LD) coding configurations. It is shown in experimental results that the proposed technique provides -2.9% and -5.7% coding gains, respectively, in RA and LD coding configurations as compared to the HEVC reference software, HM 16.6 version.","2169-3536","","10.1109/ACCESS.2020.2993566","Institute of Information and Communications Technology Planning and Evaluation (IITP); Korea government (MSIT) (Development of Audio/Video Coding and Light Field Media Fundamental Technologies for Ultra Realistic Tera-Media and Development of Compression and Transmission Technologies for Ultra High Quality Immersive Videos Supporting 6DoF)(grant numbers:2017-0-00072,2018-0-00765); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9090881","Video coding;deep learning;convolutional neural network;video prediction network;inter-prediction;virtual reference frame;HEVC;VVC","Virtual private networks;Encoding;Video coding;Kernel;Interpolation;Extrapolation;Convolution","convolutional neural nets;extrapolation;interpolation;optimisation;video coding","convolutional neural network;motion prediction;high efficiency video coding;CNN VPN;virtual reference frame;VRF;sub-VPN architectures;HEVC inter-coding framework;HEVC reference picture list;HEVC inter-prediction mechanisms;PU-wise multihypothesis prediction techniques;video interpolation;video extrapolation;HEVC reference software;deep video prediction network;inter-frame coding;low delay B coding configurations;rate-distortion optimization","","3","","37","CCBY","11 May 2020","","","IEEE","IEEE Journals"
"Interleaving-based error concealment for scalable video coding system","B. Zhao","School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana, USA","2011 Visual Communications and Image Processing (VCIP)","29 Dec 2011","2011","","","1","4","Scalable video coding (SVC) is desirable for video communication in heterogeneous environments where end-users retain different types of terminals. Because motion estimation and compensation are used in SVC to reduce redundancy between frames, the visual quality degrades due to the errors of transmission channels and the error propagation across multiple frames. In order to decrease error propagation and enhance visual quality of reconstructed frames, error concealment method is used to estimate the erroneous motion vectors by exploiting the spatial and temporal correlation of video frames in different layers. Compared with two existing error concealment methods, an improved error concealment method is proposed with incorporation of interleaving technique. A spatial-temporal scalable video coding system is implemented in two layers. Especially, the impact of burst errors on motion vectors are investigated in this paper. Experimental results verify that the proposed error concealment method with interleaving technique outperforms the existing methods.","","978-1-4577-1322-4","10.1109/VCIP.2011.6115965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6115965","Scalable video coding;video communication;error concealment;interleaving technique;motion estimation","Vectors;Video coding;Video sequences;PSNR;Error analysis;Spatial resolution;Decoding","image enhancement;motion compensation;motion estimation;video coding","interleaving-based error concealment method;video communication;motion vector estimation;motion compensation;visual quality enhancement;transmission channel error;error propagation;spatial-temporal scalable video coding system","","","","7","","29 Dec 2011","","","IEEE","IEEE Conferences"
"Predictive patch matching method for inter frame coding in advanced video coding","T. Talawar; N. M. Naik; B. D. Parameshachari; R. Banu; Rajashekarappa","Dept of ECE, UBDTCE, Davangere, India (Affiliated to VTU, Belagavi); Dept of ECE, UBDTCE, Davangere, India (Affiliated to VTU, Belagavi); Dept of TCE, GSSSIETW, Mysuru, India (Affiliated to VTU, Belagavi); Dept of ISE, GSSSIETW, Mysuru, India, (Affiliated to VTU, Belagavi); Dept. of ISE, SDMCET, Dharwad (Affiliated to VTU, Belagavi)","2017 International Conference on Electrical, Electronics, Communication, Computer, and Optimization Techniques (ICEECCOT)","8 Feb 2018","2017","","","1","5","In video coding, inter-frame prediction acts a vital role in convalescing (improving) compression competence. The enhanced efficiency is attained by finding the predictors for video blocks such that residual information is close to zero as much as promising. The modern video coding standards, motion vectors needed for the decoder to situate the predictors through the video reconstruction. The block matching algorithms can be employed in stage of the motion estimation to discover the motion vectors. A linear combination of rebuild the predicted pixels permit to produce a prediction though maintaining a local variation of block. Then the mode system is introduced to adaptively choose the predictive patch matching at the sub block level and H.264. The experimental results express the efficiency of propose predictive patch matching method.","","978-1-5386-2361-9","10.1109/ICEECCOT.2017.8284633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8284633","Video Coding;Predictive Patch Matching;Integer Transform and Quantization;H.264 and Encoder CAVLC","Encoding;Video coding;Transforms;Motion estimation;Image coding;Standards;Decoding","data compression;image matching;image reconstruction;motion estimation;video coding","inter frame coding;advanced video coding;inter-frame prediction;compression competence;video blocks;residual information;modern video coding standards;motion vectors;video reconstruction;block matching algorithms;motion estimation;sub block level;predictive patch matching method","","","","12","","8 Feb 2018","","","IEEE","IEEE Conferences"
"Using QRCode to Enhance Extraction Efficiency of Video Watermark Algorithm","Z. Lv; H. Guan; Y. Huang; S. Zhang","University of Chinese Academy of Sciences,Institute of Automation, Chinese Academy of Sciences,Beijing,China; Beijing Engineering Research Center of Digital Content Technology,Beijing,China; Chinese Academy of Sciences,Institute of Automation,Beijing,China; Beijing Engineering Research Center of Digital Content Technology,Beijing,China","2020 International Conference on Culture-oriented Science & Technology (ICCST)","24 Nov 2020","2020","","","336","339","Video watermarking can effectively protect the copyright of video contents, but how to improve efficiency of watermarking algorithms is an urgent problem to be solved. In this paper, QRCode is embedded in the scene change frames of videos based on the advantage of QRCode's strong fault tolerance. Combined with the characteristic of high decoding reliability of QRCode, a strategy to terminate the extraction process in advance is proposed to improve the extraction efficiency of the watermark algorithm. Experimental results show that the proposed algorithm has higher extraction efficiency than the algorithm that directly uses character string as watermark.","","978-1-7281-8138-7","10.1109/ICCST50977.2020.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9262781","video watermark;QRCode;extraction efficiency","Watermarking;Decoding;Data mining;Reliability;Streaming media;Media;Histograms","fault tolerance;video coding;video watermarking","decoding reliability;video watermark algorithm;video watermarking;video contents;QRCode fault tolerance;character string","","","","11","","24 Nov 2020","","","IEEE","IEEE Conferences"
"A Switchable Deep Learning Approach for In-Loop Filtering in Video Coding","D. Ding; L. Kong; G. Chen; Z. Liu; Y. Fang","School of Information Science and Engineering, Hangzhou Normal University, Hangzhou, China; School of Information Science and Engineering, Hangzhou Normal University, Hangzhou, China; School of Information Science and Engineering, Hangzhou Normal University, Hangzhou, China; Visionular Inc., Mountain View, CA, USA; School of Information Engineering, Chang’an University, Xi’an, China","IEEE Transactions on Circuits and Systems for Video Technology","1 Jul 2020","2020","30","7","1871","1887","Deep learning provides a great potential for in-loop filtering to improve both coding efficiency and subjective quality in video coding. State-of-the-art work focuses on network structure design and employs a single powerful network to solve all problems. In contrast, this paper proposes a deep learning based systematic approach that includes an effective Convolutional Neural Network (CNN) structure, a hierarchical training strategy, and a video codec oriented switchable mechanism. First, we propose a novel CNN structure, i.e., Squeeze-and-Excitation Filtering CNN (SEFCNN), as an optional in-loop filter. To capture the non-linear interaction between channels, the SEFCNN is comprised of two subnets, i.e., Feature EXtracting (FEX) subnet and Feature ENhancing (FEN) subnet. Then, we develop a hierarchical model training strategy to adapt the two subnets to different coding scenarios. For high-rate videos with small artifacts, we train a single global model using the FEX for all types of frames, whereas for low-rate videos with large artifacts, different models are trained using both FEX and FEN for different types of frames. Finally, we propose an adaptive enhancing mechanism which is switchable between the CNN-based and the conventional methods. We selectively apply the CNN model to some frames or some regions in a frame. Experimental results show that the proposed scheme outperforms state-of-the-art work in coding efficiency, while the computational complexity is acceptable after GPU acceleration.","1558-2205","","10.1109/TCSVT.2019.2935508","National Key Research and Development Program of China(grant numbers:2017YFB1002803); Google Chrome University Research Program; Fundamental Research Fund for the Central Universities of China(grant numbers:300102249304,310824173601,300102248303); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8801877","CNN;in-loop filter;video coding;enhancement","Encoding;Video coding;Feature extraction;Adaptation models;Tools;Training;Correlation","convolutional neural nets;feature extraction;image enhancement;image filtering;learning (artificial intelligence);video codecs;video coding","convolutional neural network;video codec;hierarchical model training;video coding;in-loop filtering;switchable deep learning;squeeze-and-excitation filtering CNN;feature extracting subnet;feature enhancing subnet","","5","","51","IEEE","15 Aug 2019","","","IEEE","IEEE Journals"
"Enhancing Quality for HEVC Compressed Videos","R. Yang; M. Xu; T. Liu; Z. Wang; Z. Guan","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","1 Jul 2019","2019","29","7","2039","2054","The latest High Efficiency Video Coding (HEVC) standard has been increasingly applied to generate video streams over the Internet. However, HEVC compressed videos may incur severe quality degradation, particularly at low bit rates. Thus, it is necessary to enhance the visual quality of HEVC videos at the decoder side. To this end, this paper proposes a quality enhancement convolutional neural network (QE-CNN) method that does not require any modification of the encoder to achieve quality enhancement for HEVC. In particular, our QE-CNN method learns QE-CNN-I and QE-CNN-P models to reduce the distortion of HEVC I and P/B frames, respectively. The proposed method differs from the existing CNN-based quality enhancement approaches, which only handle intra-coding distortion and are thus not suitable for P/B frames. Our experimental results validate that our QE-CNN method is effective in enhancing quality for both I and P/B frames of HEVC videos. To apply our QE-CNN method in time-constrained scenarios, we further propose a time-constrained quality enhancement optimization (TQEO) scheme. Our TQEO scheme controls the computational time of QE-CNN to meet a target, meanwhile maximizing the quality enhancement. Next, the experimental results demonstrate the effectiveness of our TQEO scheme from the aspects of time control accuracy and quality enhancement under different time constraints. Finally, we design a prototype to implement our TQEO scheme in a real-time scenario.","1558-2205","","10.1109/TCSVT.2018.2867568","National Natural Science Foundation of China(grant numbers:61876013,61573037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8450025","HEVC;quality enhancement;convolutional neural network","Decoding;Videos;Transform coding;Visualization;Distortion;Encoding;Complexity theory","convolutional neural nets;data compression;image enhancement;optimisation;video coding;video streaming","TQEO scheme;HEVC compressed videos;video streams;severe quality degradation;visual quality;quality enhancement convolutional neural network;QE-CNN method;P/B frames;time-constrained quality enhancement optimization scheme;high efficiency video coding standard;Internet;QE-CNN-I model;QE-CNN-P model;CNN-based quality enhancement approaches;time constraints","","22","","42","","29 Aug 2018","","","IEEE","IEEE Journals"
"Enhancing Quality of Experience (QoE) assessment models for video applications","X. Li; S. Aggarwal; A. Singh; A. Könsgen; C. Görg; M. Kus","Communication Networks, University of Bremen, Gemrany; Communication Networks, University of Bremen, Gemrany; Communication Networks, University of Bremen, Gemrany; Communication Networks, University of Bremen, Gemrany; Communication Networks, University of Bremen, Gemrany; OTRARIS Interactive Services GmbH Bremen, Germany","6th Joint IFIP Wireless and Mobile Networking Conference (WMNC)","27 Jun 2013","2013","","","1","4","Video applications are becoming the key services in today's networks (both fixed networks and mobile networks). Consideration of video service quality has become essential to provide the end users with satisfying Quality of Experience (QoE). In order to evaluate and manage the video quality, methods for QoE assessment are desired to estimate the service quality perceived by the end users. In this paper, we study a number of existing objective quality assessment models for assessing the QoE of video applications, and compare their performance with simulations to find out their individual advantages and limitations to use in practice. To overcome the shortcomings of the existing models, this paper proposes an enhanced QoE model which considers a wide range of codec rates and video formats, and moreover considers the packet losses and video content dependence in quality assessment. Simulation results show that the proposed QoE model can be applied for evaluating the quality of different video sources in the Long Term Evolution (LTE) networks, considering the lossy property of mobile networks. In addition to the objective QoE methods, we also plan to carry out subjective tests in the lab as well as in the real system for the subjective evoluation of QoE.","","978-1-4673-5616-9","10.1109/WMNC.2013.6548959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6548959","","Quality assessment;Bit rate;Video recording;Packet loss;Complexity theory;Streaming media","Long Term Evolution;quality of experience;quality of service;video codecs;video coding","quality of experience assessment models;video applications;mobile networks;fixed networks;video service quality;objective quality assessment models;enhanced QoE model;codec rates;video formats;packet losses;video content dependence;Long Term Evolution networks;LTE networks","","2","","11","","27 Jun 2013","","","IEEE","IEEE Conferences"
"Streaming Location-Based Panorama Videos into Augmented Virtual Environment","Y. Zhou; P. Liu; J. You; Z. Zhou","State Key Lab. of Virtual Reality Technol. & Syst., Beihang Univ., Beijing, China; State Key Lab. of Virtual Reality Technol. & Syst., Beihang Univ., Beijing, China; State Key Lab. of Virtual Reality Technol. & Syst., Beihang Univ., Beijing, China; State Key Lab. of Virtual Reality Technol. & Syst., Beihang Univ., Beijing, China","2014 International Conference on Virtual Reality and Visualization","1 Oct 2015","2014","","","208","213","Location-based panorama systems such as Google Street View let users explore places around the world through panoramic bubbles or strips. The panorama image is easy to be deployed, but it can only provide the static views of capturing time and lacks developing process. In this paper, we present an augmented virtual environment system that combines multiple location-based panorama videos with the structural context of scenes. The raw panorama images are from several independent video cameras. A frame synchronization method of video streams is proposed to provide the temporal consistency in the panorama stitching. Our novel method augments the virtual environment through mixing it with the panorama videos. To the best of our knowledge, this is the first paper to fuse panorama videos with virtual environments. The system is demonstrated in a campus-wide area, and it enhances users' walk-through experiences in the experiment environment.","","978-1-4799-6854-1","10.1109/ICVRV.2014.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7281066","Panorama video;augmented virtual environment;Video streaming synchronization","Videos;Cameras;Synchronization;Servers;Three-dimensional displays;Virtual environments;Solid modeling","augmented reality;video cameras;video signal processing;video streaming","streaming location-based panorama video;location-based panorama system;Google street view;panoramic bubble;panoramic strip;panorama image;augmented virtual environment system;video camera;frame synchronization method;video stream;temporal consistency;panorama stitching","","","","16","","1 Oct 2015","","","IEEE","IEEE Conferences"
"An intelligent high dynamic range video codec for handheld devices","A. N. J. Léonce; D. S. Wickramanayake; E. A. Edirisinghe","Digital Imaging Research Group, Loughborough University, United Kingdom; Digital Imaging Research Group, Loughborough University, United Kingdom; Digital Imaging Research Group, Loughborough University, United Kingdom","2011 IEEE International Conference on Consumer Electronics (ICCE)","3 Mar 2011","2011","","","691","692","This paper presents a system that improves the viewing quality of video broadcast on current commercially available mobile display devices. Existing coding standards are used to transmit High Dynamic Range video and ambient lighting data is used to enhance the image displayed. The system remains fully backward compatible.","2158-4001","978-1-4244-8712-7","10.1109/ICCE.2011.5722812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5722812","High Dynamic Range;Video Coding;Video Transmission;Image Enhancement","Video coding;Lighting;Image color analysis;Dynamic range;Automatic voltage control;Mobile communication;Decoding","code standards;high definition video;image enhancement;mobile radio;video codecs","handheld devices;video broadcast quality;mobile display devices;coding standards;high dynamic range video codec;image enhancement","","","4","7","","3 Mar 2011","","","IEEE","IEEE Conferences"
"Tile-Based Rate Assignment for 360-Degree Video Based on Spatio-Temporal Activity Metrics","R. Skupin; Y. Sanchez; L. Jiao; C. Hellge; T. Schierl","Fraunhofer Heinrich-Hertz-Inst., Berlin, Germany; Fraunhofer Heinrich-Hertz-Inst., Berlin, Germany; Fraunhofer Heinrich-Hertz-Inst., Berlin, Germany; Fraunhofer Heinrich-Hertz-Inst., Berlin, Germany; Fraunhofer Heinrich-Hertz-Inst., Berlin, Germany","2018 IEEE International Symposium on Multimedia (ISM)","6 Jan 2019","2018","","","65","68","Tile-based video systems have recently emerged as a viable solution to overcome the challenges of 360-degree video. For instance, the HEVC based viewport-dependent profile of MPEG OMAF allows serving clients independently coded tiles of the 360-degree video at varying resolution to enhance fidelity within the actual user viewport. During streaming, the client constantly adapts its tile selection and feeds a single merged bitstream to the video decoder. This paper addresses the open issue of rate assignment in a distributed encoding system in such a multi-resolution tiled streaming scenario. A model for tile rate assignment based on the spatio-temporal activity of the video is presented to reduce variance of the quality distribution and experimental results are reported.","","978-1-5386-6857-3","10.1109/ISM.2018.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8603260","Rate assignment, spatio temporal activity, tile based streaming, HEVC, 360-degree video","Streaming media;Bit rate;Encoding;Video sequences;Training;Complexity theory","video coding;video streaming","video decoder;tile selection;tiles;HEVC based viewport-dependent profile;tile-based video systems;spatio-temporal activity metrics;360-degree video;tile-based rate assignment","","1","","6","","6 Jan 2019","","","IEEE","IEEE Conferences"
"The SJTU UHD 360-Degree Immersive Video Sequence Dataset","X. Liu; Y. Huang; L. Song; R. Xie; X. Yang","Inst. of Image Commun. & Network Eng., Shanghai Jiao Tong Univ., Shanghai, China; Inst. of Image Commun. & Network Eng., Shanghai Jiao Tong Univ., Shanghai, China; Inst. of Image Commun. & Network Eng., Shanghai Jiao Tong Univ., Shanghai, China; Inst. of Image Commun. & Network Eng., Shanghai Jiao Tong Univ., Shanghai, China; Inst. of Image Commun. & Network Eng., Shanghai Jiao Tong Univ., Shanghai, China","2017 International Conference on Virtual Reality and Visualization (ICVRV)","23 May 2019","2017","","","400","401","This paper presents a set of 12 new 8K resolution ultra-high definition (UHD) 360-degree immersive video sequences. This video sequences can satisfy the enhancing requirements of the researches on UHD 360-degree immersive video, such as next-generation video encoding research. In this paper, the processing and characteristics of the video sequences are illustrated.","2375-141X","978-1-5386-2636-8","10.1109/ICVRV.2017.00095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8719147","UHD;360-degree video;video dataset;8K","Video sequences;Streaming media;Logic gates;Cameras;Virtual reality;Image resolution;Transform coding","high definition video;image resolution;image sequences;video signal processing","SJTU UHD 360-degree immersive video sequence dataset;8K resolution ultra-high definition;next-generation video encoding research","","8","","5","","23 May 2019","","","IEEE","IEEE Conferences"
"Enhanced Bi-Prediction With Convolutional Neural Network for High-Efficiency Video Coding","Z. Zhao; S. Wang; S. Wang; X. Zhang; S. Ma; J. Yang","Key Laboratory of Mathematics and Its Applications (LMAM), School of Mathematical Sciences, Peking University, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong; School of Electronics Engineering and Computer Science, Institute of Digital Media, Peking University, Beijing, China; Viterbi School of Engineering, University of Southern California, Los Angeles, CA, USA; School of Electronics Engineering and Computer Science, Institute of Digital Media, Peking University, Beijing, China; Key Laboratory of Mathematics and Its Applications (LMAM), School of Mathematical Sciences, Peking University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","30 Oct 2019","2019","29","11","3291","3301","In this paper, we propose an enhanced bi-prediction scheme based on the convolutional neural network (CNN) to improve the rate-distortion performance in video compression. In contrast to the traditional bi-prediction strategy which computes the linear superposition as the predictive signals with pixel-to-pixel correspondence, the proposed scheme employs CNN to directly infer the predictive signals in a data-driven manner. As such, the predicted blocks are fused in a nonlinear fashion to improve the coding performance. Moreover, the patch-to-patch inference strategy with CNN also improves the prediction accuracy since the patch-level information for the prediction of each individual pixel can be exploited. The proposed enhanced bi-prediction scheme is further incorporated into the high-efficiency video coding standard, and the experimental results exhibit a significant performance improvement under different coding configurations.","1558-2205","","10.1109/TCSVT.2018.2876399","National Natural Science Foundation of China(grant numbers:61632001,61421062,61520106004,61571017); National Basic Research Program of China (973 Program)(grant numbers:2015CB351800); Hong Kong RGC Early Career Scheme(grant numbers:9048122 (CityU 21211018)); City University of Hong Kong(grant numbers:7200539/CS); Peking University; Microsoft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8493529","Bi-prediction;deep neural network;inter prediction;video coding","Encoding;Video coding;Machine learning;Image restoration;Neural networks;Interpolation;Image coding","convolutional neural nets;data compression;image resolution;video coding","convolutional neural network;high-efficiency video coding;enhanced bi-prediction scheme;CNN;rate-distortion performance;video compression;traditional bi-prediction strategy;predictive signals;pixel-to-pixel correspondence;predicted blocks;coding performance;patch-to-patch inference strategy;prediction accuracy","","17","","48","","16 Oct 2018","","","IEEE","IEEE Journals"
"Exploiting inter-layer correlations in scalable HEVC for the support of screen content videos","S. Tsang; Y. Chan; W. Siu","Centre of Signal Processing, Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong; Centre of Signal Processing, Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong; Centre of Signal Processing, Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong","2014 19th International Conference on Digital Signal Processing","18 Sep 2014","2014","","","888","892","In this paper, we propose a scalable video coding (SVC) scheme for efficient coding of screen content videos while still providing the backward compatibility of HEVC (High Efficiency Video Coding). By this means, the base layer is encoded as an HEVC-compatible bitstream while the enhancement layer is encoded with the proposed screen content coding (SCC) techniques for the support of screen content videos. For the enhancement layer, Base Colors plus Escape Colors (BCEC) approach is used in which a textual block is represented by base colors, escape colors and an index map. To further enhance the coding efficiency of enhancement layer, inter-layer intra prediction for BCEC is introduced by using index map differential coding and index map reuse techniques. Experimental results show that the proposed algorithm can achieve 8.0% bitrate reduction in average and up to 18.9% bitrate reduction for screen content video as compared with the conventional scalable HEVC.","2165-3577","978-1-4799-4612-9","10.1109/ICDSP.2014.6900796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900796","HEVC;Inter-layer Intra Prediction;Scalable Video Coding;Screen Content Coding;Base Colors;Escape Colors","Indexes;Image color analysis;Encoding;Video coding;Videos;Digital signal processing;Bit rate","correlation methods;data compression;image colour analysis;video coding","interlayer correlations;high efficiency video coding;HEVC-compatible bitstream;screen content videos;scalable video coding scheme;SVC scheme;screen content coding techniques;SCC techniques;base colors plus escape colors;BCEC;textual block;enhancement layer;interlayer intra prediction;index map differential coding;index map reuse techniques;bitrate reduction","","12","","13","","18 Sep 2014","","","IEEE","IEEE Conferences"
"Joint Separable and Non-Separable Transforms for Next-Generation Video Coding","X. Zhao; J. Chen; M. Karczewicz; A. Said; V. Seregin","Qualcomm Technology, Inc., San Diego, CA, USA; Huawei, Santa Clara, Santa Clara, CA, USA; Qualcomm Technologies, Inc., San Diego, CA, USA; Qualcomm Technologies, Inc., San Diego, CA, USA; Qualcomm Technologies, Inc., San Diego, CA, USA","IEEE Transactions on Image Processing","5 Mar 2018","2018","27","5","2514","2525","Throughout the past few decades, the separable discrete cosine transform (DCT), particularly the DCT type II, has been widely used in image and video compression. It is well-known that, under first-order stationary Markov conditions, DCT is an efficient approximation of the optimal Karhunen-Loève transform. However, for natural image and video sources, the adaptivity of a single separable transform with fixed core is rather limited for the highly dynamic image statistics, e.g., textures and arbitrarily directed edges. It is also known that non-separable transforms can achieve better compression efficiency for images with directional texture patterns, yet they are computationally complex, especially when the transform size is large. In order to achieve higher transform coding gains with relatively low-complexity implementations, we propose a joint separable and non-separable transform. The proposed separable primary transform, named enhanced multiple transform (EMT), applies multiple transform cores from a pre-defined subset of sinusoidal transforms, and the transform selection is signaled in a joint block level manner. Moreover, a non-separable secondary transform (NSST) method is proposed to operate in conjunction with EMT. Unlike the existing non-separable transform schemes which require excessive amounts of memory and computation, the proposed NSST efficiently improves coding gain with much lower complexity. Extensive experimental results show that the proposed methods, in a state-of-the-art video codec, such as high efficiency video coding, can provide significant coding gains (average 6.9% and 4.5% bitrate reductions for intra and random-access coding, respectively).","1941-0042","","10.1109/TIP.2018.2802202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8281012","Video compression;video codecs;transform coding;non-separable transform;2-D transform;high efficiency video coding (HEVC);next-generation video coding","Discrete cosine transforms;Image coding;Encoding;Complexity theory;Correlation;Video coding","computational complexity;data compression;discrete cosine transforms;image texture;Karhunen-Loeve transforms;Markov processes;transform coding;video coding","DCT type II;video compression;first-order stationary Markov conditions;compression efficiency;directional texture patterns;sinusoidal transforms;nonseparable secondary transform method;high efficiency video coding;video codec;transform coding gains;computational complexity;separable primary transform;enhanced multiple transform;EMT;NSST;separable discrete cosine transform","","24","","34","","5 Feb 2018","","","IEEE","IEEE Journals"
"Perceptual Intra Video Encoder for High-Quality High-Definition Content","M. Martínez-Rach; O. López-Granado; P. Piñol; M. P. Malumbres","Miguel Hernandez Univ., Alicante, Spain; Miguel Hernandez Univ., Alicante, Spain; Miguel Hernandez Univ., Alicante, Spain; Miguel Hernandez Univ., Alicante, Spain","2013 Data Compression Conference","20 Jun 2013","2013","","","509","509","This paper presents a perceptually enhanced intra-mode video encoder based on the Contrast Sensitivity Function (CSF) with a gracefully quality degradation as compression rate increases. The proposed encoder is highly competitive especially for high definition video formats at high video quality applications with constrained real-time and power processing demands.","1068-0314","978-0-7695-4965-1","10.1109/DCC.2013.89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6543119","Perceptual encoding;Contrast Sensitivity Function","Image coding;Memory management;Streaming media;High definition video;Quality assessment;Encoding;Proposals","data compression;high definition video;video coding","high-video quality applications;high-definition video formats;compression rate;quality degradation;CSF;contrast sensitivity function;perceptually-enhanced intramode video encoder;high-quality high-definition content","","","","3","","20 Jun 2013","","","IEEE","IEEE Conferences"
"Demonstrating an Enhanced Ethernet Switch Supporting Video Sensing with Dynamic QoS","R. Santos; P. Pedreiras; L. Almeida","Inst. of Telecommun., Univ. of Aveiro, Aveiro, Portugal; Inst. of Telecommun., Univ. of Aveiro, Aveiro, Portugal; Inst. of Telecommun., Univ. of Porto, Porto, Portugal","2012 IEEE 8th International Conference on Distributed Computing in Sensor Systems","2 Jul 2012","2012","","","293","294","Video applications, which include industrial uses like machine vision, object tracking, surveillance, driving aids, etc. are becoming increasingly common. These sensors produce large amounts of data, being normally compressed at the source nodes to save network bandwidth. As a side effect, video streams exhibit a large variability in their bandwidth utilization. On the other hand, many video applications are highly dynamic. For instance, a video surveillance application can meet its goals with a low frame-rate video, when the environment being monitored is static, but require an high frame-rate when the environment changes. Another example is applications that have several video sources that are activated on demand. For instance, a rear-view video camera, nowadays commonly found in cars and trucks, is only necessary during certain maneuvers. Applications like the ones presented above can be efficiently supported by enhanced Ethernet switching that provides hierarchical server-based traffic scheduling, in particular by the FTT-enabled switch [1] [2] that resulted from instantiating the FTT (Flexible Time-Triggered) communication paradigm [3] onto a customized Ethernet switch. Servers are, in fact, adequate abstractions to handle real-time video transmission since:1) servers discipline the use of the network, thus coping with the large variability of compressed video sources, 2) the adaptability, obtained by changing the servers budget, permits varying dynamically the bandwidth allocated to each video stream, 3) the reconfigurability permits enabling and disabling video-streams. We demonstrate these features with a simplified video surveillance demo, using the setup shown in Figure 1.","2325-2944","978-1-4673-1693-4","10.1109/DCOSS.2012.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227756","","Servers;Cameras;Streaming media;Switches;Bandwidth;Monitoring;Quality of service","bandwidth allocation;computer vision;local area networks;object tracking;quality of service;telecommunication switching;telecommunication traffic;video streaming;video surveillance","video sensing;QoS;video streaming;machine vision;object tracking;surveillance;driving aids;bandwidth utilization;Ethernet switching;hierarchical server-based traffic scheduling;FTT-enabled switch;flexible time-triggered communication;bandwidth allocation","","2","","3","","2 Jul 2012","","","IEEE","IEEE Conferences"
"An Efficient Encoder Rate Control Solution for Transform Domain Wyner–Ziv Video Coding","C. Brites; F. Pereira","Instituto Superior Técnico, Instituto de Telecomunicações, Lisboa, Portugal; Instituto Superior Técnico, Instituto de Telecomunicações, Lisboa, Portugal","IEEE Transactions on Circuits and Systems for Video Technology","1 Sep 2011","2011","21","9","1278","1292","Most Wyner-Ziv (WZ) video coding solutions in the literature use a feedback channel (FC) based decoder rate control (DRC) strategy to adjust the bitrate to correct the side information (SI) errors. More recently, some encoder rate control (ERC) strategies have been proposed to address application scenarios where a FC is not available. The ERC based WZ video coding RD performance depends not only on the (encoder) parity rate estimator (PRE) accuracy but also on the decoder “intelligence” in dealing with the residual errors due to parity rate underestimation. In this context, the main objective of this paper is to propose a more efficient and powerful ERC solution for transform domain WZ (TDWZ) video coding by simultaneously tackling the two issues aforementioned with the following technical novelty: 1) integration in an ERC context of Gray mapping for the quantized DCT coefficients to enhance the correlation between WZ and SI data; 2) more accurate PRE to better estimate the needed parity rate to avoid undesired parity rate underestimations and overestimations; 3) novel soft reconstruction function to reduce the impact of the residual bitplane errors in the decoded WZ frame quality; and 4) weighted overlapped block motion compensation technique to refine the SI used in an iterative WZ decoding framework with the correlation noise model parameters dynamically updated. Experimental results show a considerable RD performance improvement with a reduction of up to about 2 dB of the gap between the ERC and DRC based approaches in TDWZ video coding solutions, thus making this ERC based WZ codec the most efficient available and competitive regarding DRC based WZ video coding solutions.","1558-2205","","10.1109/TCSVT.2011.2147210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5756225","Distributed video coding;encoder rate control;side information refinement;soft reconstruction;transform domain;Wyner–Ziv (WZ) video coding","Silicon;Decoding;Discrete cosine transforms;Video coding;Correlation;Iterative decoding;Video codecs","correlation methods;encoding;feedback;image reconstruction;video coding","encoder rate control solution;transform domain;Wyner Ziv video coding;feedback channel;decoder rate control strategy;side information errors;parity rate estimator;Gray mapping;soft reconstruction function;weighted overlapped block motion compensation technique;correlation noise model parameters","","26","","41","","25 Apr 2011","","","IEEE","IEEE Journals"
"Manipulating Ultra-High Definition Video Traffic","Y. Ye; Y. He; X. Xiu",InterDigital Communications; InterDigital Communications; InterDigital Communications,"IEEE MultiMedia","7 Aug 2015","2015","22","3","73","81","Compared to the widely deployed high-definition (HD) format, ultra-high definition (UHD) defines video parameters associated with higher spatial resolutions, higher frame rates, higher sample bit depths, and a wider color gamut. UHD promises to significantly enhance the user experience with pictures that offer the ""look out the window""' effect. However, this promise comes at the cost of increased bandwidth required to deliver UHD. This article explores on-demand UHD video streaming using the latest video compression and delivery technologies. The proposed solution uses the scalable extensions of HEVC (SHVC) to efficiently compress, store, and deliver UHD video in a large-scale streaming system. With scalability, backward compatibility is maintained and the quality of the existing HD services is guaranteed.","1941-0166","","10.1109/MMUL.2015.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7021856","ultra-high definition;UHD;HEVC;scalable video coding;SHVC;adaptive streaming;video delivery;multimedia;graphics;Internet/Web technologies","Streaming media;High definition video;Video coding;Image color analysis;Multimedia communication;Scalability;Encoding;Web and internet services","data compression;image colour analysis;telecommunication traffic;video coding;video streaming","backward compatibility;large-scale streaming system;scalable HEVC extensions;video delivery technology;video compression technology;on-demand UHD video streaming;user experience enhancement;wider color gamut;higher sample bit depths;higher frame rates;higher spatial resolutions;video parameters;ultra-high definition video traffic manipulation","","17","","19","","26 Jan 2015","","","IEEE","IEEE Magazines"
"A Framework for Robust Online Video Contrast Enhancement Using Modularity Optimization","A. Choudhury; G. Medioni","University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA","IEEE Transactions on Circuits and Systems for Video Technology","30 Aug 2012","2012","22","9","1266","1279","We address the problem of video contrast enhancement. Existing techniques either do not exploit temporal information at all or do not exploit it correctly. This results in inconsistency that causes undesirable flash and flickering artifacts. Our method analyzes video streams and cluster frames that are similar to each other. Our method does not have omniscient information about the entire video sequence. It is an online process with a fixed delay. A sliding window mechanism successfully detects shot boundaries “on-the-fly” in a video. A graph-based technique called “modularity” performs automatic clustering of video frames without a priori information about clusters. For every cluster in the video, we extract key frames belonging to each cluster using eigen analysis and estimate enhancement parameters for only the key frame, then use these parameters to enhance frames belonging to that cluster, thus making our method robust. We evaluate the clustering method on video sequences from the TRECVid 2001 dataset and compare it with existing methods. We show reduction of flash artifacts in enhanced videos. We show statistically significant improvement in perceived video quality and validate that by conducting experiments on human observers. We show application of our clustering process to perform robust video segmentation.","1558-2205","","10.1109/TCSVT.2012.2198136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6196206","Human validation;modularity optimization;shot detection;video enhancement;video segmentation","Histograms;Lighting;Image color analysis;Clustering algorithms;Image edge detection;Video sequences;Indexes","feature extraction;graph theory;image enhancement;image segmentation;image sequences;pattern clustering;video streaming","robust online video contrast enhancement;modularity optimization;flickering artifacts;video stream analysis method;cluster frames;video sequence;sliding window mechanism;on-the-fly shot boundary detection;graph-based technique;video frame automatic clustering method;key frame extraction;eigen analysis;flash artifact reduction;robust video segmentation","","12","11","40","","7 May 2012","","","IEEE","IEEE Journals"
"Adaptive DCT based depth map resampling for depth enhanced multiview video coding","M. Homayouni; P. Aflaki; M. Gabbouj","Department of Signal Processing, Tampere University of Technology, Finland; Department of Signal Processing, Tampere University of Technology, Finland; Department of Signal Processing, Tampere University of Technology, Finland","2014 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)","11 Aug 2014","2014","","","1","4","In this paper, we propose an adaptive downsampling pattern selection method for depth enhanced multiview video coding taking into account local characteristics of the depth map. The video content is divided into different stripes and a frequency-based measure is deployed to estimate the amount of degradation caused by downsampling and/or coding. The sampling pattern is determined for each vertical/horizontal stripe within a video in such a way that preserves the rectangular shape of the frames, hence the video sequence can be coded with any standard encoder. Simulation results show that the proposed method outperforms the reference by 0.18 dB of Bjontegaard delta Peak Signal-to-Noise Ratio (PSNR) or equivalently brings up to 22.56% and average of 9.84% of Bjontegaard delta bitrate reduction (dBR). It is also shown that the proposed algorithm reduces the complexity of the process to find a proper pattern to about 1% compared to the case where a full search is applied.","2161-203X","978-1-4799-4758-4","10.1109/3DTV.2014.6874737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6874737","Depth map;MVD;adaptive resampling;high frequency components;DCT","Discrete cosine transforms;Encoding;Bit rate;Video coding;Spatial resolution;Rendering (computer graphics);Image coding","adaptive codes;discrete cosine transforms;image sampling;image sequences;video coding","adaptive downsampling pattern selection method;depth enhanced multiview video coding;video content;frequency-based measure;degradation estimation;sampling pattern determination;video sequence;standard encoder;peak signal-to-noise ratio;PSNR;Bjontegaard delta bitrate reduction;dBR;complexity reduction;adaptive DCT based depth map resampling","","","","15","","11 Aug 2014","","","IEEE","IEEE Conferences"
"Video entity resolution: Applying ER techniques for Smart Video Surveillance","Liyan Zhang; R. Vaisenberg; S. Mehrotra; D. V. Kalashnikov","Department of Computer Science, University of California, Irvine, USA; Department of Computer Science, University of California, Irvine, USA; Department of Computer Science, University of California, Irvine, USA; Department of Computer Science, University of California, Irvine, USA","2011 IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)","12 May 2011","2011","","","26","31","Smart Video Surveillance (SVS) applications enhance situational awareness by allowing domain analysts to focus on the events of higher priority. This in turn leads to improved decision making, allows for better resource management, and helps to reduce information overload. SVS approaches operate by trying to extract and interpret higher “semantic” level events that occur in video. On of the key challenges of Smart Video Surveillance is that of person identification where the task is for each subject that occur in a video shot to identify the person it corresponds to. The problem of person identification is very complex in the resource constrained environments where transmission delay, bandwidth restriction, and packet loss may prevent the capture of high quality data. In this paper we connect the problem of person identification in video data with the problem of entity resolution that is common in textual data. Specifically, we show how the PI problem can be successfully resolved using a graph-based entity resolution framework called RelDC that leverages relationships among various entities for disambiguation. We apply the proposed solution to a dataset consisting of several weeks of surveillance videos. The results demonstrate the effectiveness and efficiency of our approach even with low quality video data.","","978-1-61284-937-9","10.1109/PERCOMW.2011.5766881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5766881","Smart Video Surveillance;Video Entity Resolution;Person Identification;Video Data Cleaning","Histograms;Image color analysis;Face recognition;Feature extraction;Surveillance;Face;Legged locomotion","image resolution;video signal processing;video surveillance","video entity resolution;ER techniques;smart video surveillance;person identification;RelDC framework;graph-based entity resolution framework","","1","","11","","12 May 2011","","","IEEE","IEEE Conferences"
"QoE-Driven Cross-Layer Optimization for Wireless Dynamic Adaptive Streaming of Scalable Videos Over HTTP","M. Zhao; X. Gong; J. Liang; W. Wang; X. Que; S. Cheng","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; School of Engineering Science, Simon Fraser University, Burnaby, BC, Canada; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","3 Mar 2015","2015","25","3","451","465","Recently, Dynamic Adaptive Streaming over HTTP (DASH) has attracted significant attention. In this paper, we consider DASH-based transmission of scalable videos in wireless broadband access networks (e.g., long-term evolution and WiMAX), and propose three methods to enhance the quality of experience of wireless DASH users. First, we design an improved mapping scheme from scalable video coding layers to DASH layers that can provide the desired bitrates, enhance the video end-to-end throughput, and reduce the HTTP communication overhead. Second, we develop a DASH-friendly scheduling and resource allocation algorithm by integrating the DASH-based media delivery and the radio-level adaptation via a cross-layer approach. It utilizes the characteristics of video content and scalable video coding, and greatly reduces the possibility of video playback interruption by considering the client buffer status. The optimization problem is formulated as a mixed binary integer programming problem, and is solved by a subgradient method. Finally, a DASH proxy-based bitrate stabilization algorithm is proposed to improve the video playback smoothness that can achieve the desired tradeoff between playback quality and stability. Simulations with the Qualnet tool demonstrate that our schemes achieve better performances than other methods in the literature.","1558-2205","","10.1109/TCSVT.2014.2357094","National High-Tech Research and Development Program (863 Program) of China(grant numbers:2013AA013301,2013AA013303); Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN312262,STPGP447223); National Natural Science Foundation of China(grant numbers:61370197); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898001","Cross-layer optimization;Dynamic Adaptive Streaming over HTTP (DASH);quality of experience (QoE);Scalable Video Coding (SVC);scheduling and resource allocation;video streaming","Streaming media;Static VAr compensators;Bit rate;Indexes;Resource management;Wireless communication;Scalability","dynamic programming;gradient methods;hypermedia;integer programming;optimisation;quality of experience;transport protocols;video coding;video streaming","QoE-driven cross layer optimization;wireless dynamic adaptive streaming;scalable videos;HTTP DASH;wireless broadband access networks;WiMAX;long-term evolution;scalable video coding layers;HTTP communication;resource allocation algorithm;radio level adaptation;video content;video playback;binary integer programming problem;subgradient method","","61","","53","","12 Sep 2014","","","IEEE","IEEE Journals"
"Peer-to-Peer Collaborative Video-on-Demand Streaming over Mobile Content Centric Networking","R. BOUSSAHA; Y. Challal; A. Bouabdallah; D. Ighit; L. Tairi","Lab. LMCS, Ecole Nat. Suporieure d'Inf., Algiers, Algeria; Lab. LMCS, Ecole Nat. Suporieure d'Inf., Algiers, Algeria; Lab. HEUDIASYC, Univ. de Technol. de Compiegne, Compiegne, France; Lab. LMCS, Ecole Nat. Suporieure d'Inf., Algiers, Algeria; Lab. LMCS, Ecole Nat. Suporieure d'Inf., Algiers, Algeria","2018 IEEE 32nd International Conference on Advanced Information Networking and Applications (AINA)","13 Aug 2018","2018","","","1053","1059","Nowadays, multimedia is omnipresent in the Internet and generates the major total traffic in fixed and mobile networks. While video streaming services become more crucial for mobile users, their traffic may often exceed the bandwidth capacity of cellular networks. Content Centric Networking (CCN) can be an attractive solution which adapts the network architecture to the current network usage pattern. In this paper, we propose a CCN peer-to-peer video-on-demand streaming protocol based on scalable video coding. We implement a collaborative strategy which improves the video segments availability in the network and reduces latency relying on CCN functionalities such as caching and routing by name. We also propose a control strategy allowing to scale to highly dynamic networks. Through the tests carried out to evaluate the performance of our solution, we show its effectiveness. Indeed, it reduces significantly the initial playback delay and enhances the streaming quality compared to a traditional service with no collaboration policy.","2332-5658","978-1-5386-2195-0","10.1109/AINA.2018.00152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8432354","Content Centric Networking;Video-on-demand streaming;Peer-to-peer;Scalable video coding","Streaming media;Peer-to-peer computing;Servers;Collaboration;Static VAr compensators;Protocols;Video coding","mobile communication;multimedia communication;peer-to-peer computing;protocols;video coding;video on demand;video streaming","peer-to-peer collaborative video-on-demand streaming;mobile content centric networking;streaming quality;scalable video coding;CCN peer-to-peer video-on-demand streaming protocol;cellular networks;mobile users;video streaming services","","","","11","","13 Aug 2018","","","IEEE","IEEE Conferences"
"Experimental Study on the Digital Color Video Using Absolute Difference and Piecewise Mapping Techniques","S. Madhura","RV Inst. of Technol. & Manage., Bangalore, India","2020 2nd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA)","23 Apr 2020","2020","","","641","645","Video processing acts as a major role in various applications and it easy to record at anytime and anywhere in the today's world. Visually pleasing videos are generally required for most of the video applications like medical imagery, high definition video photography, and liquid crystal display and surveillance systems such that these videos have a reduced noise and enhanced contrast. The proposed method improves the contrast of the video by giving importance to the color details and provides visually acceptable result as compared to various other existing methods. The algorithm adopts ROAD and adaptive piecewise mapping function to automatically enhance the contrast of the video.","","978-1-7281-4167-1","10.1109/ICIMIA48430.2020.9074958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9074958","ROAD;adaptive mapping function;contrast enhancement and video processing","Histograms;Roads;Conferences;Industry applications;Filtering algorithms;Image color analysis;Visualization","high definition video;image colour analysis;image denoising;image enhancement;liquid crystal displays;video signal processing;video surveillance","digital color video;piecewise mapping techniques;visually pleasing videos;video applications;medical imagery;high definition video photography;liquid crystal display;video surveillance systems;reduced noise;visually acceptable result;adaptive piecewise mapping function;video processing","","","","11","","23 Apr 2020","","","IEEE","IEEE Conferences"
"Distributed compressive video coding using Enhanced side information for WSN","V. Angayarkanni; V. Akshaya; S. Radha","Department of Electronics and Communication Engineering, SSN College of Engineering, Chennai-603110, Tamil Nadu, India; Department of Electronics and Communication Engineering, SSN College of Engineering, Chennai-603110, Tamil Nadu, India; Department of Electronics and Communication Engineering, SSN College of Engineering, Chennai-603110, Tamil Nadu, India","2016 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)","15 Sep 2016","2016","","","1133","1136","An efficient framework of distributed compressive video coding using Enhanced side information for WSN is proposed. At the encoder, the incoming video frames are split into key frames and Wiener-Ziv (WZ) frames depending on Group of Picture (GOP). Based on Distributed Video Coding (DVC) these frames are processed independently and transmitted over WSN. Generally the WZ frames are encoded at a lower measurement rate than those of key frames. At the decoder, key frames are reconstructed directly whereas the WZ frames are decoded with the help of Enhanced Side Information (ESI). ESI is generated based on motion vector which is estimated using log search algorithm. The motion vector is calculated from initially reconstructed key frames and WZ frames. On an average PSNR of WZ frames of DCVC-ESI exceeds 3.68dB than DVC-SI technique. On an average SSIM of WZ frames of DCVC-ESI exceeds 0.0174 than DVC technique.","","978-1-4673-9338-6","10.1109/WiSPNET.2016.7566313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7566313","Compressed Sensing;DCVC;ESI;Motion vector;Motion compensation;Motion Estimation","Encoding;Video coding;Wireless sensor networks;Conferences;Streaming media;Decoding;Multimedia communication","data compression;decoding;image motion analysis;image reconstruction;search problems;video coding;wireless sensor networks","distributed compressive video coding;WSN;Wiener-Ziv frame;group of picture;GOP;WZ frame;decoder;enhanced side information;log search algorithm;motion vector;key frame reconstruction;average PSNR;DCVC-ESI;average SSIM","","4","","15","","15 Sep 2016","","","IEEE","IEEE Conferences"
"An enhanced structure of layered forward error correction and interleaving for scalable video coding in wireless video delivery","T. Wu; S. Guizani; W. Lee; P. Huang",NATIONAL ILAN UNIVERSITY; ALFAISAL UNIVERSITY; TAMKANG UNIVERSITY; TAMKANG UNIVERSITY,"IEEE Wireless Communications","12 Sep 2013","2013","20","4","146","152","The forward error correction (FEC) mechanism adds redundancy to the original information and uses this redundancy to recover the errors in, or loss of, the original information. Compared with the automatic retransmission request (ARQ) approach, FEC reduces delay but utilizes more bandwidth resources. To enhance video transmission over wireless networks, another mechanism, enhanced adaptive FEC (EAFEC), has been recently presented. According to the throughput and wireless channel state, EAFEC dynamically determines the length of the redundancy and reduces the transmission of unnecessary redundancy to achieve the efficient utilization of wireless networking resources. To further enhance the efficiency of EAFEC, we propose an enhanced structure, adaptive and interleaving FEC (AIFEC) using the features of FEC. Our proposed AIFEC scheme not only dynamically provides different protection strengths for video information of different priority levels based on the throughput and wireless channel state, but also significantly improves the recovery rate for consecutive packet loss.","1558-0687","","10.1109/MWC.2013.6590062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6590062","","Forward error correction;Packet loss;Streaming media;Video coding;Static VAr compensators;Error analysis","automatic repeat request;forward error correction;video coding","layered forward error correction mechanism;scalable video coding;wireless video delivery;automatic retransmission request approach;ARQ approach;enhanced adaptive FEC mechanism;EAFEC;wireless channel state;wireless networking resources;AIFEC;adaptive and interleaving FEC mechanism;video information","","7","","10","","12 Sep 2013","","","IEEE","IEEE Magazines"
"Enhancing Quality for VVC Compressed Videos with Multi-Frame Quality Enhancement Model","X. HoangVan; H. -H. Nguyen","Vietnam National University,University Of Engineering and Technology,Faculty of Electronics and Telecommunications; Le Quy Don Technical University,Institute of System Integration","2020 International Conference on Advanced Technologies for Communications (ATC)","18 Nov 2020","2020","","","172","176","Versatile Video Coding (VVC) is the most recent video coding standard, released in July 2020 with two major purposes: (1) providing a similar perceptual quality as the current state-of-the-art High Efficiency Video Coding (HEVC) solution at around half the bitrate and (2) offering native flexible, high-level syntax mechanisms for resolution adaptivity, scalability, and multi-view. However, despite of the compression efficiency, the decoded video obtained with VVC compression still contains distortions and quality degradation due to the nature of the hybrid block and transform based coding approach. To overcome this problem, this paper proposes a novel quality enhancement method for VVC compressed videos where the most advanced deep learning-based multi-frame quality enhancement model (MFQE) is employed. In the proposed QE method, the VVC decoded video is firstly segmented into the peak quality and non-peak quality pictures. After that, a Long-short term memory and two sub-networks are created to achieve better quality video pictures. Experimental results show that, the proposed MFQE based VVC quality enhancement method is able to achieve important quality improvement when compared to the original VVC decoded video.","2162-1039","978-1-7281-8065-6","10.1109/ATC50776.2020.9255448","National Foundation for Science and Technology Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9255448","Versatile Video Coding;Multi-Frame Quality Enhancement;High Efficiency Video Coding","Videos;Encoding;Standards;Image coding;Transforms;Decoding;Visualization","data compression;decoding;image enhancement;image resolution;image segmentation;learning (artificial intelligence);recurrent neural nets;video coding","perceptual quality;high efficiency video coding;high-level syntax mechanisms;VVC compression;quality degradation;deep learning;quality video pictures;quality improvement;VVC compressed videos;versatile video coding;video coding standard;VVC decoded video;VVC quality enhancement;multiframe quality enhancement;resolution adaptivity;transform based coding;video segmentation;peak quality pictures;nonpeak quality pictures;long-short term memory","","","","24","","18 Nov 2020","","","IEEE","IEEE Conferences"
"Subsampling Input Based Side Information Creation in Wyner-Ziv Video Coding","Y. Shen; J. Luo; J. Wu","Grad. Inst. of Networking & Multimedia, Nat. Taiwan Univ., Taipei, Taiwan; Grad. Inst. of Networking & Multimedia, Nat. Taiwan Univ., Taipei, Taiwan; Grad. Inst. of Networking & Multimedia, Nat. Taiwan Univ., Taipei, Taiwan","2013 Data Compression Conference","20 Jun 2013","2013","","","519","519","Summary form only given. Distributed video coding (DVC) has been intensively studied in recent years. This new coding paradigm substantially differs from conventional prediction-based video codecs such as MPEG and H.26x, which are characterized by a complex encoder and simple decoder. The conventional DVC codec, e.g., DISCOVER codec, uses advanced frame interpolation techniques to create SI based on adjacent decoded reference frames. The quality of SI is a well-recognized factor in the RD performance of WZ video coding. A high SI quality implies a high correlation between the created SI and the original WZ frame, which then decreases the rate required to achieve a given decoded quality. Clearly, the performance of an SI creation process based on adjacent previously decoded frames is limited by the quality of the past and the future reference frames as well as the distance and motion behavior between them. The correlation between high-motion frames is low and vice versa. That is, SI quality in the conventional codecs depends on the temporal correlation of key frames, which affects the bitrate and PSNR of the compression process. In this work, a novel DVC architecture for dealing with the cases of high-motion and large GOP-size sequences is proposed to better the rate-distortion (RD) performance. For high-motion video sequences, the proposed architecture generates SI by using subsampled spatial information instead of interpolated temporal information. the proposed approach separates the video sequence into subsampled key frames and corresponding WZ frames, which changes the creation of SI. That is, all successive frames on the encoder side are downsized to sub-frames, which are then compressed by an H.264/AVC intra encoder. Experimental results reveal that the subsampling input based DVC codec can gain up to 1.47 dB in the RD measures and maintains the most important characteristic of the DVC codec, the encoder is lightweight, as compared with the conventional WZ codec, respectively. The novel DVC architecture evaluated in this study exploits spatial relations to create SI. The experimental results confirm that the RD performance of the proposed approach is superior to that of the conventional one for high-motion and/or large GOP-size sequences. The quality of spatial interpolation based SI is higher than that of the temporal interpolation one, which leads to a high-PSNR reconstructed WZ frame. The subsampled key frames are also decoded by LDPCA decoder to recover the information lost when H.264/AVC intra coding is used to increase PSNR gain. Since many spatial domain interpolation and super resolution schemes have been proposed for use in the fields of image processing and computer vision, the performance of the proposed DVC codec can be further enhanced by using better schemes to generate even better SI.","1068-0314","978-0-7695-4965-1","10.1109/DCC.2013.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6543129","Distributed Video Coding;DISCOVER;subsampling input based;Wyner-Ziv coding","Silicon;Codecs;Video coding;Interpolation;Correlation;Image coding;Computer architecture","computer vision;data compression;image reconstruction;image resolution;image sampling;image sequences;interpolation;parity check codes;video codecs;video coding","subsampling input based side information creation;Wyner-Ziv video coding;distributed video coding;prediction-based video codecs;MPEG;H.26x;advanced frame interpolation techniques;adjacent decoded reference frames;RD performance;WZ video coding;SI creation process;high-motion frames;key frame temporal correlation;compression process;GOP-size sequences;rate-distortion performance;high-motion video sequences;subsampled spatial information;interpolated temporal information;H.264-AVC intraencoder;subsampling input based DVC codec;high-PSNR reconstructed WZ frame;LDPCA decoder;spatial domain interpolation;super resolution schemes;image processing;computer vision","","","","","","20 Jun 2013","","","IEEE","IEEE Conferences"
"Multi-view Multi-modality Priors Residual Network of Depth Video Enhancement for Bandwidth Limited Asymmetric Coding Framework","S. Chen; Q. Liu; Y. Yang",Huazhong University of Science and Technology; Huazhong University of Science and Technology; Huazhong University of Science and Technology,"2019 Data Compression Conference (DCC)","13 May 2019","2019","","","560","560","Asymmetric coding methodology for multi-view video plus depth is a promising technique for future three-dimensional and multi-view driven visual applications for its superior coding performance in bandwidth limited conditions. Since the depth video suffers from asymmetric distortions corresponding to viewpoint, it's a challenge in smooth and quality consistent content based interaction. To solve this challenge, we propose a residual learning framework to enhance the quality of compression distorted multi-view depth video. In this work, we exploit the correlation between viewpoints to restore the target viewpoint depth maps by using multi-modality priors, which are depth maps from adjacent viewpoints with better quality and color frames in the same viewpoint. A residual network is designed to fully exploit the contribution from these priors. Experimental results show the superiority of our framework in the quality improvement on both decoded depth video and synthesized virtual viewpoint images.","2375-0359","978-1-7281-0657-1","10.1109/DCC.2019.00072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8712642","multi view video plus depth","Training;Bandwidth;Encoding;Bit rate;Image restoration;Data compression;Visualization","data compression;image enhancement;video coding;video signal processing","multimodality priors residual network;depth video enhancement;bandwidth limited asymmetric coding framework;asymmetric coding methodology;multiview video plus depth;multiview driven visual applications;bandwidth limited conditions;depth video suffers;asymmetric distortions;residual learning framework;compression distorted multiview depth video;target viewpoint depth maps;adjacent viewpoints;decoded depth video;synthesized virtual viewpoint images;coding performance","","1","","0","","13 May 2019","","","IEEE","IEEE Conferences"
