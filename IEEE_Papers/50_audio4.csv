"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Leveraging Unlabeled Data for Emotion Recognition With Enhanced Collaborative Semi-Supervised Learning","Z. Zhang; J. Han; J. Deng; X. Xu; F. Ringeval; B. Schuller","Group on Language, Audio & Music, Imperial College London, London, U.K.; ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Germany; audEERING GmbH, Gilching, Germany; School of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China; Laboratoire d’Informatique de Grenoble, Université Grenoble Alpes, Saint-Martin-d’Hères, France; Group on Language, Audio & Music, Imperial College London, London, U.K.","IEEE Access","9 May 2018","2018","6","","22196","22209","One of the major obstacles that has to be faced when applying automatic emotion recognition to realistic human-machine interaction systems is the scarcity of labeled data for training a robust model. Motivated by this concern, this paper seeks to utmost exploit unlabeled data that are pervasively available in the real-world and easy to be collected, by means of novel semi-supervised learning (SSL) approaches. Conventional SSL methods such as self-training, suffer from their inherent drawback of error accumulation, i.e., the samples that are misclassified by the system are continuously employed to train the model in the following learning iterations. To address this major issue, we first propose an enhanced learning strategy, by which we re-evaluate the previously automatically labeled samples in each learning iteration, in order to update the training set by correcting the mislabeled samples. We further exploit multiple modalities and models in the SSL system, by using collaborative SSL, where all modalities and models are considered simultaneously; samples are selected by means of minimizing the joint entropy. This strategy is supposed to not only improve the performance of the model for data annotation and consequently enhance the trustability of the automatically labeled data, but also to elevate the diversity of selected data. To evaluate the effectiveness of the proposed approaches, we performed extensive experiments on the remote collaborative and affective database, which includes multimodal recordings of spontaneous affective interactions of dyads. The empirical results show that the proposed approaches significantly outperform recently well-established SSL methods.","2169-3536","","10.1109/ACCESS.2018.2821192","Economic and Social Research Council(grant numbers:HJ-253479); H2020 European Research Council(grant numbers:645094); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8345573","Enhanced semi-supervised learning;collaborative learning;audiovisual emotion recognition","Emotion recognition;Data models;Training;Predictive models;Semisupervised learning;Entropy;Collaboration","emotion recognition;interactive systems;learning (artificial intelligence)","realistic human-machine interaction systems;error accumulation;learning iteration;enhanced learning strategy;automatically labeled samples;mislabeled samples;multiple modalities;collaborative SSL;data annotation;automatically labeled data;selected data;remote collaborative;automatic emotion recognition;unlabeled data leveraging;enhanced collaborative semisupervised learning","","6","","57","CCBY","24 Apr 2018","","","IEEE","IEEE Journals"
"ST 340:2015 - SMPTE Standard - Format for Non-PCM Audio and Data in AES3 — ATSC A/52 Digital Audio Compression Standard for AC-3 and Enhanced AC-3 Data Types","",,"ST 340:2015","19 Oct 2015","2015","","","1","12","This standard specifies data type specific format requirements for both AC-3 and E-AC-3 (“Enhanced AC-3”) data bursts carried within an AES3 interface according to SMPTE ST 337.","","978-1-61482-871-6","10.5594/SMPTE.ST340.2015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7290786","Sound;Interface;Compression;AES;SDI","","","","","","","7","","19 Oct 2015","","","SMPTE","SMPTE Standards"
"Information fusion based on kernel entropy component analysis in discriminative canonical correlation space with application to audio emotion recognition","L. Gao; L. Qi; L. Guan","School of Information Engineering, Zhengzhou University; School of Information Engineering, Zhengzhou University; Department of Electrical and Computer Engineering, Ryerson University","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 May 2016","2016","","","2817","2821","As an information fusion tool, Kernel Entropy Component Analysis (KECA) is realized by using descriptor of information entropy and optimized by entropy estimation. However, as an unsuper-vised method, it merely puts the information or features from different channels together without considering their intrinsic structures and relations. In this paper, we introduce an enhanced version of KECA for information fusion, KECA in Discriminative Canonical Correlation Space (DCCS). Not only the intrinsic structures and discriminative representations are considered, but also the natural representations of input data are revealed by entropy estimation, leading to improved recognition accuracy. The effectiveness of the proposed solution is evaluated through experiments on two audio emotion databases. Experimental results show that the proposed solution outperforms the existing methods based on similar principles.","2379-190X","978-1-4799-9988-0","10.1109/ICASSP.2016.7472191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472191","Information fusion;emotion recognition;kernel entropy component analysis;discriminative canonical correlation space","Emotion recognition;Kernel;Databases;Entropy;Correlation;Feature extraction;Principal component analysis","audio signal processing;correlation methods;emotion recognition;entropy;principal component analysis;sensor fusion","information fusion tool;kernel entropy component analysis;KECA;discriminative canonical correlation space;audio emotion recognition;entropy estimation;audio emotion databases","","3","","13","","19 May 2016","","","IEEE","IEEE Conferences"
"Projection-Based Demixing of Spatial Audio","D. FitzGerald; A. Liutkus; R. Badeau","Cork School of Music, Cork Institute of Technology, Cork, Ireland; Inria, Nancy Grand-Est, Multispeech Team, LORIA UMR 7503, Université de Lorraine, Nancy, France; LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay, Paris, France","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 May 2017","2016","24","9","1560","1572","We propose a method to unmix multichannel audio signals into their different constitutive spatial objects. To achieve this, we characterize an audio object through both a spatial and a spectro-temporal modeling. The particularity of the spatial model we pick is that it neither assumes an object has only one underlying source point, nor does it attempt to model the complex room acoustics. Instead, it focuses on a listener perspective, and takes each object as the superposition of many contributions with different incoming directions and interchannel delays. Our spectro-temporal probabilistic model is based on the recently proposed α-harmonisable processes, which are adequate for signals with large dynamics, such as audio. Then, the main originality of this paper is to provide a new way to estimate and exploit interchannel dependences of an object for the purpose of demixing. In the Gaussian α = 2 case, previous research focused on covariance structures. This approach is no longer valid for α <; 2 where covariances are not defined. Instead, we show how simple linear combinations of the mixture channels can be used to learn the model parameters, and the method we propose consists in pooling the estimates based on many projections to correctly account for the original multichannel audio. Intuitively, each such downmix of the mixture provides a new perspective where some objects are canceled or enhanced. Finally, we also explain how to recover the different spatial audio objects when all parameters have been computed. Performance of the method is illustrated on the separation of stereophonic music signals.","2329-9304","","10.1109/TASLP.2016.2570945","research program EDiSon3D(grant numbers:ANR-13-CORD-0008-01); KAMoulox(grant numbers:ANR-15-CE38-0003-01); ANR; French State Agency for research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7473924","Source separation;probabilistic models;nonnegative matrix factorization (NMF);musical source separation;Source separation;probabilistic models;nonnegative matrix factorization (NMF);musical source separation","Delays;Probabilistic logic;Acoustics;Time-frequency analysis;Tensile stress;Microphone arrays","audio signal processing;covariance analysis;probability","projection-based demixing;multichannel audio signals;constitutive spatial objects;listener perspective;interchannel delays;spectro-temporal probabilistic model;α-harmonisable processes;interchannel dependences;covariance structures;mixture channels;spatial audio objects;stereophonic music signals","","11","","46","","19 May 2016","","","IEEE","IEEE Journals"
"Navigational 3D audio-based game-training towards rich auditory spatial representation of the environment","O. Bălan; A. Moldoveanu; F. Moldoveanu; M. Dascălu","Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, Bucharest, Romania; Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, Bucharest, Romania; Faculty of Automatic Control and Computers, University POLITEHNICA of Bucharest, Bucharest, Romania; Department of Engineering Taught in Foreign Languages, University POLITEHNICA of Bucharest, Bucharest, Romania","2014 18th International Conference on System Theory, Control and Computing (ICSTCC)","15 Dec 2014","2014","","","682","687","As the number of people suffering from visual impairments continuously increases, there is strong need for efficient sensory substitution devices, that can support creating a rich mental spatial depiction of the environment. The use of the auditory sense has proved to be an effective approach towards creating a method of interaction with the elements of the surrounding space in a way which resembles the natural 3D visual representation of normal sighted people. Training is an essential component in the process of employing an auditory-based visual substitution device for blind people, as it helps them to learn and become proficient to process and decode the audio information and convert it into spatial mental representation. Taking into account the well-known advantages of game based learning, we propose a new method of training, consisting in a navigational 3D audio-based game. In this exploratory, goal-directed application, the player has to perform route-navigational tasks under different conditions, with the purpose of training and testing their orientation and mobility skills, relying exclusively on the perception of 3D audio cues. Experimental results showed that this game-based learning strategy leads to substantial improvements and can be a starting point for developing more enhanced sound-based navigational applications. The ludic-oriented, motivational training approach achieved straightforward immersion and concentration on the cognitive depiction of the environment, ensuring behavioral gains in the sound-directed spatial orientation.","","978-1-4799-4601-3","10.1109/ICSTCC.2014.6982496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6982496","3D binaural sound;HRTF;neuroplasticity;audio game;Sensory Substitution Device;Virtual Auditory Display","Games;Three-dimensional displays;Navigation;Training;Visualization;Sonification;Auditory system","acoustic signal processing;audio coding;cognition;computer based training;computer games;handicapped aids;hearing;signal representation","navigational 3D audio-based game-training;auditory spatial representation;visual impairments;sensory substitution devices;mental spatial depiction;auditory sense;natural 3D visual representation;normal sighted people;auditory-based visual substitution device;blind people;audio information decode;spatial mental representation;game based learning;goal-directed application;route-navigational tasks;orientation skills;mobility skills;3D audio cues perception;sound-based navigational applications;ludic-oriented motivational training approach;cognitive depiction;behavioral gains;sound-directed spatial orientation","","5","","22","","15 Dec 2014","","","IEEE","IEEE Conferences"
"FEC-based packet loss recovery for AVS-M audio codec","Jianli Liu; Shenghui Zhao; J. Wang; Jingming Kuang","School of Information and Electronics, Beijing Institute of Technology, China; School of Information and Electronics, Beijing Institute of Technology, China; School of Information and Electronics, Beijing Institute of Technology, China; School of Information and Electronics, Beijing Institute of Technology, China","2011 International Conference on Multimedia Technology","25 Aug 2011","2011","","","3069","3072","In this paper, we utilize sender-based Forward Error Correction (FEC) techniques to enhance the robustness of packet loss recovery for AVS Mobile speech and audio (AVS-M) codec. Two FEC schemes are proposed which take the advantage of the codec's structure characteristics and do not introduce extra delay. The objective and subjective listening tests results show that the two methods achieve higher reconstructed quality than the codec's original frame erasure scheme in the case of packet loss.","","978-1-61284-774-0","10.1109/ICMT.2011.6002009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6002009","AVS-M;packet loss;FEC","Codecs;Speech;Forward error correction;Decoding;Encoding;Redundancy;Robustness","audio coding;forward error correction;mobile radio;speech codecs;speech coding;video codecs;video coding","FEC-based packet loss recovery;AVS-M audio codec;sender-based forward error correction technique;sender-based FEC technique;audio and video coding standard mobile speech and audio codec;codec structure characteristic;objective listening test;subjective listening test;quality reconstruction;codec original frame erasure scheme","","","","6","","25 Aug 2011","","","IEEE","IEEE Conferences"
"Secret key watermarking in WAV audio file in perceptual domain","V. Bibhu; P. K. Kushwaha; R. Kohli; D. Singh","Department of CSE, Amity University, Greater Noida, U.P, India; Department of CSE, Amity University, Greater Noida, U.P, India; Department of CSE, Amity University, Greater Noida, U.P, India; Department of CSE, Amity University, Greater Noida, U.P, India","2015 International Conference on Futuristic Trends on Computational Analysis and Knowledge Management (ABLAZE)","13 Jul 2015","2015","","","629","634","Digital audio watermarking is a system of insertion and detection of the key which called watermarking key in host audio to prove the intellectual property right of the audio files. In this paper I proposed an enhanced method for watermark generation and embedding according to input audio clip. The generated key is embedded in audio clip with sampling rate of 44.1 KHz with consideration of properties of perceptual domain of human auditory system. Audio clip sequences particularly uses th direct sequence spread spectrum by utilizing the concept of watermark key. The embedding algorithm creates 128 samples and calculates masking threshold value. The Fast Fourier Transform operates at 128 points at each samples and single bit of key is added. After that 128 point reverse mode of Fast Fourier Transformation is taken to write the audio samples in output sequences. The extraction algorithm reads both original and watermarked audio clips and matches each bits from both input audio samples to determine the differences. If there is difference, it is considered as key bit difference and extracted.","","978-1-4799-8433-6","10.1109/ABLAZE.2015.7154940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7154940","Human Auditory System;Discrete Fourier Transform;Fast Fourier Transform;Signal to Mask Ratio;Pulse Code Modulation","Watermarking;Media;Bit rate;Signal to noise ratio;Masking threshold;Standards","audio signal processing;audio watermarking;discrete Fourier transforms","secret key watermarking;WAV audio file;watermark generation;perceptual domain property;human auditory system;audio clip sequences;direct sequence spread spectrum;fast Fourier transform;frequency 44.1 kHz","","","","14","","13 Jul 2015","","","IEEE","IEEE Conferences"
"Audio-based age and gender identification to enhance the recommendation of TV content","S. E. Shepstone; Z. Tan; S. H. Jensen","Bang and Olufsen A/S, Peter Bangs Vej 15, 7600 Struer, Denmark; Department of Electronic Systems, Aalborg University, Niels Jernes Vej 12, 9220 Aalborg, Denmark; Department of Electronic Systems, Aalborg University, Niels Jernes Vej 12, 9220 Aalborg, Denmark","IEEE Transactions on Consumer Electronics","15 Oct 2013","2013","59","3","721","729","Recommending TV content to groups of viewers is best carried out when relevant information such as the demographics of the group is available. However, it can be difficult and time consuming to extract information for every user in the group. This paper shows how an audio analysis of the age and gender of a group of users watching the TV can be used for recommending a sequence of N short TV content items for the group. First, a state of the art audio-based classifier determines the age and gender of each user in an M-user group and creates a group profile. A genetic recommender algorithm then selects for each user in the profile, a single personalized multimedia item for viewing. When the number of items to be presented is different to the number of viewers in the group, i.e. M = N, a novel adaptation algorithm is proposed that first converts the M-user group profile to an N-slot content profile, thus ensuring that items are proportionally allocated to users with respect to their demographic categorization. The proposed system is compared to an ideal system where the group demographics are provided explicitly. Results using real speaker utterances show that, in spite of the inaccuracies of state-of-the-art age-and-gender detection systems, the proposed system has a significant ability to predict an item with a matching age and gender category. User studies were conducted where subjects were asked to rate a sequence of advertisements, where half of the advertisements were randomly selected, and the other half were selected using the audio-derived demographics. The recommended advertisements received a significant higher median rating of 7.75, as opposed to 4.25 for the randomly selected advertisements.","1558-4127","","10.1109/TCE.2013.6626261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6626261","demographic filtering;genetic algorithms;ageidentification;gender identification;proportional recommendation;advertisement","TV;Biological cells;Accuracy;Feature extraction;Statistics;Sociology;Genetic algorithms","audio signal processing;digital television;genetic algorithms;recommender systems;signal classification","audio based age identification;gender identification;TV content;demographic filtering;genetic algorithms;proportional identification;audio analysis;audio based classifier;group profile;genetic recommender algorithm;content profile;demographic categorization;real speaker utterances","","11","1","18","","15 Oct 2013","","","IEEE","IEEE Journals"
"Audio Coding Based on Spectral Recovery by Convolutional Neural Network","S. Shin; S. K. Beack; T. Lee; H. Park","Kwangwoon University, Seoul, Korea; Electronics and Telecommunication Research Institute, Daejeon, Korea; Electronics and Telecommunication Research Institute, Daejeon, Korea; Kwangwoon University, Seoul, Korea","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","725","729","This study proposes a new method of audio coding based on spectral recovery, which can enhance the performance of transform audio coding. An encoder represents spectral information of an input in a time-frequency domain and transmits only a portion of it so that the remaining spectral information can be recovered based on the transmitted information. A decoder recovers the magnitudes of missing spectral information using a convolutional neural network. The signs of missing spectral information are either transmitted or randomly assigned, according to their importance. By combining transmission and recovery of spectral information, the proposed method can enhance the coding performance, compared with conventional transform coding. The subjective performance evaluation shows that, for mono coding at 39.4 kbps, the proposed method provides higher sound quality than the USAC, by an average MUSHRA score of 8.5.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8682268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682268","audio coding;convolutional neural network;spectral recovery;transform coding","Decoding;Transform coding;Two dimensional displays;Audio coding;Convolutional neural networks;Correlation","audio coding;convolutional neural nets;decoding;neural nets;spectral analysis;transform coding","coding performance;mono coding;spectral recovery;convolutional neural network;time-frequency domain;remaining spectral information;transmitted information;audio coding;spectral information;average MUSHRA score;missing spectral information","","2","","22","","17 Apr 2019","","","IEEE","IEEE Conferences"
"Information Fusion of Audio Emotion Recognition Based on Kernel Entropy Component Analysis in Canonical Correlation Space","L. Gao; L. Qi; L. Guan","Sch. of Inf. Eng., Zhengzhou Univ., Zhengzhou, China; Sch. of Inf. Eng., Zhengzhou Univ., Zhengzhou, China; Dept. of Electr. & Comput. Eng., Ryerson Univ., Toronto, ON, Canada","2015 IEEE International Symposium on Multimedia (ISM)","28 Mar 2016","2015","","","241","244","Kernel Entropy Component Analysis(KECA), an effective information fusion tool, is realized using descriptor of information entropy and optimized by entropy estimation. However, it merely put the information or data from different channels together to achieve the information fusion without considering their intrinsic structures and relations. In this paper, we enhance the performance of KECA by introducing KECA in Canonical Correlation Space (CCS) or KECA+CCS. Not only the intrinsic structures and relations are considered in CCS, but also the nature of input data are revealed by entropy estimation. It improves the recognition accuracy effectively. The effectiveness of the proposed method is evaluated through experimentation on two audio-based emotion databases. The results show that the proposed method outperforms the existing methods based on similar principles.","","978-1-5090-0379-2","10.1109/ISM.2015.129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442333","Information fusion;emotion recognition;kernel entropy component analysis;canonical correlation space","Kernel;Emotion recognition;Correlation;Entropy;Feature extraction;Databases;Mel frequency cepstral coefficient","audio signal processing;correlation methods;emotion recognition;entropy;operating system kernels","audio-based emotion database;recognition accuracy improvement;CCS;KECA performance enhancement;entropy estimation;information entropy;information fusion tool;canonical correlation space;kernel entropy component analysis;audio emotion recognition information fusion","","","","15","","28 Mar 2016","","","IEEE","IEEE Conferences"
"Automatic recognition of audio event using dynamic local binary patterns","C. Wang; Y. Chin; T. Tai; D. Gunawan; J. Wang","Department of Computer Science and Information Engineering, National Central University; Department of Computer Science and Information Engineering, National Central University; Department of Computer Science and Information Engineering, Providence University; Department of Computer Science and Information Engineering, National Central University; Department of Computer Science and Information Engineering, National Central University","2015 IEEE International Conference on Consumer Electronics - Taiwan","24 Aug 2015","2015","","","246","247","This work proposes an automatic recognition system for recognizing audio events. First, an audio signal is converted into a spectrogram by short time Fourier transform. The acoustic background noises in the spectrogram are reduced by box filtering. The contrast of the spectrogram is then enhanced by VAR operation. With the enhanced spectrogram, this work further proposes a novel dynamic local binary pattern (DLBP) feature based on human auditory system. Finally, the DLBP features are fed to multi-class support vector machines to achieve the audio event recognition. The experimental results on 16 classes of audio events demonstrate the performance of the proposed audio event recognition system.","","978-1-4799-8745-0","10.1109/ICCE-TW.2015.7216879","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7216879","","Spectrogram;Feature extraction;Pattern recognition;Support vector machines;Filtering;Auditory system;Speech","audio signal processing;filtering theory;Fourier transforms;support vector machines","dynamic local binary patterns;audio event automatic recognition;short time Fourier transform;box filtering;VAR operation;human auditory system;multiclass support vector machines","","","","9","","24 Aug 2015","","","IEEE","IEEE Conferences"
"5.2 An 8Ω 10W 91%-power-efficiency 0.0023%-THD+N multi-level Class-D audio amplifier with folded PWM","J. Lee; J. Bang; K. Kim; H. Gwon; S. Park; Y. Huh; K. Yoon; J. Baek; Y. Ju; G. Lee; H. Park; H. Bae; G. Cho","KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea","2017 IEEE International Solid-State Circuits Conference (ISSCC)","6 Mar 2017","2017","","","88","89","As the portable device market tries to enhance user experience, high-power audio systems with boosted supply voltage have been the main design focus recently. Several past works have addressed issues related to boosted supply voltages [1,2]. Nevertheless, the power stage retained the classical H-bridge structure in the previous works, which resulted in aggravated electromagnetic interference (EMI) from high switching amplitude and poor efficiency due to voltage boosting. The use of multi-level pulse-width modulation (PWM) shown in Fig. 5.2.1 can naturally eliminate the complications caused by high supply voltages. Since the audio signal has a high crest factor, a multi-level Class-D amplifier draws most power directly from a low-voltage battery source, which in turn improves the power efficiency significantly [3]. Spread spectrum techniques prevent energy localization in the power spectral density [2]. Nevertheless, the diffusion of switching harmonics into the nearby frequencies complicates EMI management. However, the multi-level switching scheme suppresses EMI by reducing the switching amplitude without spreading the energy spectrum [4]. In this work, a new folded-PWM (FPWM) architecture implementing a multi-level H-bridge topology is presented.","2376-8606","978-1-5090-3758-2","10.1109/ISSCC.2017.7870274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7870274","","Pulse width modulation;Switches;Electromagnetic interference;Power generation;Power amplifiers;Topology;Bandwidth","audio-frequency amplifiers;bridge circuits;harmonic distortion;interference suppression;network topology;power aware computing;pulse width modulation","multilevel class-D audio amplifier;folded PWM;portable device market;user experience enhancement;high-power audio systems;electromagnetic interference;multilevel pulse width modulation;audio signal;low-voltage battery source;power efficiency improvement;spread spectrum techniques;energy localization prevention;power spectral density;switching harmonics;EMI management;switching amplitude reduction;EMI suppression;FPWM architecture;multilevel H-bridge topology;resistance 8 ohm;power 10 W","","","","6","","6 Mar 2017","","","IEEE","IEEE Conferences"
"Dynamic stochastic resonance-based improved watermark extraction from audio signal","O. Krishna; R. K. Jha; P. K. Biswas; M. M. Mushrif","Indian Institute of Information Technology, Design & Manufacturing Jabalpur (India); Indian Institute of Information Technology, Design & Manufacturing Jabalpur (India); IIT Kharagpur (India); YCCE Nagpur (India)","2012 National Conference on Communications (NCC)","3 Apr 2012","2012","","","1","5","In this paper a dynamic stochastic resonance (DSR)-based watermark extraction technique from audio signal in discrete wavelet transform domain has been presented. The watermark embedding has been done in detail coefficient of DWT transformed audio signal. DSR improves the authenticity of the extraction process by utilizing noise or degradation introduced during different signal processing attacks. DSR is an iterative process that tunes the coefficient of possibly attacked watermarked audio signal so that effect of noise is suppressed and hidden information is enhanced. An adaptive optimization procedure has been adopted for selection of bistable parameters to achieve maximum correlation coefficient under minimum computational complexity. Resilience of this technique has been tested in presence of various signal processing attacks. Using proposed technique robust extraction of watermark is obtained without trading off the audibility of audio signal.","","978-1-4673-0816-8","10.1109/NCC.2012.6176799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176799","Audio Watermarking;DWT;Dynamic Stochastic Resonance;Noise","Watermarking;Discrete wavelet transforms;Signal to noise ratio;Robustness;Stochastic resonance;Correlation","audio signal processing;audio watermarking;computational complexity;correlation methods;discrete wavelet transforms;iterative methods;optimisation;signal denoising","dynamic stochastic resonance-based improved watermark extraction;audio signal;discrete wavelet transform domain;watermark embedding;extraction process authenticity;signal processing attacks;iterative process;noise suppression;adaptive optimization procedure;bistable parameter selection;maximum correlation coefficient;minimum computational complexity","","1","","16","","3 Apr 2012","","","IEEE","IEEE Conferences"
"Audio segmentation based approach for improved emotion recognition","M. A. Pandharipande; S. K. Kopparapu","TCS Innovation Labs - Mumbai, Tata Consultancy Services, Thane (West), Maharastra 400 601, India; TCS Innovation Labs - Mumbai, Tata Consultancy Services, Thane (West), Maharastra 400 601, India","TENCON 2015 - 2015 IEEE Region 10 Conference","7 Jan 2016","2015","","","1","4","Recognition of emotion expressed by a customer in spoken speech is gaining importance because of the expansion of services industry and the need to identify the sentiment of the customer which are not available through a text channel. In this paper, we propose a simple yet effective method to enhance the recognition accuracy of emotion in spoken speech. The approach is based on segmenting the audio into smaller, fixed length, segments and recognizing the emotion of each segment individually before fusing the identified emotions to mark the emotion of the audio file. We show through experiments the enhanced performance of the our method on a freely available emotion database called Emo-DB.","2159-3450","978-1-4799-8641-5","10.1109/TENCON.2015.7372933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372933","","","audio signal processing;emotion recognition;speech recognition","audio segmentation based approach;emotion recognition improvement;text channel;audio file;emotion database","","1","","10","","7 Jan 2016","","","IEEE","IEEE Conferences"
"Leveraging Frequency-Dependent Kernel and DIP-Based Clustering for Robust Speech Activity Detection in Naturalistic Audio Streams","H. Dubey; A. Sangwan; J. H. L. Hansen","Robust Speech Technologies Lab, Center for Robust Speech Systems, Department of Electrical Engineering, The University of Texas at Dallas, Richardson, TX, USA; Robust Speech Technologies Lab, Center for Robust Speech Systems, Department of Electrical Engineering, The University of Texas at Dallas, Richardson, TX, USA; Robust Speech Technologies Lab, Center for Robust Speech Systems, Department of Electrical Engineering, The University of Texas at Dallas, Richardson, TX, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","8 Aug 2018","2018","26","11","2056","2071","Speech activity detection (SAD) is front-end in most speech systems, e.g., speaker verification, speech recognition etc. Supervised SAD typically leverages machine learning models trained on annotated data. For applications like zero-resource speech processing and NIST-OpenSAT-2017 public safety communications task, it might not be feasible to collect SAD annotations. SAD is challenging for naturalistic audio streams containing multiple noise-sources simultaneously. We propose a novel frequency-dependent kernel (FDK) based SAD features. FDK provides enhanced spectral decomposition from which several statistical descriptors are derived. FDK statistical descriptors are combined by principal component analysis into one-dimensional FDK-SAD features. We further proposed two decision backends: First, variable model-size Gaussian mixture model (VMGMM); and second, Hartigan dip-based robust feature clustering. While VMGMM is a model-based approach, the DipSAD is nonparametric. We used both backends for comparative evaluations in two phases: first, standalone SAD performance; and second, the effect of SAD on text-dependent speaker verification using RedDots data. The NIST-OpenSAD-2015 and NIST-OpenSAT-2017 corpora are used for standalone SAD evaluations. We establish two Center for Robust Speech Systems (CRSS) corpora namely CRSS-PLTL-II and CRSS long-duration naturalistic noise corpus. The CRSS corpora facilitate standalone SAD evaluations on naturalistic audio streams. We performed comparative studies of the proposed approaches with multiple baselines including SohnSAD, rSAD, semisupervised Gaussian mixture model, and Gammatone spectrogram features.","2329-9304","","10.1109/TASLP.2018.2848698","Air Force Research Laboratory(grant numbers:FA8750-15-1-0205); University of Texas at Dallas from the Distinguished University Chair in Telecommunications Engineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8401923","Clustering;DARPA RATS;frequency-dependent kernel;Hartigan dip test;peer-led team learning;speech activity detection;NIST OpenSAD;NIST OpenSAT","Training;Robustness;Speech processing;Rats;Signal to noise ratio;Kernel;Gaussian mixture model","audio streaming;feature extraction;Gaussian processes;learning (artificial intelligence);pattern clustering;principal component analysis;speaker recognition;speech processing;statistical analysis","naturalistic audio streams;supervised SAD;machine learning models;SAD annotations;multiple noise-sources;frequency-dependent kernel based SAD features;enhanced spectral decomposition;FDK statistical descriptors;one-dimensional FDK-SAD features;variable model-size Gaussian mixture model;VMGMM;Hartigan dip-based robust feature clustering;model-based approach;text-dependent speaker verification;NIST-OpenSAD-2015;standalone SAD evaluations;CRSS long-duration naturalistic noise corpus;semisupervised Gaussian mixture model;robust speech activity detection;NIST-OpenSAT-2017;principal component analysis","","4","","63","","2 Jul 2018","","","IEEE","IEEE Journals"
"Improving hands-free speech recognition in a car through audio-visual voice activity detection","F. Faubel; M. Georges; K. Kumatani; A. Bruhn; D. Klakow","Saarland University, Saarbrücken, Germany; Saarland University, Saarbrücken, Germany; Saarland University, Saarbrücken, Germany; Saarland University, Saarbrücken, Germany; Saarland University, Saarbrücken, Germany","2011 Joint Workshop on Hands-free Speech Communication and Microphone Arrays","7 Jul 2011","2011","","","70","75","In this work, we show how the speech recognition performance in a noisy car environment can be improved by combining audio-visual voice activity detection (VAD) with microphone array processing techniques. That is accomplished by enhancing the multi-channel audio signal in the speaker localization step, through per channel power spectral subtraction whose noise estimates are obtained from the non-speech segments identified by VAD. This noise reduction step improves the accuracy of the estimated speaker positions and thereby the quality of the beamformed signal of the consecutive array processing step. Audio-visual voice activity detection has the advantage of being more robust in acoustically demanding environments. This claim is substantiated through speech recognition experiments on the AVICAR corpus, where the proposed localization framework gave a WER of 7.1% in combination with delay-and-sum beamforming. This compares to a WER of 8.9% for speaker localizing with audio-only VAD and 11.6% without VAD and 15.6 for a single distant channel.","","978-1-4577-0999-9","10.1109/HSCMA.2011.5942412","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5942412","microphone arrays;audio-visual systems;acoustic signal detection;time of arrival estimation;automatic speech recognition","Feature extraction;Visualization;Speech;Mouth;Speech recognition;Noise;Hidden Markov models","acoustic signal detection;audio-visual systems;microphone arrays;speech recognition","hands-free speech recognition;audio-visual voice activity detection;noisy car environment;microphone array processing;multichannel audio signal enhancement;speaker localization;power spectral subtraction;non-speech segments;noise reduction;speaker positions;AVICAR corpus;delay-and-sum beamforming;acoustic signal detection","","7","","17","","7 Jul 2011","","","IEEE","IEEE Conferences"
"An Improved Discrete Fourier Transform-Based Algorithm for Electric Network Frequency Extraction","L. Fu; P. N. Markham; R. W. Conners; Y. Liu","School of Electrical Engineering, Southwest Jiaotong University, Chengdu, China; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA; Bradley Department of Electrical and Computer Engineering, Virginia Polytechnic Institute and State University, Blacksburg, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA","IEEE Transactions on Information Forensics and Security","13 Jun 2013","2013","8","7","1173","1181","This paper introduces a Discrete Fourier Transform (DFT)-based algorithm to extract the Electric Network Frequency (ENF) information from an audio recording for use in audio authentication. The basic idea of the proposed algorithm is to calculate the specific spectral lines by DFT in the frequency domain at the desired frequency point instead of throughout the entire frequency band. Then a binary search technique is employed to search the next desired frequency bin to repeat the spectral line calculation until the hidden ENF information is extracted. The purpose is to improve the accuracy and precision of conventional ENF extraction methods and also to enhance the calculation efficiency. Both simulated audio signals with different signal-to-noise ratios (SNRs) and actual audio recordings are studied to verify the performance of the proposed algorithm. Two error-evaluation criteria, frequency offset and frequency bias, are defined to evaluate the algorithm performance on accuracy and precision. The test results and the error evaluation prove the validation and demonstrate the improvement of the proposed algorithm.","1556-6021","","10.1109/TIFS.2013.2265088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6521399","Audio authentication;digital audio recording;discrete Fourier transform (DFT);electric network frequency (ENF);frequency disturbance recorder (FDR)","Discrete Fourier transforms;Accuracy;Audio recording;Data mining;Time-frequency analysis;Signal to noise ratio","audio recording;fast Fourier transforms;search problems","error-evaluation criteria;simulated audio signals;signal-to-noise ratios;hidden ENF information;binary search technique;DFT;electric network frequency;audio recording;DFT-based algorithm;electric network frequency extraction;discrete fourier transform-based algorithm","","19","","26","","29 May 2013","","","IEEE","IEEE Journals"
"SARIM: A gesture-based sound augmented reality interface for visiting museums","F. Z. Kaghat; A. Azough; M. Fakhour","Cedric, National Conservatory of Arts and Crafts, Paris, France; LIM Laboratory, Faculty of Sciences, Sidi Mohamed Ben Abdellah University, Fez, Morocco; LIM Laboratory, Faculty of Sciences, Sidi Mohamed Ben Abdellah University, Fez, Morocco","2018 International Conference on Intelligent Systems and Computer Vision (ISCV)","7 May 2018","2018","","","1","9","The objective of this work is to explore the use of sound augmented reality in order to enhance the museum visit. We aim to provide an audio guide to immerse the visitor in an audio scene consisting of ambient sounds and comments associated with the exhibits, while minimizing its effort to discover these objects and interact with the sound environment. The first contribution of this work is the implementation the concept proof of SARIM (Sound Augmented Reality Interface for visiting Museum). The second contribution is the modeling of the museum visit augmented by the sound dimension. Inspired from some existing models, the objective is to design a complete and integrated model that includes a representation of the visitor, the soundscape and the navigation parts. The purpose of this model is to facilitate the design of different scenarios based on the concept of audibility zones. The third contribution is the evaluation conducted in a real environment which is the Musée des arts et Métiers (MAM) in Paris. This evaluation has confirmed the usability, as well as the educational and playful positive impact of the audio augmented reality in general, and the SARIM in particular comparing to traditional audio-guides.","","978-1-5386-4396-9","10.1109/ISACV.2018.8354050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8354050","Audio Augmented Reality;Sound Spatialisation;Human Behaviour tracking;indoor localisation","Navigation;Augmented reality;Headphones;Hip;Art;Games;Corona","art;audio signal processing;audio user interfaces;augmented reality;gesture recognition;human computer interaction;museums","SARIM;sound augmented reality interface;museums;museum visit;audio guide;audio scene;ambient sounds;sound environment;sound dimension;audio augmented reality;traditional audio-guides;Sound Augmented Reality Interface for visiting Museum","","","","31","","7 May 2018","","","IEEE","IEEE Conferences"
"Audiovisual Synchrony Detection with Optimized Audio Features","S. Sieranoja; M. Sahidullah; T. Kinnunen; J. Komulainen; A. Hadid","School of Computing, University of Eastern Finland, Joensuu, Finland; School of Computing, University of Eastern Finland, Joensuu, Finland; School of Computing, University of Eastern Finland, Joensuu, Finland; Center for Machine Vision and Signal Analysis (CMVS), University of Oulu, Oulu, Finland; Center for Machine Vision and Signal Analysis (CMVS), University of Oulu, Oulu, Finland","2018 IEEE 3rd International Conference on Signal and Image Processing (ICSIP)","3 Jan 2019","2018","","","377","381","Audiovisual speech synchrony detection is an important part of talking-face verification systems. Prior work has primarily focused on visual features and joint-space models, while standard mel-frequency cepstral coefficients (MFCCs) have been commonly used to present speech. We focus more closely on audio by studying the impact of context window length for delta feature computation and comparing MFCCs with simpler energy-based features in lip-sync detection. We select state-of-the-art hand-crafted lip-sync visual features, space-time auto-correlation of gradients (STACOG), and canonical correlation analysis (CCA), for joint-space modeling. To enhance joint space modeling, we adopt deep CCA (DCCA), a nonlinear extension of CCA. Our results on the XM2VTS data indicate substantially enhanced audiovisual speech synchrony detection, with an equal error rate (EER) of 3.68%. Further analysis reveals that failed lip region localization and beardedness of the subjects constitutes most of the errors. Thus, the lip motion description is the bottleneck, while the use of novel audio features or joint-modeling techniques is unlikely to boost lip-sync detection accuracy further.","","978-1-5386-6396-7","10.1109/SIPROCESS.2018.8600424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8600424","Audiovisual Synchrony;Presentation Attack Detection;Multimodal Processing;Feature Extraction;Mel-Frequency Cepstral Coefficients (MFCCs)","Feature extraction;Mel frequency cepstral coefficient;Visualization;Correlation;Microsoft Windows;Lips;Face","audio signal processing;audio-visual systems;cepstral analysis;correlation methods;face recognition;feature extraction;speaker recognition;speech recognition","optimized audio features;audiovisual speech synchrony detection;talking-face verification systems;joint-space models;standard mel-frequency cepstral coefficients;context window length;delta feature computation;simpler energy-based features;space-time auto-correlation;canonical correlation analysis;CCA;joint-space modeling;failed lip region localization;novel audio features;lip-sync detection accuracy;audiovisual synchrony detection;hand-crafted lip-sync visual features","","1","","28","","3 Jan 2019","","","IEEE","IEEE Conferences"
"Enhanced Mining of Audio Signals from Optimal Intrinsic Mode Functions Through Statistical Analysis","A. J. Albin; N. M. Nandhitha; S. E. Roslin","Faculty of Computing, Sathyabama Institute of Science and Technology, Chennai, Tamil Nadu; School of Electrical & Electronics Engg., Sathyabama Institute of Science and Technology, Chennai, Tamil Nadu; School of Electrical & Electronics Engg., Sathyabama Institute of Science and Technology, Chennai, Tamil Nadu","2018 2nd International Conference on Trends in Electronics and Informatics (ICOEI)","2 Dec 2018","2018","","","970","975","Automated speaker recognition system is extremely important in areas such as Forensic and Defence. Performance of an automated speaker recognition system is dependent on feature extraction and classification. As speech is a non stationary signal and most of the information is present in the low frequency region, sub band coding or multi resolution analysis is favored. Empirical Mode Decomposition (EMD) is used for decomposing the signal and pitch, period, number of peaks, mean, standard deviation, skewness, kurtosis, energy, zero crossing rate, second order moment and third order moment are used for aggregating these coefficients. Adaptive Resonance Theory (ART) is chosen as the classifier. Sensitivity is the metric used for studying the performance of the proposed technique. From the results, it is also proved that the proposed ART based speaker recognition system using Empirical Mode Decomposition parameters gives 100% sensitivity for the acquired research database.","","978-1-5386-3570-4","10.1109/ICOEI.2018.8553905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8553905","Automated speaker recognition system;Feature extraction;Empirical mode decomposition;Adaptive resonance theory","Databases;Conferences;Feature extraction;Empirical mode decomposition;Standards;Speaker recognition;Subspace constraints","feature extraction;Hilbert transforms;speaker recognition;statistical analysis;wavelet transforms","enhanced mining;audio signals;statistical analysis;automated speaker recognition system;feature extraction;nonstationary signal;multiresolution analysis;order moment;ART based speaker recognition system;intrinsic mode functions;empirical mode decomposition parameters;EMD parameters;standard deviation;skewness;kurtosis;zero crossing rate;second order moment;third order moment","","","","13","","2 Dec 2018","","","IEEE","IEEE Conferences"
"Image2Audio: Facilitating Semi-supervised Audio Emotion Recognition with Facial Expression Image","G. He; X. Liu; F. Fan; J. You",Florida State University; Harvard University; Harvard University; HK PolyU,"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","28 Jul 2020","2020","","","3978","3983","There is a large amount of public available labeled image-based facial expression recognition datasets. How could these images help for the audio emotion recognition with limited labeled data according to their inherent correlations can be a meaningful and challenging task. In this paper, we propose a semi-supervised adversarial network that allows the knowledge transfer from the labeled videos to the heterogeneous labeled audio domain hence enhancing the audio emotion recognition performance. Specifically, face image samples are translated to the spectrograms class-wisely. To harness the translated samples in a sparsely distributed area and construct a tighter decision boundary, we propose to precisely estimate the density on feature space and incorporate the reliable low-density sample with an annealing scheme. Moreover, the unlabeled audios are collected with the high-density path in a graph representation. As a possible ""recognition via generation"" framework, we empirically demonstrated its effectiveness on several audio emotional recognition benchmarks.","2160-7516","978-1-7281-9360-1","10.1109/CVPRW50498.2020.00464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9150723","","Spectrogram;Training;Gallium nitride;Emotion recognition;Reliability;Visualization;Face recognition","audio signal processing;emotion recognition;face recognition;feature extraction","image2audio;semisupervised audio emotion recognition;facial expression image;public available labeled image-based facial expression recognition datasets;semisupervised adversarial network;labeled videos;heterogeneous labeled audio domain;audio emotion recognition performance;face image samples;translated samples;low-density sample;unlabeled audios;audio emotional recognition benchmarks","","3","","46","","28 Jul 2020","","","IEEE","IEEE Conferences"
"Joint Estimation of Chords and Downbeats From an Audio Signal","H. Papadopoulos; G. Peeters","Sound Analysis/Synthesis Team, IRCAM/CNRS-STMS, Paris, FRANCE; Sound Analysis/Synthesis Team, IRCAM/CNRS-STMS, Paris, FRANCE","IEEE Transactions on Audio, Speech, and Language Processing","4 Oct 2010","2011","19","1","138","152","We present a new technique for joint estimation of the chord progression and the downbeats from an audio file. Musical signals are highly structured in terms of harmony and rhythm. In this paper, we intend to show that integrating knowledge of mutual dependencies between chords and metric structure allows us to enhance the estimation of these musical attributes. For this, we propose a specific topology of hidden Markov models that enables modelling chord dependence on metric structure. This model allows us to consider pieces with complex metric structures such as beat addition, beat deletion or changes in the meter. The model is evaluated on a large set of popular music songs from the Beatles that present various metric structures. We compare a semi-automatic model in which the beat positions are annotated, with a fully automatic model in which a beat tracker is used as a front-end of the system. The results show that the downbeat positions of a music piece can be estimated in terms of its harmonic structure and that conversely the chord progression estimation benefits from considering the interaction between the metric and the harmonic structures.","1558-7924","","10.1109/TASL.2010.2045236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5428860","Chords;downbeat;hidden Markov model (HMM)","Hidden Markov models;Music information retrieval;Content based retrieval;Postal services;Rhythm;Topology;Information analysis;Data mining;Signal synthesis;Electrical capacitance tomography","audio signal processing;hidden Markov models;music","audio signals;chord progression;downbeats;musical signal estimation;music harmony;music rhythm;metric structure;hidden Markov model;beat deletion;beat addition;music songs;beat tracker;chord progression estimation;audio file","","28","2","39","","11 Mar 2010","","","IEEE","IEEE Journals"
"Speech and audio processing laboratory: Speech coding related signal processing modules","S. I. Ali; R. Hasan; M. S. Hayat","Department of Computing, Middle East College, Muscat, Sultanate of Oman; Department of Computing, Middle East College, Muscat, Sultanate of Oman; Department of Computing, Middle East College, Muscat, Sultanate of Oman","2015 2nd World Symposium on Web Applications and Networking (WSWAN)","20 Aug 2015","2015","","","1","5","This research paper reports in the Speech and Audio Processing Laboratory (SAP). Introducing the basic concepts of speech coding related to signal processing modules. The SAP Laboratory enhances the student's education through multimedia signal processing learning through speech and audio coding.","","978-1-4799-8172-4","10.1109/WSWAN.2015.7210356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7210356","Speech processing;matlab;Linear predication;signal;LP Filtering;Quantization;Levinson Durbin Recurison","Quantization (signal);Speech;Mathematical model;Finite impulse response filters;Speech recognition;Speech processing;MATLAB","audio coding;laboratories;speech coding","speech and audio processing laboratory;speech coding;signal processing modules;SAP laboratory;multimedia signal processing learning;audio coding;education","","","","6","","20 Aug 2015","","","IEEE","IEEE Conferences"
"Audio-Based Emotion Recognition Enhancement Through Progressive Gans","C. Athanasiadis; E. Hortal; S. Asteriadis","Maastricht University,Department of Data Science and Knowledge Engineering (DKE),the Netherlands; Maastricht University,Department of Data Science and Knowledge Engineering (DKE),the Netherlands; Maastricht University,Department of Data Science and Knowledge Engineering (DKE),the Netherlands","2020 IEEE International Conference on Image Processing (ICIP)","30 Sep 2020","2020","","","236","240","Training large-scale architectures such as Generative Adversarial Networks (GANs) in order to investigate audio-visual relations in emotion-enriched interactions is a challenging task. This procedure is hindered by the high complexity as well as the mode collapse phenomenon. Sufficiently training these architectures requires a massive amount of data. Furthermore, creating extensive audio-visual datasets for specific tasks, like emotion recognition, is a complicated task handicapped by the annotation cost and labelling ambiguities. On the other hand, it is much more forthright to get access to unlabeled audio-visual datasets mainly due to the easy access to online multimedia content. In this work, a progressive process for training GANs was conducted. The first step leverages enormous audio-visual unlabeled datasets to expose concealed cross-modal relationships. Meanwhile in the second step, a calibration of the weights by employing a limited amount of emotion annotated data was performed. Through experimentation, it was shown that our progressive GANs schema leads to a more efficient optimization of the whole network, and the generated samples from the target domain, when fused with the authentic ones, provides enhanced emotion recognition results.","2381-8549","978-1-7281-6395-6","10.1109/ICIP40778.2020.9190959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9190959","Domain Adaptation;Affective Computing;Generative Adversarial Networks","Training;Emotion recognition;Face recognition;Volume measurement;Labeling;Task analysis;Optimization","audio signal processing;emotion recognition;learning (artificial intelligence);neural nets","audio-based emotion recognition enhancement;large-scale architectures;Generative Adversarial Networks;audio-visual relations;emotion-enriched interactions;mode collapse phenomenon;audio-visual datasets;complicated task;annotation cost;labelling ambiguities;progressive process;training GANs;audio-visual unlabeled datasets;emotion annotated data;progressive GANs schema;enhanced emotion recognition results","","","","22","","30 Sep 2020","","","IEEE","IEEE Conferences"
"Using enhanced F0-trajectories for multiple speaker detection in audio monitoring scenarios","A. Cornaggia-Urrigshardt; F. Kurth","Fraunhofer FKIE, Communication Systems Fraunhoferstr. 20 53343 Wachtberg, Germany; Fraunhofer FKIE, Communication Systems Fraunhoferstr. 20 53343 Wachtberg, Germany","2015 23rd European Signal Processing Conference (EUSIPCO)","28 Dec 2015","2015","","","1093","1097","We propose to use enhanced F0-trajectories, which are extracted using shift-autocorrelation (shift-ACF), for multiple speaker detection in audio monitoring scenarios. After introducing spectral shift-ACF features, their performance in a multiple FO-extraction in the presence of different noise types is estimated for synthetic signal scenarios. Afterwards, at novel method for F0-supertrajectory extraction is proposeds and evaluated for multiple speaker detection in the presence of background noises that typically occur in audio monitoring. It turns out that due to their improved sharpness in representing harmonic components, spectral shift-ACF features outperform classical features in many cases.","2076-1465","978-0-9928-6263-3","10.1109/EUSIPCO.2015.7362552","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362552","Multiple Speaker Detection;Audio Monitoring;FO-Trajectories;Shift-ACF","Feature extraction;Speech;Harmonic analysis;Noise measurement;Gaussian noise;Monitoring;Robustness","correlation methods;feature extraction;speaker recognition;spectral analysis","enhanced F0-trajectories;multiple speaker detection;audio monitoring scenario;shift-autocorrelation;spectral shift-ACF feature;multiple F0-extraction;synthetic signal scenario;F0-supertrajectory extraction;background noises;harmonic component representation","","","","8","","28 Dec 2015","","","IEEE","IEEE Conferences"
"Watch to Listen Clearly: Visual Speech Enhancement Driven Multi-modality Speech Recognition","B. Xu; J. Wang; C. Lu; Y. Guo",Xpeng motors; Xpeng motors; Xpeng motors; Xpeng motors,"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 May 2020","2020","","","1626","1635","Multi-modality (talking face video and audio) information helps improve speech recognition performance compared to the single modality. In noisy environments, the effect of audio modality is weakened, which further affects the performance of multi-modality speech recognition (MSR). Most of the MSR methods use noisy audio signal as input of the audio modality without any enhancement (filtering the noisy components in the audio signal). In this paper, we propose an audio-enhanced multi-modality speech recognition model. In particular, the proposed model consists of two sub-networks, one is the visual speech enhancement (VE) sub-network and the other is the multi-modality speech recognition (MSR) sub-network. The VE sub-network is able to separate a speaker's voice from background noises when given the corresponding talking face to enhance audio modality. Then the audio modality together with video modality are fed into the MSR sub-network to produce characters. We introduce a pseudo-3D residual network (P3D)based visual front-end to extract more advantageous visual features. The MSR sub-network is built on top of the Element-wise-Attention Gated Recurrent Unit (EleAttGRU) architecture which is more effective than Transformer in long sequences. We demonstrate the effectiveness of audio enhancement for MSR by extensive experiments. The proposed method surpasses the state-of-the-art MSR models on the LRS3-TED dataset and the LRW dataset.","2642-9381","978-1-7281-6553-0","10.1109/WACV45572.2020.9093314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093314","","Visualization;Speech recognition;Feature extraction;Speech enhancement;Noise measurement;Lips;Convolution","audio signal processing;audio-visual systems;face recognition;feature extraction;speech enhancement;speech recognition","audio enhancement;video modality;multimodality speech recognition sub-network;visual speech enhancement sub-network;audio-enhanced multimodality speech recognition model;noisy audio signal;MSR;audio modality;single modality;speech recognition performance;talking face video;visual speech enhancement driven multimodality speech recognition","","","","49","","14 May 2020","","","IEEE","IEEE Conferences"
"Improving bitrate in detail coefficient based audio watermarking using wavelet transformation","K. Datta; I. Sengupta","School of Computer Engineering, KIIT University, Bhubaneswar-751024, India; Dept of Computer Science and Engg, Indian Institute of Technology, Kharagpur-721302, India","2011 International Conference on Communications and Signal Processing","24 Mar 2011","2011","","","160","164","With the development in communication technology over the past few decade, the usage of multimedia contents have increased progressively. Multimedia data protection has become a very important issue which needs to be addressed at the earliest. In this paper we have proposed a multimedia data protection technique for audio files. There exists various audio watermarking techniques in the literature. In this paper we propose a wavelet based watermarking technique where embedding is performed on the third level detail wavelet coefficients. The robustness of the scheme is found to be at an acceptable level with respect to some of the existing techniques in wavelet domain. The proposed method is essentially an improvement of the works reported in [1], [2], where the bit rates of the watermark data are enhanced with modest degradation in robustness. Subjective tests have been performed to evaluate the performance of the proposed method.","","978-1-4244-9799-7","10.1109/ICCSP.2011.5739291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5739291","","Variable speed drives;Rocks;Robustness","audio watermarking;error statistics;multimedia communication;wavelet transforms","bit rate;wavelet transformation;multimedia content;multimedia data protection technique;audio files;wavelet based audio watermarking technique;wavelet coefficients;watermark data","","4","","13","","24 Mar 2011","","","IEEE","IEEE Conferences"
"Hiding of compressed bit stream into audio file to enhance the confidentiality and portability of a data transmission system","S. Bhattacharjee; L. B. A. Rahim; I. B. A. Aziz","Department of Computer & Information Science, Universiti Teknologi PETRONAS, Bandar Seri-Iskandar, Perak, Malaysia; Department of Computer & Information Science, Universiti Teknologi PETRONAS, Bandar Seri-Iskandar, Perak, Malaysia; Department of Computer & Information Science, Universiti Teknologi PETRONAS, Bandar Seri-Iskandar, Perak, Malaysia","2015 International Symposium on Mathematical Sciences and Computing Research (iSMSC)","20 Oct 2016","2015","","","196","201","Data security is always an important aspect of any data transmission system. With the advancement of internet, various security threats and challenges are also becoming more powerful. However, among the various security challenges in any transmission system, confidentiality, portability, and data losses are most important concern. Therefore, to solve these issues, we have proposed a lossless fixed length coding based compression to reduce the file size and to increase portability of transmitted data. To enhance confidentiality and reduce data loss, the compressed bit stream is incorporated with in the audio file using a unique audio steganography. Efficiencies of the proposed technique are examined by experimented results and compared with other existing to show its effectiveness.","","978-1-4799-7896-0","10.1109/ISMSC.2015.7594052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7594052","security threats and challenges;confidentiality and portability of data;audio steganography;compressed bit stream","Encoding;Security;Propagation losses;Robustness;Transforms;Data compression;Media","audio coding;data compression;steganography","compressed bit stream hiding;data transmission system;data confidentiality;data portability;data security;security threats;Internet;data losses;lossless fixed length coding based compression;audio steganography","","1","","14","","20 Oct 2016","","","IEEE","IEEE Conferences"
"Enhanced, blind and robust far-field audio acquisition for portable devices","R. Tong; Z. Ye","Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei 230027, China, and also with the National Engineering Laboratory for Speech and Language Information Processing, Hefei 230027, China; Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei 230027, China, and also with the National Engineering Laboratory for Speech and Language Information Processing, Hefei 230027, China","IEEE Transactions on Consumer Electronics","25 May 2017","2017","63","1","62","67","In this paper, a bilinear Wiener filtering method is proposed for multichannel audio acquisition without knowing the array configurations and frequency responses. Compared with mainstream algorithms, the proposed method has two important features: blindness and robustness. For the first feature, the method does not require any prior knowledge about the array manifold. The “blindness” is very attractive to the industry because even microphones picked from the same batch of the same manufacturer can be inconsistent. For the second feature, even in the presence of strong sensor noise, the method can yield a good performance in suppressing directional interferences. This helps bring better “robustness” to the method and make it more practical for real environments. In the method, a rectangular window is adopted to slide continuously over the parallel multichannel audio streams and then bilinear Wiener filtering is performed on the matrix denoting windowed parallel audio streams. Experiments in a recording studio show the method can suppress directional interferences well. Besides, it converges rapidly and is an appealing choice for consumer electronics.","1558-4127","","10.1109/TCE.2017.014687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7931971","","Wiener filters;Maximum likelihood detection;Nonlinear filters;Robustness;Array signal processing;Manifolds;Microphones","audio signal processing;interference suppression;Wiener filters","robust far-field audio acquisition;bilinear Wiener filtering method;multichannel audio acquisition;directional interference suppression;parallel multichannel audio streams","","2","","16","","25 May 2017","","","IEEE","IEEE Journals"
"Video-informed approach for enhancing audio source separation through noise source suppression","J. Harris; B. Rivet; S. M. Naqvi; J. A. Chambers; C. Jutten","GIPSA-Lab, CNRS UMR 5216, Université de Grenoble, France; GIPSA-Lab, CNRS UMR 5216, Université de Grenoble, France; School of Electronic, Electrical and Systems Engineering, Loughborough University, UK; School of Electronic, Electrical and Systems Engineering, Loughborough University, UK; GIPSA-Lab, CNRS UMR 5216, Université de Grenoble, France","2013 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)","14 Nov 2013","2013","","","1","6","This paper describes a method where an interference noise source within an audio source separation scenario is suppressed from a mixture. The principal idea of the proposed method is to use a video camera array for locating a interference noise source whose 3D position will be used to estimate a matrix of frequency responses (FRs) by linearly combining a series of previously known FRs. A filter is calculated to remove the contribution of the noise source from a convolutive mixture at each microphone, through the estimated FRs. The proposed method is assumed to implemented in a `block-wise' manner in time domain and has been tested on mixtures created by impulse responses generated by the image method for small room acoustics.","2378-928X","978-1-4799-1180-6","10.1109/MLSP.2013.6661927","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6661927","audio-visual source separation;source enhancement;transfer function estimation;non-stationary sources","Noise;Source separation;Vectors;Microphones;Speech;Databases;Three-dimensional displays","audio signal processing;blind source separation;convolution;frequency response;image denoising;interference suppression;matrix algebra;time-domain analysis;transient response;video signal processing","impulse responses;small room acoustics;time domain;convolutive mixture;frequency responses;matrix estimation;interference noise source localization;video camera array;interference noise source suppression;audio source separation enhancement;video-informed approach","","","","18","","14 Nov 2013","","","IEEE","IEEE Conferences"
"Fusing MFCC and LPC Features Using 1D Triplet CNN for Speaker Recognition in Severely Degraded Audio Signals","A. Chowdhury; A. Ross","Department of Computer Science Engineering, Michigan State University, East Lansing, MI, USA; Department of Computer Science Engineering, Michigan State University, East Lansing, MI, USA","IEEE Transactions on Information Forensics and Security","16 Jan 2020","2020","15","","1616","1629","Speaker recognition algorithms are negatively impacted by the quality of the input speech signal. In this work, we approach the problem of speaker recognition from severely degraded audio data by judiciously combining two commonly used features: Mel Frequency Cepstral Coefficients (MFCC) and Linear Predictive Coding (LPC). Our hypothesis rests on the observation that MFCC and LPC capture two distinct aspects of speech, viz., speech perception and speech production. A carefully crafted 1D Triplet Convolutional Neural Network (1D-Triplet-CNN) is used to combine these two features in a novel manner, thereby enhancing the performance of speaker recognition in challenging scenarios. Extensive evaluation on multiple datasets, different types of audio degradations, multi-lingual speech, varying length of audio samples, etc. convey the efficacy of the proposed approach over existing speaker recognition methods, including those based on iVector and xVector.","1556-6021","","10.1109/TIFS.2019.2941773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839817","Speaker recognition;degraded audio;deep learning;MFCC;LPC;1-D CNN;feature-level fusion","Speaker recognition;Speech recognition;Noise measurement;Mel frequency cepstral coefficient;Speech processing;Feature extraction;Production","audio signal processing;cepstral analysis;convolutional neural nets;feature extraction;Gaussian processes;linear predictive coding;speaker recognition","crafted 1D Triplet Convolutional Neural Network;speech perception;Linear Predictive Coding;Mel Frequency Cepstral Coefficients;severely degraded audio data;input speech signal;speaker recognition algorithms;severely degraded audio signals;1D Triplet CNN;LPC features;MFCC;audio samples;multilingual speech;audio degradations;1D-Triplet-CNN;speech production","","8","","51","IEEE","16 Sep 2019","","","IEEE","IEEE Journals"
"Privacy-enhanced perceptual hashing of audio data","H. Knospe","Institute of Communications Engineering, Cologne University of Applied Sciences, 50679 Cologne, Germany","2013 International Conference on Security and Cryptography (SECRYPT)","27 Aug 2015","2013","","","1","6","Audio hashes are compact and robust representations of audio data and allow the efficient identification of specific recordings and their transformations. Audio hashing for music identification is well established and similar algorithms can also be used for speech data. A possible application is the identification of replayed telephone spam. This contribution investigates the security and privacy issues of perceptual hashes and follows an information-theoretic approach. The entropy of the hash should be large enough to prevent the exposure of audio content. We propose a privacy-enhanced randomized audio hash and analyze its entropy as well as its robustness and discrimination power over a large number of hashes.","","978-9-8975-8131-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7223214","Perceptual Hashing;Audio Hashing;Audio Fingerprinting;Acoustic Fingerprint;Privacy;Security","Robustness;Entropy;Multimedia communication;Security;Privacy;Speech;Codecs","","","","","","20","","27 Aug 2015","","","IEEE","IEEE Conferences"
"A novel sinusoidal approach to audio signal frame loss concealment and its application in the new evs codec standard","S. Bruhn; E. Norvell; J. Svedberg; S. Sverrisson","SMN, Ericsson Research, Ericsson AB, 164 80, Stockholm, Sweden; SMN, Ericsson Research, Ericsson AB, 164 80, Stockholm, Sweden; SMN, Ericsson Research, Ericsson AB, 164 80, Stockholm, Sweden; SMN, Ericsson Research, Ericsson AB, 164 80, Stockholm, Sweden","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","6 Aug 2015","2015","","","5142","5146","The new 3GPP codec for Enhanced Voice Services (EVS) comprises a collection of frame loss concealment techniques, each specifically designed for the different coding modes of that codec. One of them, called “Phase Error Concealment Unit (Phase ECU)”, was developed for the High Quality (HQ) MDCT coding mode. Despite this target application, Phase ECU is a generic stand-alone tool operating on a buffer of the previously decoded and reconstructed time signal. Its framework is based on the sinusoidal analysis and synthesis paradigm. Besides a description of the basic technology we present optimizations and adaptations required for meeting the challenging 3GPP EVS codec performance requirements, and that make the method robust for a broad range of audio signals under various frame loss conditions from isolated frame erasures to severe burst loss. Test results are reported that show significant improvements over traditional techniques.","2379-190X","978-1-4673-6997-8","10.1109/ICASSP.2015.7178951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178951","EVS;speech/audio coding;error concealment","Discrete Fourier transforms;Prototypes;Transient analysis;Speech codecs;Speech;Encoding","audio coding;codecs","severe burst loss;isolated frame erasures;frame loss conditions;audio signals;sinusoidal synthesis paradigm;sinusoidal analysis;reconstructed time signal;decoded time signal;High Quality MDCT coding mode;Phase ECU;Phase Error Concealment Unit;coding modes;frame loss concealment techniques;Enhanced Voice Services;3GPP codec;EVS codec standard;audio signal frame loss concealment","","1","2","18","","6 Aug 2015","","","IEEE","IEEE Conferences"
"An evaluation of haptic descriptions for audio described films for individuals who are blind","T. McDaniel; L. N. Viswanathan; S. Panchanathan","Center for Cognitive Ubiquitous Computing, School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, USA; Center for Cognitive Ubiquitous Computing, School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, USA; Center for Cognitive Ubiquitous Computing, School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, USA","2013 IEEE International Conference on Multimedia and Expo (ICME)","26 Sep 2013","2013","","","1","6","Narrations of visual content, or audio descriptions, enhance the accessibility of video media for individuals who are blind. While the adoption of this assistive technology continues to broaden, audio descriptions have several limitations: they often lack detail (i.e., are abridged) to fit within dialogue-free gaps, and are difficult to use in fast-paced sequences or scenes of continuous dialogue. Haptic descriptions offer a promising augmentation to audio descriptions by conveying information through the sense of touch. This work presents audio-haptic descriptions with a user evaluation showing how haptic descriptions can add relevant information to films for improved comprehension.","1945-788X","978-1-4799-0015-2","10.1109/ICME.2013.6607554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607554","Audio descriptions;haptics;assistive","Haptic interfaces;Films;Visualization;Vibrations;Belts;Motion pictures;Cameras","audio signal processing;handicapped aids;haptic interfaces;multimedia computing","haptic description evaluation;audio described films;visual content;audio descriptions;video media;assistive technology;dialogue free gaps","","1","","11","","26 Sep 2013","","","IEEE","IEEE Conferences"
"Enhanced time domain packet loss concealment in switched speech/audio codec","J. Lecomte; A. Tomasek; G. Markovic; M. Schnabel; K. Tsutsumi; K. Kikuiri","Fraunhofer IIS, Erlangen, Germany; Fraunhofer IIS, Erlangen, Germany; Fraunhofer IIS, Erlangen, Germany; Fraunhofer IIS, Erlangen, Germany; NTT DOCOMO, INC., Yokosuka, Japan; NTT DOCOMO, INC., Yokosuka, Japan","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","6 Aug 2015","2015","","","5922","5926","This paper describes new time domain techniques for concealing packet loss in the new 3GPP Enhanced Voice Services codec. Enhancements to the existing ACELP concealment methods include guided, improved pitch prediction, increased flexibility and accuracy of pulse resynchronization. Furthermore, the new method of separate linear predictive (LP) filter synthesis aims for sound quality improvement in case of multiple packet loss, especially for noisy signals. Another enhancement consists of a guided LP concealment approach to limit the risk of creating artifacts during recovery. These enhancements are also used in the presented advanced TCX concealment method. Subjective listening tests show that quality is significantly increased with these methods.","2379-190X","978-1-4673-6997-8","10.1109/ICASSP.2015.7179108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7179108","EVS;Packet Loss Concealment;guided concealment;ACELP;TCX","Codecs;Speech;Noise measurement;Packet loss;Time-domain analysis;Noise","audio acoustics;speech","enhanced time domain packet loss concealment;switched speech-audio codec;concealing packet loss;3GPP enhanced voice services codec;ACELP;pitch prediction;pulse resynchronization;separate linear predictive filter synthesis;TCX concealment method","","5","13","16","","6 Aug 2015","","","IEEE","IEEE Conferences"
"Complete SOC transceiver in 0.18µm CMOS using Q-enhanced filtering, sub-sampling and injection locking","R. Mason; J. Fortier; C. DeVries","SMSC, Ottawa, Ontario, Canada; Hittite Microwave Corporation, Ottawa, Ontario, Canada; Research In Motion, Waterloo, Ontario, Canada","2011 IEEE Custom Integrated Circuits Conference (CICC)","20 Oct 2011","2011","","","1","4","Portable audio products have not yet seen a wireless headphone solution that has been widely accepted. The main reason for this is that power consumption for current solutions (Bluetooth, analog) is too high. Secondly, the interference management is poor, leading to unacceptable performance in real-life applications. We present a solution that meets these problems and provides a workable platform for developing wireless audio headphones. The transceiver is fabricated in TSMC 0.18 μm CMOS and consumes a peak current of 10.2 mA in RX mode and 22 mA in TX mode.","2152-3630","978-1-4577-0223-5","10.1109/CICC.2011.6055351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6055351","","Receivers;Power harmonic filters;Injection-locked oscillators;Phase locked loops;Band pass filters;Resonator filters;Power demand","audio equipment;CMOS integrated circuits;filters;headphones;low-power electronics;radio transceivers;system-on-chip;UHF filters;UHF integrated circuits","SOC transceiver;CMOS;Q-enhanced filtering;subsampling;injection locking;portable audio product;power consumption;interference management;wireless audio headphone;size 0.18 mum;current 10.2 mA;current 22 mA;frequency 2.4 GHz","","","","10","","20 Oct 2011","","","IEEE","IEEE Conferences"
"Enhanced audio transmission over ADSL using prioritised DMT modulation and retransmissions","T. P. Fowdur; P. Ragpot; S. K. M. Soyjaudah","Department of Electrical and Electronic Engineering, University of Mauritius, Réduit, Mauritius; Department of Electrical and Electronic Engineering, University of Mauritius, Réduit, Mauritius; Department of Electrical and Electronic Engineering, University of Mauritius, Réduit, Mauritius","IEEE EUROCON 2015 - International Conference on Computer as a Tool (EUROCON)","2 Nov 2015","2015","","","1","6","Asymmetric Digital Subscriber Line (ADSL) is a broadband communications technology which is widely used for transmitting high bandwidth data such as audio and video over the internet. However, ensuring a good Quality of Service (QoS) in the transmission of such data over ADSL is challenging due to channel impairments such as noise and crosstalk. Moreover, given that the total monthly fixed data traffic is increasing at a phenomenal rate, (QoS) management is becoming a crucial issue. This paper proposes an enhanced transmission scheme for audio over ADSL. The proposed scheme exploits the unequal importance of the subbands generated by an MP1 codec to offer varying protection to them during ADSL transmission. This is achieved by multiplexing the bit streams from the subbands before Discrete Multi-tone Modulation (DMT) so that bit streams corresponding to more important subbands are offered higher protection than the least important ones. Additionally, a sophisticated retransmission protocol is proposed which allows frames from more important subbands to benefit from a greater number of retransmissions than least important ones. The retransmission protocol uses eight different Negative Acknowledgement (NACK) packet types which allow adaptive multiplexing of frames at the transmitter. The proposed Unequal Error Protection (UEP) scheme provides an average gain of over 28 dB in Segmented Signal to Noise Ratio (SSNR) over a conventional Equal Error Protection (EEP) one but incurs a negligible loss in throughput.","","978-1-4799-8569-2","10.1109/EUROCON.2015.7313673","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7313673","ADSL;Audio transmission;Prioritisation;QoS;DMT","Modulation;Multiplexing;Receivers;Gain;Transmitters;Throughput;Buffer storage","access protocols;codecs;digital subscriber lines;multiplexing;quality of service;radio transmitters","enhanced audio transmission;asymmetric digital subscriber line;ADSL;prioritised DMT modulation;prioritised DMT retransmissions;discrete multitone modulation;broadband communications technology;quality of service;QoS;MP1 codec;bit stream multiplexing;retransmission protocol;negative acknowledgement;NACK packet types;adaptive multiplexing;transmitter frames;unequal error protection;UEP;segmented signal to noise ratio;SSNR","","1","","11","","2 Nov 2015","","","IEEE","IEEE Conferences"
"An Audio Data Representation for Traffic Acoustic Scene Recognition","D. Jiang; D. Huang; Y. Song; K. Wu; H. Lu; Q. Liu; T. Zhou","Department of Computer Science, College of Engineering, Shantou University, Shantou, China; Department of Computer Science, College of Engineering, Shantou University, Shantou, China; Center for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong; Department of Computer Science, College of Engineering, Shantou University, Shantou, China; Department of Computer Science, College of Engineering, Shantou University, Shantou, China; Department of Computer Science, College of Engineering, Shantou University, Shantou, China; Department of Computer Science, College of Engineering, Shantou University, Shantou, China","IEEE Access","6 Oct 2020","2020","8","","177863","177873","Acoustic scene recognition (ASR), recognizing acoustic environments given an audio recording of the scene, has a wide range of applications, e.g. robotic navigation and audio forensic. However, ASR remains challenging mainly due to the difficulty of representing audio data. In this article, we focus on traffic acoustic data. Traffic acoustic sense recognition provides complementary information to visual information of the scene; for example, it can be used to verify the visual perception result. The acoustic analysis and recognition, in consideration of its simple and convenient, can effectively enhance the perception ability which only applies visual information. We propose an audio data representation method to improve the traffic acoustic scene recognition accuracy. The proposed method employs the constant Q transform (CQT) and histogram of gradient (HOG) to transfer the one-dimensional audio signals into a time-frequency representation. We also propose two data representation mechanisms, called global and local feature selections, in order to select features that are able to describe the shape of time-frequency structures. We finally exploit the least absolute shrinkage and selection operator (LASSO) technique to further improve the recognition accuracy, by further selecting the most representative information for the recognition. We implemented extensive experiments, and the results show that the proposed method is effective, significantly outperforming the state-of-the-art methods.","2169-3536","","10.1109/ACCESS.2020.3027474","NSFC(grant numbers:61902232,61902231); Natural Science Foundation of Guangdong Province(grant numbers:2018A030313291,2019A1515010943); Department of Education of Guangdong Province(grant numbers:2018GXJK048,2018KZDXM035,2017KCXTD015); Basic and Applied Basic Research of Colleges and Universities in Guangdong Province (Special Projects in Artificial Intelligence)(grant numbers:2019KZDZX1030); Shantou University (STU) Scientific Research Foundation for Talents(grant numbers:NTF18006); 2020 Li Ka Shing Foundation Cross-Disciplinary Research Grant(grant numbers:2020LKSFG05D,2020LKSFG04D); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207916","Feature extraction;acoustic scene recognition;transportation;acoustic material","Acoustics;Feature extraction;Spectrogram;Transforms;Histograms;Time-frequency analysis;Visualization","acoustic signal processing;audio recording;audio signal processing;feature extraction;mobile robots;time-frequency analysis;visual perception","HOG;histogram of gradient;CQT;constant Q transform;LASSO technique;global feature selections;representative information;local feature selections;data representation mechanisms;time-frequency representation;one-dimensional audio signals;traffic acoustic scene recognition accuracy;audio data representation method;perception ability;acoustic analysis;visual perception result;visual information;complementary information;traffic acoustic sense recognition;traffic acoustic data;robotic navigation;audio recording;acoustic environments;ASR","","1","","38","CCBY","28 Sep 2020","","","IEEE","IEEE Journals"
"Public Speaking Enhancement via Virtual Reality","Piyush","Department of Computer Science and Engineering, SRM University, Delhi-NCR, India","2018 4th International Conference on Computing Communication and Automation (ICCCA)","29 Jul 2019","2018","","","1","4","In this paper I proposed the one of a kind way to deal with the dread of speaking publically (Gloss fear) using Virtual Reality (VR) and Audio Signal preparing. The framework is planned on Unity3D (a Game Development Engine) and it utilizes the ideas and Algorithms of Digital Signal Processing to outline a Native Audio Plugin for Unity3D which can be utilized to analyze Audio Signals. The VR Simulator records a reference audio from the user and compares the user's execution with respect to the Audio input got from the user in a Virtual talking condition. The discourse/ speech correlation and coordinating of the two Audio Signals is finished using speaker verification algorithm using Spectrogram. Pause detection algorithm estimates level of familiarity with discourse/ speech. Additionally, eye contact of the user with group of onlookers is checked using Ray Caster in Unity3D. The mix of the outcomes abridged together creates a score which causes the user to enhance his/her aptitudes of public speaking.","2642-7354","978-1-5386-6947-1","10.1109/CCAA.2018.8777707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8777707","Audio Signals;VR Simulator;Pause Detection;Public Speaking;Speaker Verification;Spectrogram Speech;Virtual Reality;Unity3D","Spectrogram;Virtual reality;Smart phones;Signal processing algorithms;Correlation;Headphones;Earth","audio signal processing;computer games;ray tracing;speaker recognition;virtual reality","Unity3D;reference audio;pause detection algorithm;virtual reality;audio signal;game development engine;digital signal processing;VR simulator;audio input;virtual talking condition;native audio plugin;public speaking enhancement;discourse/speech correlation;speaker verification algorithm;spectrogram;raycaster","","","","19","","29 Jul 2019","","","IEEE","IEEE Conferences"
"Data collection for mobile audio-visual speech recognition in various environments","S. Tamura; T. Seko; S. Hayamizu","Dep. Electrical, Electronic and Computer Engineering, Gifu University, 1-1 Yanagido, 501-1193 Japan; Dep. Information Science, Graduate School of Gifu University, 1-1 Yanagido, Gifu, 501-1193 Japan; Dep. Electrical, Electronic and Computer Engineering, Gifu University, 1-1 Yanagido, 501-1193 Japan","2014 17th Oriental Chapter of the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)","2 Mar 2015","2014","","","1","6","This paper introduces our recent activities for audio-visual speech recognition on mobile devices and data collection in various environments. Audio-visual automatic speech recognition is effective in noisy or real conditions to enhance the robustness of speech recognizer and to improve the recognition accuracy. We have developed an audio-visual speech recognition interface for mobile devices. In order to evaluate the recognizer and investigate issues related to audio-visual processing on mobile computers, we collected speech data and lip images of 16 subjects in eight conditions, where there were various audio noises and visual difficulties. Audio-only speech recognition and visual-only lipreading were then conducted. Through these experiments, we found some issues and future works not only for construction of audio-visual database but also for robust audio-visual speech recognition.","","978-1-4799-7094-0","10.1109/ICSDA.2014.7051434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7051434","","Visualization;Hidden Markov models;Robustness;Noise;Quantization (signal)","audio-visual systems;mobile computing;speech recognition","data collection;mobile audio-visual speech recognition;mobile devices;audio-visual automatic speech recognition;audio-visual speech recognition interface;audio-visual processing;mobile computers;audio noises;visual difficulties;audio-only speech recognition;visual-only lipreading;audio-visual database","","2","","23","","2 Mar 2015","","","IEEE","IEEE Conferences"
"A Robust Audio Fingerprinting Using a New Hashing Method","H. -S. Son; S. -W. Byun; S. -P. Lee","Department of Computer Science, Graduate School, Sangmyung University, Seoul, South Korea; Department of Computer Science, Graduate School, Sangmyung University, Seoul, South Korea; Department of Electronic Engineering, Sangmyung University, Seoul, South Korea","IEEE Access","28 Sep 2020","2020","8","","172343","172351","To enhance the tracking performance of illegal audio copies, we introduce a robust audio fingerprinting method against various attacks in this paper. Most audio fingerprints consist of the information in the frequency band of audio. These fingerprinting methods may lose the uniqueness of the audio fingerprint by irregular movement such as an attack with pitch value changes. The proposed fingerprint method in a fundamental frequency band makes up for the weakness of existing methods generated from frequency domain. Using the geometrical property of the proposed method, a new hashing method is employed in the similarity calculation process to compare the audio contents. In order to prove the validity of proposed algorithm, we experiment for six environments such as tempo, pitch, speed modification, noise addition, low pass filter and high pass filter. The proposed method shows the highest level of performance in most experimental environments. Especially, with respect to the tempo, pitch, and speed manipulation experiments, the proposed method archives the precision rate of the range from 95 to 100% according to the degree of manipulation, which compares favorably with the precision rate obtained by traditional approaches, and yields a precision rate between 85 and 100% in noise addition and filtering experiments.","2169-3536","","10.1109/ACCESS.2020.3024951","2020 Research Grant from Sangmyung University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200471","Audio fingerprint;FFMAP;SAH;frequency band separation","Fingerprint recognition;Feature extraction;Robustness;Databases;Spectrogram;Time-frequency analysis","audio signal processing;high-pass filters;low-pass filters","audio contents;illegal audio copies;audio fingerprinting method;hashing method;similarity calculation process;low pass filter;high pass filter","","","","27","CCBY","18 Sep 2020","","","IEEE","IEEE Journals"
"Enhanced local feature approach for overlapping sound event recognition","J. Dennis; H. Dat Tran","Institute for Infocomm Research, A∗STAR, 1 Fusionopolis Way, Singapore 138632; Institute for Infocomm Research, A∗STAR, 1 Fusionopolis Way, Singapore 138632","Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific","16 Feb 2015","2014","","","1","4","In this paper, we propose a feature-based approach to address the challenging task of recognising overlapping sound events from single channel audio. Our approach is based on our previous work on Local Spectrogram Features (LSFs), where we combined a local spectral representation of the spectrogram with the Generalised Hough Transform (GHT) voting system for recognition. Here we propose to take the output from the GHT and use it as a feature for classification, and demonstrate that such an approach can improve upon the previous knowledge-based scoring system. Experiments are carried out on a challenging set of five overlapping sound events, with the addition of non-stationary background noise and volume change. The results show that the proposed system can achieve a detection rate of 99% and 91% in clean and 0dB noise conditions respectively, which is a strong improvement over our previous work.","","978-6-1636-1823-8","10.1109/APSIPA.2014.7041646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7041646","","Spectrogram;Training;Noise;Transforms;Speech;Feature extraction;Databases","audio signal processing;Hough transforms;signal classification","enhanced local feature based approach;overlapping sound event recognition;single channel audio;local spectrogram features;LSF;local spectral representation;generalised Hough transform voting system;GHT;nonstationary background noise","","1","","15","","16 Feb 2015","","","IEEE","IEEE Conferences"
"AXES-RESEARCH — A user-oriented tool for enhanced multimodal search and retrieval in audiovisual libraries","P. van der Kreeft; K. Macquarrie; M. Kemman; M. Kleppe; K. McGuinness","New Media/Innovation Department, Deutsche Welle, Bonn, Germany; New Media/Innovation Department, Deutsche Welle, Bonn, Germany; Erasmus School of History, Culture and Communication, Erasmus University Rotterdam, the Netherlands; Erasmus School of History, Culture and Communication, Erasmus University Rotterdam, the Netherlands; Insight Centre for Data Analytics, Dublin City University, Ireland","2014 12th International Workshop on Content-Based Multimedia Indexing (CBMI)","10 Jul 2014","2014","","","1","4","AXES, Access for Audiovisual Archives, is a research project developing tools for new engaging ways to interact with audiovisual libraries, integrating advanced audio and video analysis technologies. The presented prototype is targeted at academic researchers and journalists. The tool allows them to search and retrieve video segments through metadata, audio analysis, as well as visual concepts and similarity searches. Presented here is a user-based vision on the research-oriented tool provided by AXES.","1949-3991","978-1-4799-3990-9","10.1109/CBMI.2014.6849852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6849852","audiovisual archive;library;audio analysis;video analysis;similarity search;tool;information retrieval","Visualization;Prototypes;Educational institutions;Face;Streaming media;Search problems;Libraries","content-based retrieval;meta data;prototypes;video retrieval","AXES-RESEARCH;user-oriented tool;enhanced multimodal search;enhanced multimodal retrieval;audiovisual libraries;access for audiovisual archives;advanced audio-video analysis technologies;video segment search;video segment retrieval;metadata;visual similarity searches;visual concept searches;user-based vision;research-oriented tool","","2","","11","","10 Jul 2014","","","IEEE","IEEE Conferences"
"Robust Audio-visual Speech Recognition Using Bimodal Dfsmn with Multi-condition Training and Dropout Regularization","S. Zhang; M. Lei; B. Ma; L. Xie","Machine Intelligence Technology, Alibaba Group; Machine Intelligence Technology, Alibaba Group; Machine Intelligence Technology, Alibaba Group; School of Computer Science, Northwestern Polytechnical University","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","6570","6574","Audio-visual speech recognition (AVSR) is thought to be one of the potential solutions for robust speech recognition, especially in noisy environments. Compared to audio only speech recognition, the major issues of AVSR include the lack of publicly available audio-visual corpora and the need of robust knowledge fusion of both speech and vision. In this work, based on the recently released NTCD-TIMIT audio-visual corpus, we address the challenges of AVSR through three aspects: 1) optimal integration of acoustic and visual information; 2) robust performance with multi-condition training; 3) robust modeling against missing visual information during decoding. We propose a bimodal-DFSMN to jointly learn feature fusion and acoustic modeling, and utilize a per-frame dropout approach to enhance the robustness of AVSR system against the missing of visual modality. In the experiments, we construct two setups based on the NTCD-TIMIT corpus that consists of 5 hours clean training data and 150 hours multi-condition training data, respectively. As a result, we achieve a phone error rate of 12.6% on clean test set and an average phone error rate of 26.2% on all test sets (clean, various SNRs, various noise types), which both dramatically improve the baseline performance in NTCD-TIMIT task.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8682566","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682566","Audio-visual speech recognition;bimodal DF-SMN;robust speech recognition;dropout;multi-condition training","Visualization;Training;Speech recognition;Acoustics;Noise measurement;Decoding;Error analysis","audio-visual systems;speech recognition","AVSR system;visual modality;robust audio-visual speech recognition;dropout regularization;robust knowledge fusion;visual information;bimodal DFSMN;NTCD-TIMIT audio-visual corpus","","4","","28","","17 Apr 2019","","","IEEE","IEEE Conferences"
"Delay-less frequency domain packet-loss concealment for tonal audio signals","R. Sperschneider; J. Sukowski; G. Marković","Fraunhofer IIS, Erlangen, Germany; Fraunhofer IIS, Erlangen, Germany; Fraunhofer IIS, Erlangen, Germany","2015 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","25 Feb 2016","2015","","","766","770","A delay-less packet-loss concealment (PLC) method for stationary tonal signals is presented, that addresses audio codecs utilizing a modified discrete cosine transformation (MDCT). In the case of a frame loss, tonal components are detected using the last two received spectra and their accompanied pitch information. Phases of the tonal components are subsequently predicted, aiming for a continuous phase evolution between successive frames. Thus, accurate estimates of tonal components are achieved for stationary signals during frame losses, leading to an improved listening impression. This method is used in the recently standardized enhanced voice services (EVS) codec. Subjective quality evaluation using a MUSHRA test demonstrates its performance.","","978-1-4799-7591-4","10.1109/GlobalSIP.2015.7418300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7418300","Enhanced Voice Service (EVS);Packet-Loss Concealment (PLC);Modified Discrete Cosine Transform (MDCT);Phase Prediction","Decoding;Codecs;Mathematical model;Power harmonic filters;Adaptive filters;Information filters","audio signal processing;discrete cosine transforms","delay-less frequency domain packet-loss concealment;tonal audio signals;PLC method;audio codecs;modified discrete cosine transformation;MDCT;continuous phase evolution;improved listening impression;standardized enhanced voice services codec;MUSHRA test","","","","27","","25 Feb 2016","","","IEEE","IEEE Conferences"
"Enhancing MP3 encoding by utilizing a predictive Complex-Valued Neural Network","A. Y. H. Al-Nuaimi; M. Faijul Amin; K. Murase","Department of System Design Engineering, University of Fukui, Japan; Department of System Design Engineering, University of Fukui, Japan; Department of System Design Engineering, University of Fukui, Japan","The 2012 International Joint Conference on Neural Networks (IJCNN)","30 Jul 2012","2012","","","1","6","We design a Complex-Valued Neural Network (CVNN) used for audio prediction. This CVNN is inserted into an MP3 encoder to increase the compression ratio of audio data. Experiments show that CVNNs can increase the compression capabilities of the encoder, and that CVNN is more suitable than Real Valued Neural Networks for audio prediction.","2161-4407","978-1-4673-1490-9","10.1109/IJCNN.2012.6252535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6252535","CVNN;MP3 encoding;predictive encoding;audio","Digital audio players;Encoding;Neural networks;Discrete cosine transforms;Bit rate;Transform coding;Quantization","audio coding;data compression;neural nets","MP3 encoding;predictive complex-valued neural network;audio prediction;CVNN;audio data compression ratio;real valued neural networks","","2","","11","","30 Jul 2012","","","IEEE","IEEE Conferences"
"Browsing videos by automatically detected audio events","V. Barbosa; T. Pellegrini; M. Bugalho; I. Trancoso","IST/UTL, Avenida Rovisco Pais, 1, Lisboa, 1049-001; INESC-ID Rua Alves Redol, 9, Lisboa, 1000-029; INESC-ID Rua Alves Redol, 9, Lisboa, 1000-029; IST/UTL, Avenida Rovisco Pais, 1, Lisboa, 1049-001","2011 IEEE EUROCON - International Conference on Computer as a Tool","23 Jun 2011","2011","","","1","4","This paper focuses on Audio Event Detection (AED), a research area which aims to substantially enhance the access to audio in multimedia content. With the ever-growing quantity of multimedia documents uploaded on the Web, automatic description of the audio content of videos can provide very useful information, to index, archive and search multimedia documents. Preliminary experiments with a sound effects corpus showed good results for training models. However, the performance on the real data test set, where there are overlapping audio events and continuous background noise is lower. This paper describes the AED framework and methodologies used to build 6 Audio Event detectors, based on statistical machine learning tools (Support Vector Machines). The detectors showed some promising improvements achieved by adding background noises to the training data, comprised of clean sound effects that are quite different from the real audio events in real life videos and movies. A graphical interface prototype is also presented, that allows browsing a movie by its content and provides an audio event description with time codes.","","978-1-4244-7487-5","10.1109/EUROCON.2011.5929358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929358","","Videos;Motion pictures;Noise measurement;Event detection;Feature extraction;Detectors;Speech","audio signal processing;cinematography;multimedia communication;statistical analysis;support vector machines;video retrieval;video signal processing","video browsing;audio event detection;audio access;multimedia content;World Wide Web;video audio content;multimedia document index;multimedia document archive;multimedia document search;sound effect corpus;overlapping audio event;continuous background noise;AED framework;statistical machine learning tool;support vector machine;clean sound effect;real audio event;real life video;real life movies;graphical interface prototype;movie browsing;audio event description;time code","","","","10","","23 Jun 2011","","","IEEE","IEEE Conferences"
"QoE enhancement by capacity allocation and piggyback bandwidth request in audio-video IP transmission over the IEEE 802.16 BE service","T. Nunome; S. Tasaka","Department of Computer Science and Engineering, Graduate School of Engineering, Nagoya Institute of Technology, Nagoya 466-8555, Japan; Department of Computer Science and Engineering, Graduate School of Engineering, Nagoya Institute of Technology, Nagoya 466-8555, Japan","2012 IEEE Wireless Communications and Networking Conference (WCNC)","11 Jun 2012","2012","","","1245","1250","This paper studies QoE (Quality of Experience) enhancement of audio-video IP transmission over the uplink channel, i.e., from subscriber stations (SSs) to the base station (BS), in the IEEE 802.16 BE service. We assume two types of capacity allocation schemes for uplink and downlink burst durations: static and adaptive. Furthermore, we introduce a piggyback request mechanism for uplink bandwidth requests from SSs to the BS in addition to a random access-based request mechanism. We assess QoE of audio-video streams for four schemes obtained from the combination of the bandwidth request mechanisms and the capacity allocation schemes. We also employ two types of audio-video contents. From the assessment result, we notice that the piggyback request mechanism can enhance QoE of audio-video transmission. In addition, the adaptive allocation scheme is effective for QoE enhancement particularly under heavily loaded conditions because of its efficient usage of OFDM symbols. We also find that the effects of piggyback request mechanism and capacity allocation schemes on QoE change according to the content types.","1558-2612","978-1-4673-0437-5","10.1109/WCNC.2012.6213968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6213968","IEEE 802.16;WiMAX;Audio-Video streaming;QoE;QoS","Resource management;Bandwidth;Downlink;Streaming media;Quality of service;IEEE 802.16 Standards;OFDM","audio streaming;IP networks;OFDM modulation;video streaming","QoE enhancement;capacity allocation;piggyback bandwidth request;audio-video IP transmission;IEEE 802.16 BE service;quality of experience enhancement;subscriber stations;downlink burst;uplink burst;piggyback request mechanism;random access-based request mechanism;audio-video streams QoE;bandwidth request mechanisms;OFDM symbols","","1","","17","","11 Jun 2012","","","IEEE","IEEE Conferences"
"18.1 A -105dBc THD+N (-114dBc HD2) at 2.8V<inf>PP</inf> Swing and 120dB DR Audio Decoder with Sample-and-Hold Noise Filtering and Poly Resistor Linearization Schemes","S. Wen; K. Chen; C. Hsiao; Y. Chen","MediaTek, Hsinchu, Taiwan; MediaTek, Hsinchu, Taiwan; MediaTek, Hsinchu, Taiwan; MediaTek, Hsinchu, Taiwan","2019 IEEE International Solid- State Circuits Conference - (ISSCC)","7 Mar 2019","2019","","","294","295","Three major design issues that arise for high-fidelity audio decoders are: 1) DAC reference noise limiting achievable SNR [1], [2]; 2) THD+N degradation at large output swing [3], [4]; and 3) Distortion arising from limited amplifier loop gain as a consequence of high output load capacitance (C<sub>L</sub>) [5]. In the first issue, reference noise along with individual DAC cell noise generally limits SNR for a full-scale signal. The use of large device sizes [1], source degeneration [2] and chopping can mitigate 1/f noise, but none are effective for reducing thermal noise. Consequently, either more power or an external bypass capacitor for noise filtering is necessary for reducing DAC reference noise. In the second issue, THD+N of high-output-swing amplifiers degrades proportionally as the output swing increases above 1.6V<sub>PP</sub>, even with a 4.5V supply [3], [4]. The primary cause of this severe 2<sup>nd</sup>-order harmonic distortion (HD2) is due to the depletion effect of poly resistors [6]. Lastly, for adequate stability margin, the UGB and loop gain of the conventional nested Miller compensation (NMC) amplifier is restricted by an output limiting pole (ω<sub>limit</sub>) and C<sub>L</sub>. In [5], a frequency compensation scheme is proposed to push the UGB close to ω<sub>limit</sub> and enhance the loop gain over the audio band (20Hz to 20kHz) while handling a C<sub>L</sub> up to 10nF. However, with a C<sub>L</sub> of 22nF, the amplifier begins to ring for a transient step. In this work, three solutions are presented to solve the aforementioned issues: 1) an area- and power-efficient sample-and-hold (S&H) noise filtering technique is introduced to shape the 1/f and thermal noise of the reference to frequencies below the audio band, thus greatly improving SNR for a full-scale signal; 2) a poly resistor linearization scheme is presented to improve HD2 by mitigating the depletion effect of resistors; and 3) a frequency compensation method for multistage amplifiers is introduced that boosts loop gain and thus enhances amplifier linearity without being limited by large C<sub>L</sub>. Combining these techniques, the decoder and amplifier achieve -105dBc THD+N (-114dBc HD2) and 120dB DR, and can support a C<sub>L</sub> up to 22nF.","2376-8606","978-1-5386-8531-0","10.1109/ISSCC.2019.8662456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8662456","","Resistors;Decoding;Damping;Signal to noise ratio;Capacitance;Thermal noise;Integrated circuit modeling","1/f noise;amplifiers;audio coding;capacitors;decoding;digital-analogue conversion;filtering theory;harmonic distortion;resistors;sample and hold circuits;thermal noise","PP swing;high-fidelity audio decoders;THD+N degradation;amplifier loop gain;external bypass capacitor;noise filtering;DAC reference noise;high-output-swing amplifiers degrades;HD2;depletion effect;frequency compensation scheme;power-efficient sample;frequency compensation method;multistage amplifiers;SNR;load capacitance;source degeneration;thermal noise reduction;2nd-order harmonic distortion;nested Miller compensation amplifier;DAC cell noise;polyresistor linearization scheme;1/f noise chopping;UGB;NMC amplifier;sample-and-hold noise filterin;voltage 4.5 V;capacitance 22.0 nF;frequency 20.0 Hz to 20.0 kHz;noise figure 120.0 dB;current 18.1 A;voltage 2.8 V","","","","8","","7 Mar 2019","","","IEEE","IEEE Conferences"
"Research on digital audio watermark technology for broadcast information security","X. Wang; X. Wang; S. Liu","College of Information Engineering, Communication University of China, Beijing, China; College of Information Engineering, Communication University of China, Beijing, China; Graduate school","2012 International Conference on Image Analysis and Signal Processing","31 Jan 2013","2012","","","1","6","This paper introduces the technology of spread-spectrum watermarking of audio signals. We adopt the direct-sequence spread-spectrum watermark, which is based on rate concept, making use of chess watermark to shape watermark signals. After making audio signals MCLT transform, according to the Psycho-Acoustic Frequency Masking (PAFM), we use multiplication to add the watermark to the MCLT transform domain of audio signals. The added location is determined by pre-detection technology. In the watermark detection side, we use the cepstrum filter to reduce the variance of the correlation test to improve the watermark detection reliability. This paper proposes to change both the spread factor and the repeated factor to reduce window size and enhance watermark capacity considering the robustness, imperceptibility, and watermark capacity. The system has certain practical and referenced value, and is a useful exploration for the utility of broadcast monitoring technology in the future, which is with inspiration and reference.","2156-0129","978-1-4673-2546-2","10.1109/IASP.2012.6425030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6425030","digital imaae watermarkinz;DCT transformation;DC coefficients;dual transformation","Watermarking;Transforms;Noise;Resists;Error analysis;Monitoring;Encoding","audio coding;audio watermarking;digital audio broadcasting;spread spectrum communication;transforms","digital audio watermark technology;broadcast information security;audio signal;direct-sequence spread-spectrum watermarking;rate concept;chess watermark;MCLT transform;psycho-acoustic frequency masking;PAFM;predetection technology;cepstrum filter;correlation test;watermark detection reliability;spread factor;repeated factor;window size;watermark capacity;broadcast monitoring technology","","","","15","","31 Jan 2013","","","IEEE","IEEE Conferences"
