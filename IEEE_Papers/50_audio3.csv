"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Enhanced Steganographic Method Preserving Base Quality of Information Using LSB, Parity and Spread Spectrum Technique","R. Kaur; A. Thakur; H. S. Saini; R. Kumar","Indo Global Coll. of Eng., Mohali, India; Indo Global Coll. of Eng., Mohali, India; Indo Global Coll. of Eng., Mohali, India; Indo Global Coll. of Eng., Mohali, India","2015 Fifth International Conference on Advanced Computing & Communication Technologies","6 Apr 2015","2015","","","148","152","In order to protect the data on internet from hackers, it is necessary to secure systems to send data safely. For having this security, audio steganography, using multilevel simple embedding technique is used. The main concept in our approach is to use three methods of audio steganography on a single audio file instead of using single method. Basic approach of this paper is to hide three messages in a single audio file. By using this technique host message before steganography and after steganography remains the same. In this paper, we calculate capacity and bit error rate (BER) at different bit indices and we improve bit error rate by applying multilevel technique. Average value of BER of previous technique (i.e. audio steganography using bit modification) is 0.7645 and BER of proposed technique is 0.6354, which is less than the previous one.","2327-0659","978-1-4799-8488-6","10.1109/ACCT.2015.139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7079069","Steganography; Audio steganography; Multilevel technique; Bit error rate (BER); Capacity.","Bit error rate;Encoding;Indexes;Electronic mail;MATLAB;Security;Bandwidth","error statistics;steganography","enhanced steganographic method preserving base quality;LSB;parity;spread spectrum technique;Internet;hackers;secure systems;audio steganography;multilevel simple embedding technique;single audio file;single method;technique host message;bit error rate;BER;bit indices","","2","","13","","6 Apr 2015","","","IEEE","IEEE Conferences"
"Enhancing audio surveillance with hierarchical recurrent neural networks","F. Colangelo; F. Battisti; M. Carli; A. Neri; F. Calabró","Universitá degli Studi Roma Tre, Rome, Italy; Universitá degli Studi Roma Tre, Rome, Italy; Universitá degli Studi Roma Tre, Rome, Italy; Universitá degli Studi Roma Tre, Rome, Italy; Leonardo Finmeccanica, Rome, Italy","2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)","23 Oct 2017","2017","","","1","6","The need for effective and reliable surveillance techniques is getting nowadays more and more of primary importance, especially in the actual scenario in which safety and security have become a priority. While classical techniques rely on video-based surveillance systems, such as Close-Circuit television, many studies show that also the audio signal can be effectively used for these purposes. There are many characteristics that make the audio signal particularly suited for this task and, above all, the fact that the analysis of the audio signal can greatly improve thanks to the introduction of automatic classification. Recently, a large focus has been on the use of Deep Neural Networks for classifying audio data and, in this work, we aim to test their performance in the audio surveillance field. In this contribution we propose an algorithm for audio events detection in noisy environments based on the use of deep recurrent neural network. The achieved results show satisfactory and improved performances with respect to state-of-the-art techniques.","","978-1-5386-2939-0","10.1109/AVSS.2017.8078496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8078496","","Computer architecture;Feature extraction;Surveillance;Training;Microprocessors;Noise measurement;Logic gates","audio signal processing;recurrent neural nets;signal classification;signal detection;video surveillance","hierarchical recurrent neural networks;effective surveillance techniques;reliable surveillance techniques;safety;security;classical techniques;video-based surveillance systems;audio signal;audio surveillance field;audio events detection;deep recurrent neural network;audio surveillance enhancement;automatic classification;audio data classification;noisy environments","","3","","21","","23 Oct 2017","","","IEEE","IEEE Conferences"
"A QR-code based audio watermarking technique for tracing traitors","F. Chaabane; M. Charfeddine; W. Puech; C. Ben Amaf","REGIM-Lab.: REsearch Groups in Intelligent Machines, University of Sfax, ENIS, BP 1173, Sfax, 3038, Tunisia; REGIM-Lab.: REsearch Groups in Intelligent Machines, University of Sfax, ENIS, BP 1173, Sfax, 3038, Tunisia; LIRMM Laboratory, UMR 5506 CNRS, University of Montpellier II, 161, Rue Ada, 34392, Montpellier Cedex 05, France; REGIM-Lab.: REsearch Groups in Intelligent Machines, University of Sfax, ENIS, BP 1173, Sfax, 3038, Tunisia","2015 23rd European Signal Processing Conference (EUSIPCO)","28 Dec 2015","2015","","","51","55","Handling a great number of users and surviving different types of attacks present fundamental challenges of the majority fingerprinting systems in the tracing traitor field. In this paper, the proposed technique consists in embedding a fingerprint, a QR code in the audio stream extracted from the media release. Using the QR-code provides several advantages as supporting a large amount of information in a compact format end damage resiliency. This paper proposes to encode the identifier which is a parallel concatenation of two tracing codes: Boneh Shaw and Tardos codes into QR-code. The proposed approach should not only improve the two-stage tracing strategy by reducing the complexity computation, but also enhance the secure side of the proposed technique by the preprocessing treatment before generating the QR-code.","2076-1465","978-0-9928-6263-3","10.1109/EUSIPCO.2015.7362343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362343","QR-code;tracing traitors;Boneh Shaw;Tardos;preprocessing","Watermarking;Robustness;Media;Europe;Signal processing;Multimedia communication;Streaming media","audio streaming;audio watermarking;computational complexity;QR codes","Quick Response codes;QR code;audio watermarking technique;fingerprinting systems;tracing traitor field;audio stream;tracing codes;Boneh Shaw codes;Tardos codes;two-stage tracing strategy;complexity computation;preprocessing treatment","","5","","11","","28 Dec 2015","","","IEEE","IEEE Conferences"
"Secure and robust high quality DWT domain audio watermarking algorithm with binary image","A. R. Elshazly; M. M. Fouad; M. E. Nasr","Electronics and Electrical Communication Dept., Zagazig University, Zagazig, Egypt; Electronics and Electrical Communication Dept., Zagazig University, Zagazig, Egypt; Electronics and Electrical Communication Dept., Tanta University, Tanta, Egypt","2012 Seventh International Conference on Computer Engineering & Systems (ICCES)","10 Jan 2013","2012","","","207","212","To enhance security and robustness of digital audio watermarking algorithms, this paper presents an algorithm based on mean-quantization in Discrete Wavelet Transform (DWT) domain. A binary image is used as a watermark, and is encrypted with chaotic encryption with secret key. This approach is based on the embedding of an encrypted watermark in the low frequency components using a two wavelet functions with adaptation to the frame size. The reason for embedding the watermark in the low frequency components is that these components' energy is high enough to embed the watermark in such a way that the watermark is inaudible; therefore, it should not alter the audible content and should not be easy to remove. The algorithm has a good security because only the authorized can detect the copyright information embedded to the host audio signal. The watermark can be blindly extracted without knowledge of the original signal. To evaluate the performance of the presented audio watermarking method, objective quality tests including bit error rate (BER), normalized cross correlation(NCC), peak-signal to noise ratio (PSNR) are conducted for the watermark and Signal-to-Noise Ratio(SNR) for audio signals. The tests' results show that the approach maintains high audio quality, and yields a high recovery rate after attacks by commonly used audio data manipulations such as noise addition, amplitude modification, low-pass filtering, re-quantization, re-sampling, cropping, cutting, and compression. Simulation results show that our approach not only makes sure robustness against common attacks, but it also further improves systemic security and robustness against malicious attack.","","978-1-4673-2961-3","10.1109/ICCES.2012.6408514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6408514","Audio watermarking;Binary image;normalized cross correlation;Robustness;Security component;formatting;style;styling;insert","Watermarking;Bit error rate;Robustness;Discrete wavelet transforms;PSNR","audio watermarking;copyright;cryptography;discrete wavelet transforms;error statistics;image watermarking","robust high quality DWT domain audio watermarking algorithm;binary image;digital audio watermarking algorithms;mean-quantization;discrete wavelet transform domain;DWT domain;chaotic encryption;secret key;low-frequency components;wavelet functions;frequency components;copyright information;host audio signal;bit error rate;BER;normalized cross correlation;NCC;peak-signal to noise ratio;PSNR;audio data manipulations;low-pass filtering;amplitude modification;malicious attack","","13","","19","","10 Jan 2013","","","IEEE","IEEE Conferences"
"An approach for enhancing message security in audio steganography","A. K. Mandal; M. Kaosar; M. O. Islam; M. D. Hossain","Dept. of Computer Engineering, Hajee Md. Danesh Science and Technology University, Dinajpur, Bangladesh; School of Computing and Mathematics, Charles Sturt University, Bathurst, NSW, Australia; Dept. of Telecommunication and Electronic Engineering, Hajee Md. Danesh Science and Technology University, Dinajpur, Bangladesh; Dept. of Computer Science and Information Technology, Hajee Md. Danesh Science and Technology University, Dinajpur, Bangladesh","16th Int'l Conf. Computer and Information Technology","29 Dec 2014","2014","","","383","388","Concealing a message and ensuring its security is inevitable in data transmission. Among various concepts, one approach is steganography that encodes secret message in indiscernible way. In this paper, we present an audio steganographic technique and propose a novel approach to hide data in the least significant bit (LSB) of the stereo-audio samples with CD-quality. Here, on the basis of stego-key and its parity, message bits are encoded into cover audio samples. In terms of security and imperceptibility, this method is a significant improvement of LSB method for hiding information in audio.","","978-1-4799-3497-3","10.1109/ICCITechn.2014.6997310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6997310","least significant bit;audio steganography;stego-key;parity bit;message security","Silicon;Encoding;Bismuth;Security;Signal to noise ratio;Decoding;Receivers","audio coding;steganography","audio steganography;message security enhancement;data transmission;least significant bit;stereo-audio samples;information hiding","","1","","15","","29 Dec 2014","","","IEEE","IEEE Conferences"
"Enhancing the Labelling of Audio Samples for Automatic Instrument Classification Based on Neural Networks","G. Castel-Branco; G. Falcao; F. Perdigão","University of Coimbra,Instituto de Telecomunicações,Portugal; University of Coimbra,Instituto de Telecomunicações,Portugal; University of Coimbra,Instituto de Telecomunicações,Portugal","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","1583","1587","The polyphonic OpenMIC-2018 dataset is based on weak and incomplete labels. The automatic classification of sound events, based on the VGGish bottleneck layer as proposed before by the AudioSet, implies the classification of only one second at a time, making it hard to find the label of that exact moment. To answer this question, this paper proposes PureMic, a new strongly labelled dataset (SLD) that isolates 1000 single instrument clips manually labelled. Moreover, the proposed model classifies clips over time and also enhances the labelling robustness of a high number of unlabelled samples in OpenMIC-2018 due to its ability of classification over time. In the paper we disambiguate and report the automatic labelling of previously unlabelled samples. Our proposed new labels achieves a mean average precision (mAP) of 0.701 for OpenMIC test data, outperforming its baseline (0.66). We released our code online in order to follow the proposed implementation <sup>1</sup>.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9053625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053625","OpenMIC-2018;AudioSet;PureMic;Instrument classification;Instrument labelling;Deep learning","Instruments;Music;Signal processing;Superluminescent diodes;Robustness;Labeling;Speech processing","acoustic signal processing;audio signal processing;neural nets;signal classification","OpenMIC test data;audio samples;automatic instrument classification;neural networks;polyphonic OpenMIC-2018 dataset;weak labels;incomplete labels;sound events;VGGish bottleneck layer;strongly labelled dataset;labelling robustness;unlabelled samples;automatic labelling;mean average precision","","1","","18","","9 Apr 2020","","","IEEE","IEEE Conferences"
"Research on dynamic range control used to audio directional system","J. Liang; S. Gao; Y. Li","School of Automotive Engineering, Dalian University of Technology, Dalian 116024, China; School of Automotive Engineering, Dalian University of Technology, Dalian 116024, China; College of materials Science and Engineering, Jilin University, Changchun 130022, China","2011 International Conference on Mechatronic Science, Electric Engineering and Computer (MEC)","22 Sep 2011","2011","","","498","501","The algorithm of the dynamic range control (DRC) was applied in the research of the audio directional system. According to the feature of audio directional system, the implementation of the static curve was modified, and the model of DRC based on simulink was built. The calculation results demonstrate that DRC is an effective optimal method to enhance the main sound and reduce the noise in the audio directional system.","","978-1-61284-722-1","10.1109/MEC.2011.6025511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6025511","Audio directional system;Dynamic range control;simulink","Dynamic range;Noise;Heuristic algorithms;Acoustics;Signal processing algorithms;Algorithm design and analysis;Field programmable gate arrays","audio systems","dynamic range control;audio directional system;static curve;optimal method","","2","","5","","22 Sep 2011","","","IEEE","IEEE Conferences"
"A Dual-Channel Time-Spread Echo Method for Audio Watermarking","Y. Xiang; I. Natgunanathan; D. Peng; W. Zhou; S. Yu","School of Information Technology, Deakin University, Melbourne, Australia; School of Engineering, Deakin University, Geelong, Australia; Machine Intelligence Laboratory, College of Computer Science, Sichuan University, Chengdu, China; School of Information Technology, Deakin University, Melbourne, Australia; School of Information Technology, Deakin University, Melbourne, Australia","IEEE Transactions on Information Forensics and Security","12 Mar 2012","2012","7","2","383","392","This work proposes a novel dual-channel time-spread echo method for audio watermarking, aiming to improve robustness and perceptual quality. At the embedding stage, the host audio signal is divided into two subsignals, which are considered to be signals obtained from two virtual audio channels. The watermarks are implanted into the two subsignals simultaneously. Then the subsignals embedded with watermarks are combined to form the watermarked signal. At the decoding stage, the watermarked signal is split up into two watermarked subsignals. The similarity of the cepstra corresponding to the watermarked subsignals is exploited to extract the embedded watermarks. Moreover, if a properly designed colored pseudonoise sequence is used, the large peaks of its auto-correlation function can be utilized to further enhance the performance of watermark extraction. Compared with the existing time-spread echo-based schemes, the proposed method is more robust to attacks and has higher imperceptibility. The effectiveness of our method is demonstrated by simulation results.","1556-6021","","10.1109/TIFS.2011.2173678","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061955","Audio watermarking;autocorrelation;colored pseudonoise (PN) sequence;echo hiding;echo kernel;time-spread echo","Watermarking;Robustness;Decoding;Educational institutions;Electronic mail;Media","audio coding;audio watermarking;decoding","dual-channel time-spread echo method;audio watermarking;perceptual quality;embedding stage;host audio signal;virtual audio channels;decoding stage;watermarked subsignals;embedded watermark extraction;colored pseudonoise sequence;autocorrelation function;imperceptibility","","73","","33","","26 Oct 2011","","","IEEE","IEEE Journals"
"MDCT audio coding with pulse vector quantizers","J. Svedberg; V. Grancharov; S. Sverrisson; E. Norvell; T. Toftgård; H. Pobloth; S. Bruhn","SMN, Ericsson Research, Ericsson AB, 164 80, Stockholm, Sweden; SMN, Ericsson Research, Ericsson AB, 164 80, Stockholm, Sweden; SMN, Ericsson Research, Ericsson AB, 164 80, Stockholm, Sweden; SMN, Ericsson Research, Ericsson AB, 164 80, Stockholm, Sweden; SMN, Ericsson Research, Ericsson AB, 164 80, Stockholm, Sweden; SMN, Ericsson Research, Ericsson AB, 164 80, Stockholm, Sweden; SMN, Ericsson Research, Ericsson AB, 164 80, Stockholm, Sweden","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","6 Aug 2015","2015","","","5937","5941","This paper describes a novel audio coding algorithm that is a building block in the recently standardized 3GPP EVS codec [1]. The presented scheme operates in the Modified Discrete Cosine Transform (MDCT) domain and deploys a Split-PVQ pulse coding quantizer, a noise-fill, and a gain control optimized for the quantizer's properties. A complexity analysis in terms of WMOPS is presented to illustrate that the proposed Split-PVQ concept and dynamic range optimized MPVQ-indexing are suitable for real-time audio coding. Test results from formal MOS subjective evaluations and objective performance figures are presented to illustrate the competitiveness of the proposed algorithm.","2379-190X","978-1-4673-6997-8","10.1109/ICASSP.2015.7179111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7179111","Audio Coding;MDCT;PVQ;Noise-Fill;EVS","Codecs;Complexity theory;Shape;Bit rate;Audio coding;Lattices","3G mobile communication;audio coding;discrete cosine transforms;vector quantisation;voice communication","modified discrete cosine transform;MDCT audio coding;pulse vector quantizers;audio coding algorithm;enhanced voice services;3GPP EVS codec;split-PVQ pulse coding quantizer;WMOPS;MPVQ-indexing","","3","2","20","","6 Aug 2015","","","IEEE","IEEE Conferences"
"Matrix Pencil Method applied to the compression of audio data in naval operations","D. Chaparro-Arce; A. Gallego; F. Albarracin-Vargas; C. Gutierrez; F. Vega; C. Pedraza","Universidad Nacional de Colombia Bogotá,Colombia; Universidad Nacional de Colombia Bogotá,Colombia; Universidad Nacional de Colombia Bogotá,Colombia; Escuela Naval de Cadetes ""Almirante Padilla"",Cartagena,Colombia; Universidad Nacional de Colombia Bogotá,Colombia; Universidad Nacional de Colombia Bogotá,Colombia","2020 IEEE International Conference on Computational Electromagnetics (ICCEM)","12 Oct 2020","2020","","","254","256","In this paper, a technique for compressing data from a time-domain audio digital signal, using the Matrix Pencil Method (MPM) is presented. The results show that the selection of critical parameters like time window and sampling frequency can effectively work alongside MPM, to reduce an audio signal data to a set of residues and poles, and have an accurate reconstruction of the vector related to the original audio. The compression process is focused on acoustic signals coming from boat engines and the reconstruction made for these poles and residues provides an R<sup>2</sup> correlation coefficient near to 100%. Besides, is possible to obtain a rate of compression between 80 and 93%. The audio signals are collected by hydrophones deployed in a coastal area, close to the shore. The signal under process is expected to present frequency components up to 1KHz. The data reduction enhances the transfer time and the bandwidth of the wireless communication channel under maritime conditions. Also, the proposed system considers the case of multiple sensor locations, acquiring a signal from multiple points in the area of influence allowing higher-level processes as the identification and tracking of the observed boat in naval operations.","","978-1-7281-3448-2","10.1109/ICCEM47450.2020.9219350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9219350","Matrix Pencil;Acoustic signals;frequency;poles;compression;reconstruction","","acoustic signal processing;audio coding;data compression;matrix algebra;signal reconstruction","audio signal data;compression process;acoustic signals;boat engines;correlation coefficient;audio signals;data reduction;transfer time;naval operations;time-domain audio digital signal;matrix pencil method;MPM;time window;sampling frequency;wireless communication channel","","","","5","","12 Oct 2020","","","IEEE","IEEE Conferences"
"VidTIMIT audio visual phoneme recognition using AAM visual features and human auditory motivated acoustic wavelet features","A. Biswas; P. K. Sahu; A. Bhowmick; M. Chandra","Dept of Electrical Engineering, National Institute of Technology, Rourkela, India-769008; Dept of Electrical Engineering, National Institute of Technology, Rourkela, India-769008; Dept of Electronics and Communication Engg., Birla Institute of Technology, Mesra, India-835215; Dept of Electronics and Communication Engg., Birla Institute of Technology, Mesra, India-835215","2015 IEEE 2nd International Conference on Recent Trends in Information Systems (ReTIS)","3 Sep 2015","2015","","","428","433","This paper presents an audio visual phoneme recognition system using the shape and appearance information extracted from jaw and lip region to enhance the robustness in noisy environment. Consideration of visual features along with traditional acoustic features have been found to be promising in complex auditory environment. Visual modality can provide complementary information to the speech recognizer when the audio modality is badly affected by background noise. Acoustic modality is represented by auditory based equivalent rectangular bandwidth (ERB) like wavelet features (WERBC) features, whereas visual modality is represented by statistically powerful active appearance model (AAM) based features. Audio and visual modalities are fused by using a proportional weighting factor to form the two stream audio visual synchronous Hidden Markov Model (SHMM) recognizer. The VidTIMIT database is chosen to study the performance of multi-modal phoneme recognition system. Artificial noises are injected to audio files at different SNR levels (0dB-20dB) to study the performance of system in noisy environment. Combination of WERBC and AAM features outperform the well known traditional combination of Mel scale cepstrum coefficients (MFCC) acoustic features and discrete cosine transform (DCT) visual features.","","978-1-4799-8349-0","10.1109/ReTIS.2015.7232917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7232917","Audio visual phoneme recognition;WERBC;Vid-TIMIT Corpus;AAM;HCI","Visualization;Active appearance model;Shape;Acoustics;Feature extraction;Hidden Markov models;Speech","audio signal processing;hidden Markov models;speech recognition","VidTIMIT audio visual phoneme recognition;AAM visual features;human auditory motivated acoustic wavelet features;audio visual phoneme recognition system;visual modality;speech recognizer;audio modality;auditory based equivalent rectangular bandwidth;active appearance model based features;AAM based features;proportional weighting factor;stream audio visual synchronous hidden Markov model;SHMM recognizer;VidTIMIT database;multimodal phoneme recognition system;artificial noises;WERBC feature;AAM feature","","3","","15","","3 Sep 2015","","","IEEE","IEEE Conferences"
"Non-Linear-Echo Based Anti-Collusion Mechanism for Audio Signals","T. Zong; Y. Xiang; I. Natgunanathan; L. Gao; G. Hua; W. Zhou","School of Information Technology, Deakin University, Victoria, Australia; School of Information Technology, Deakin University, Victoria, Australia; School of Information Technology, Deakin University, Victoria, Australia; School of Information Technology, Deakin University, Victoria, Australia; School of Electronic Engineering, Wuhan University, Wuhan, China; School of Software, University of Technology Sydney, Ultimo, NSW, Australia","IEEE/ACM Transactions on Audio, Speech, and Language Processing","2 Mar 2021","2021","29","","969","984","Collusion attacks are considered to be challenging attacks in audio copyright protection. The traditional watermarking algorithms cannot identify the traitors when other attacks, such as desynchronization attacks, are applied with a collusion attack. Instead of tracing the traitors, in this paper we aim to tackle collusion attacks by removing the commercial value from the colluded copy, which will demotivate the attackers from launching collusion attacks. Since the commercial value of an audio signal is directly reflected by its perceptual quality, we propose a novel non-linear-echo generation (NLEG) based algorithm to significantly degrade the perceptual quality of the colluded copy by embedding a time delay sequence into the host signal. The proposed NLEG is also designed to be resilient to common signal processing attacks and desynchronization attacks. Furthermore, the proposed NLEG can be combined with other digital watermarking techniques to enhance its performance on protecting the copyright information. Experimental results show the validity of the proposed NLEG.","2329-9304","","10.1109/TASLP.2021.3058892","Australian Research Council(grant numbers:LP170100458); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354555","Audio signal processing;collusion attacks;desynchronization attacks;non-linear-echo;perceptual quality degradation","Delay effects;Watermarking;Signal processing algorithms;Resists;Speech processing;Resistance;Cutoff frequency","audio signal processing;copyright;watermarking","audio signal;collusion attack;challenging attacks;audio copyright protection;desynchronization attacks;novel nonlinear-echo generation based algorithm;common signal processing attacks;nonlinear-echo based anticollusion mechanism;traditional watermarking algorithms;colluded copy;NLEG based algorithm;time delay sequence;host signal;signal processing attacks","","","","36","IEEE","15 Feb 2021","","","IEEE","IEEE Journals"
"A 16-bit low-power double-sampled delta sigma modulator for audio applications","Y. Wang; H. Wang; F. Lai; B. Cao; Y. Liu; X. Liu","Micro-electronic department, Harbin Institute of Technology, Harbin 150006, China; Micro-electronic department, Harbin Institute of Technology, Harbin 150006, China; Micro-electronic department, Harbin Institute of Technology, Harbin 150006, China; Electronic Science and technology Post-Doctoral Research Center, Heilongjiang University, Harbin, China; Micro-electronic department, Harbin Institute of Technology, Harbin 150006, China; Micro-electronic department, Harbin Institute of Technology, Harbin 150006, China","2015 IEEE 11th International Conference on ASIC (ASICON)","21 Jul 2016","2015","","","1","4","This paper presents a 1.8-V 16-bit fourth-order delta sigma modulator for audio applications. Double-sampled structure with DC bias is proposed to double the oversampling ratio in order to improve the resolution without extra demand for the clock frequency, which greatly reduces the power consumption of this system by 50%. Moreover, the gain-enhanced current-mirror amplifier is designed to be the first stage OTA, which decreases the power consumption of the first integrator by 80% compared with a folded-cascode amplifier. With a clock frequency of 3.072MHz and an oversampling ratio of 128, the delta sigma modulator has a signal-to-noise ratio (SNR) of 104.63dB over a bandwidth of 24 kHz. Even when the error ratio of mismatch of sampling paths reaches 1%, the degradation of the SNR is no more than 0.3dB. The power consumption of this modulator is 606μW.","2162-755X","978-1-4799-8485-5","10.1109/ASICON.2015.7517014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7517014","","Modulation;Power demand;Signal to noise ratio;Mathematical model;Capacitors;Feedforward neural networks;Delta-sigma modulation","audio systems;CMOS analogue integrated circuits;CMOS digital integrated circuits;current mirrors;delta-sigma modulation;integrated circuit design;low-power electronics;operational amplifiers;power aware computing","SNR degradation;sampling path mismatch;signal-to-noise ratio;integrator power consumption;OTA;gain-enhanced current-mirror amplifier design;power consumption reduction;oversampling ratio;DC bias;audio applications;fourth-order delta sigma modulator;low-power double-sampled delta sigma modulator;voltage 1.8 V;word length 16 bit","","","","7","","21 Jul 2016","","","IEEE","IEEE Conferences"
"Design of Cortex-M4F-Based Intelligent Audio Player","Y. Sun; X. Di; L. Zhu","Sch. of Optoelectron. Eng., Changchun Univ. of Sci. & Technol., Changchun, China; Sch. of Optoelectron. Eng., Changchun Univ. of Sci. & Technol., Changchun, China; Sch. of Inf. Technol., Changchun Vocational Inst. of Technol., Changchun, China","2013 International Conference on Information Technology and Applications","11 Jan 2014","2013","","","109","112","This design overcomes disadvantages of traditional MP3 players which include few supported file formats, absence of intelligent interface and incapability of replacing storage devices. In this system, the latest ARM® CortexTM-M4F-based STM32F4 chip is adopted to be the host controller to read out audio files stored in Secure Digital (SD) memory card via SPI2 and send the data into VS1003 via SPI1. VS1003 chip decodes and plays audio files. Meanwhile, the song information and lyrics are displayed on Thin Film Transistor (TFT) LCD which integrates with touch panel to realize song switchover and volume adjusting and other control functions. The separation of decoding module and storage module makes it possible to expand capacity and share music. Experimental results show that the data transmission speed is enhanced for the adoption of SD card. In addition, the number of audio file formats supported by this system is greater than the traditional MP3 players'.","","978-1-4799-2877-4","10.1109/ITA.2013.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6709947","Cortex-M4F;STM32F4;VS1003;SD card;TFT LCD","Digital audio players;Thin film transistors;Decoding;Transform coding;Data communication;Control systems;Music","audio equipment;intelligent materials;liquid crystal displays","touch panel;LCD;thin film transistor;secure digital memory card;STM32F4 chip;ARM® CortexTM-M4F;replacing storage devices;traditional MP3 players;intelligent audio player;Cortex-M4F","","","","6","","11 Jan 2014","","","IEEE","IEEE Conferences"
"Independent Deeply Learned Matrix Analysis for Determined Audio Source Separation","N. Makishima; S. Mogami; N. Takamune; D. Kitamura; H. Sumino; S. Takamichi; H. Saruwatari; N. Ono","Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; National Institute of Technology, Kagawa College, Kagawa, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Graduate School of System Design, Tokyo Metropolitan University, Tokyo, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","15 Jul 2019","2019","27","10","1601","1615","In this paper, we propose a new framework called independent deeply learned matrix analysis (IDLMA), which unifies a deep neural network (DNN) and independence-based multichannel audio source separation. IDLMA utilizes both pretrained DNN source models and statistical independence between sources for the separation, where the time-frequency structures of each source are iteratively optimized by a DNN while enhancing the estimation accuracy of the spatial demixing filters. As the source generative model, we introduce a complex heavy-tailed distribution to improve the separation performance. In addition, we address a semi-supervised situation; namely, a solo-recorded audio dataset can be prepared for only one source in the mixture signal. To solve the limited-data problem, we propose an appropriate data augmentation method to adapt the DNN source models to the observed signal, which enables IDLMA to work even in the semi-supervised situation. Experiments are conducted using music signals with a training dataset in both supervised and semi-supervised situations. The results show the validity of the proposed method in terms of the separation accuracy.","2329-9304","","10.1109/TASLP.2019.2925450","SECOM Science and Technology Foundation and JSPS KAKENHI(grant numbers:JP17H06101,JP19H01116,JP19K20306); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747523","Audio source separation;independent component analysis;deep neural networks;semi-supervised learning","Power capacitors;Source separation;Time-frequency analysis;Covariance matrices;Spectrogram;Data models;Estimation","audio signal processing;iterative methods;matrix algebra;neural nets;source separation;spatial filters;supervised learning","independent deeply learned matrix analysis;IDLMA;independence-based multichannel audio source separation;source generative model;solo-recorded audio dataset;DNN source models;deep neural network;data augmentation method","","17","","44","CCBY","27 Jun 2019","","","IEEE","IEEE Journals"
"Intelligent Signal Processing Mechanisms for Nuanced Anomaly Detection in Action Audio-Visual Data Streams","J. Kittler; I. Kaloskampis; C. Zor; Y. Xu; Y. Hicks; W. Wang","Centre for Vision Speech and Signal Processing, University of Surrey, UK; Cardiff School of Engineering, Cardiff University, UK; Centre for Vision Speech and Signal Processing, University of Surrey, UK; Centre for Vision Speech and Signal Processing, University of Surrey, UK; Cardiff School of Engineering, Cardiff University, UK; Centre for Vision Speech and Signal Processing, University of Surrey, UK","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","6563","6567","We consider the problem of anomaly detection in an audiovisual analysis system designed to interpret sequences of actions from visual and audio cues. The scene activity recognition is based on a generative framework, with a high-level inference model for contextual recognition of sequences of actions. The system is endowed with anomaly detection mechanisms, which facilitate differentiation of various types of anomalies. This is accomplished using intelligence provided by a classifier incongruence detector, classifier confidence module and data quality assessment system, in addition to the classical outlier detection module. The paper focuses on one of the mechanisms, the classifier incongruence detector, the purpose of which is to flag situations when the video and audio modalities disagree in action interpretation. We demonstrate the merit of using the Delta divergence measure for this purpose. We show that this measure significantly enhances the incongruence detection rate in the Human Action Manipulation complex activity recognition data set.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8461595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461595","Audio visual scene analysis;incongruence detection;anomaly detection","Anomaly detection;Feature extraction;Streaming media;Visualization;Task analysis;Dairy products;Probability distribution","audio signal processing;audio-visual systems;computer vision;data mining;feature extraction;image recognition;learning (artificial intelligence);video signal processing;video surveillance","audiovisual analysis system;visual cues;audio cues;scene activity recognition;generative framework;high-level inference model;anomaly detection mechanisms;classifier incongruence detector;classifier confidence module;data quality assessment system;classical outlier detection module;audio modalities;action interpretation;incongruence detection rate;intelligent signal processing mechanisms;nuanced anomaly detection;video modalities;human action manipulation complex activity recognition data;action audio-visual data streams","","2","","23","","13 Sep 2018","","","IEEE","IEEE Conferences"
"Embedded system for acquisition and enhancement of audio signals","K. Kowalczyk; S. Wozniak; T. Chyrowicz; R. Rumian","Department of Electronics, AGH University of Science and Technology, 30-059 Krakow, Poland; Department of Electronics, AGH University of Science and Technology, 30-059 Krakow, Poland; Department of Electronics, AGH University of Science and Technology, 30-059 Krakow, Poland; Department of Electronics, AGH University of Science and Technology, 30-059 Krakow, Poland","2016 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA)","5 Dec 2016","2016","","","68","71","Enhancing speech from audio signals recorded using several microphones is of interest to many audio applications such as human-computer interfaces and teleconference systems. With an increasing availability of low-cost micro-electro-mechanical systems (MEMS) and powerful processing units, embedded systems are capable of recording and processing several audio signals in realtime. In this paper, an overview of the entire signal processing chain from audio signal acquisition with an array of MEMS microphones to the signal enhancement using directional filtering is provided. The presented results indicate that good noise and interference suppression can be achieved in realtime using an embedded system equipped with low-cost microphones.","2326-0319","978-83-62065-27-1","10.1109/SPA.2016.7763589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7763589","","Signal to noise ratio;Microphone arrays;Speech enhancement;Embedded systems;Array signal processing;Speech","audio recording;audio signal processing;embedded systems;filtering theory;interference suppression;micromechanical devices;microphone arrays;signal detection;speech processing","embedded system;human-computer interfaces;teleconference systems;low-cost microelectromechanical systems;signal processing chain;audio signal acquisition;MEMS microphones;signal enhancement;directional filtering;interference suppression;low-cost microphones","","","","14","","5 Dec 2016","","","IEEE","IEEE Conferences"
"Efficient and robust audio watermarking for content authentication and copyright protection","V. Neethu; R. Kalaivani","Dept. of ECE, Coimbatore Institute of Technology, Coimbatore, India; Dept. of ECE, Coimbatore Institute of Technology, Coimbatore, India","2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)","4 Aug 2016","2016","","","1","6","Digital audio watermarking aims to embed digital information in the form of multimedia files such as text, image or audio into an original audio signal. The main requirement of audio watermarking is to prove ownership as well as copyright protection. This paper presents efficient audio watermarking based on Wavelet transform. The L level Haar Wavelet transform is performed on the audio signal and the obtained detail coefficients are divided into short frames and the magnitude of the samples are then replaced with the closest Fibonacci numbers. The security of the watermarking technique is further enhanced by adapting cryptographic methods on the embedded secret text. The suggested technique mathematically proves that the average error for each sample is 25%. The fidelity of the technique is also proved mathematically. The experimental outcomes suggest that the method is having high capacity (1kbps to 3 kbps), robustness against various signal processing attacks and no significant perceptual distortion (ODG is around -1).","","978-1-5090-1277-0","10.1109/ICCPCT.2016.7530371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530371","Audio Watermarking;fidelity;Fibonacci numbers;Golden Ratio;DWT","Watermarking;Discrete wavelet transforms;Robustness;Distortion;Computers","audio signal processing;audio watermarking;copyright;cryptography;Fibonacci sequences;Haar transforms;wavelet transforms","content authentication;copyright protection;digital audio watermarking;L level Haar wavelet transform;audio signal;Fibonacci numbers;cryptographic methods;embedded secret text","","2","","19","","4 Aug 2016","","","IEEE","IEEE Conferences"
"A novel hiss noise reduction method for audio signals based on MDCT","F. Deng; C. Bao; B. Xia; Y. Liang","Speech and Audio Signal Processing Laboratory, School of Electronic Information and Control Engineering, Beijing University of Technology, Beijing 100124, China; Speech and Audio Signal Processing Laboratory, School of Electronic Information and Control Engineering, Beijing University of Technology, Beijing 100124, China; Speech and Audio Signal Processing Laboratory, School of Electronic Information and Control Engineering, Beijing University of Technology, Beijing 100124, China; Speech and Audio Signal Processing Laboratory, School of Electronic Information and Control Engineering, Beijing University of Technology, Beijing 100124, China","2011 International Conference on Wireless Communications and Signal Processing (WCSP)","8 Dec 2011","2011","","","1","5","A kind of hiss noise reduction method used for audio signal is proposed by Modified Discrete Cosine Transform (MDCT) in this paper. The human auditory model and the parametric soft-thresholding are introduced to the proposed method. A modified median absolute deviation (MAD) is first adopted to avoid overestimate of noise levels. Next, the Modified Discrete Fourier Transform (MDFT) is constructed using the pre-enhanced MDCT coefficients and the masking threshold and the adaptive masking parameters are calculated in MDFT domain. Finally, a parametric soft-thresholding method is employed to attenuate hiss noise significantly and keep more high-frequency (HF) information. The objective and subjective listening test results show that the proposed algorithm outperforms the referenced methods.","","978-1-4577-1010-0","10.1109/WCSP.2011.6096796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6096796","hiss noise reduction;MDCT;MAD;human auditory model;MDFT;parametric soft-thresholding","Noise;Masking threshold;Noise measurement;Noise reduction;Noise level;Time frequency analysis;Discrete cosine transforms","audio signal processing;discrete cosine transforms;discrete Fourier transforms;signal denoising","hiss noise reduction method;audio signals;modified discrete cosine transform;human auditory model;modified median absolute deviation;MAD;modified discrete Fourier transform;preenhanced MDCT coefficients;masking threshold;adaptive masking parameters;MDFT domain;parametric soft-thresholding method;high-frequency information;HF information","","2","","10","","8 Dec 2011","","","IEEE","IEEE Conferences"
"Robustness and embedding capacity enhancement in time-spread echo-based audio watermarking","I. Natgunanathan; Y. Xiang; L. Pan; P. Chen; D. Peng","School of Information Technology, Deakin University, Burwood Campus, Melbourne, Australia; School of Information Technology, Deakin University, Burwood Campus, Melbourne, Australia; School of Information Technology, Deakin University, Burwood Campus, Melbourne, Australia; Machine Intelligence Laboratory, College of Computer Science, Sichuan University, Chengdu 610065, P. R. China; Machine Intelligence Laboratory, College of Computer Science, Sichuan University, Chengdu 610065, P. R. China","2016 IEEE 11th Conference on Industrial Electronics and Applications (ICIEA)","24 Oct 2016","2016","","","1536","1541","In echo-based audio watermarking methods, poor robustness and low embedding capacity are the main problems. In this paper, we propose a novel time-spread echo method for audio watermarking, aiming to improve the robustness and the embedding capacity. To improve the robustness, we design an efficient pseudonoise (PN) sequence and a corresponding decoding function. Compared to the conventional PN sequence used in time-spread echo hiding based method, more large peaks are produced during the autocorrelation of the proposed PN sequence. Our decoding function is designed to utilize these peaks to improve the robustness. To enhance the embedding capacity, multiple watermark bits are embedded into one audio segment. This is achieved by varying the delays of added echo signals. Moreover, the security of the proposed method is further improved by scrambling the watermarks at the embedding stage. Compared with the conventional time-spread echo-based method, the proposed method is more robust to conventional attacks and has higher embedding capacity. The effectiveness of our method is illustrated by simulation results.","2158-2297","978-1-4673-8644-9","10.1109/ICIEA.2016.7603829","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7603829","Audio watermarking;autocorrelation;echo hiding;echo kernel;time-spread echo;robustness","Watermarking;Robustness;Decoding;Correlation;Delays;Fourier transforms","audio coding;audio watermarking;decoding;echo","embedding capacity enhancement;time-spread echo-based audio watermarking;pseudonoise sequence;PN sequence;decoding function;time-spread echo hiding-based method;audio segment;watermark scrambling","","4","","23","","24 Oct 2016","","","IEEE","IEEE Conferences"
"Can spatial audio support pilots? 3D-audio for future pilot-assistance systems","C. A. Niermann","German Aerospace Center (DLR), Institute of Flight Guidance, Lilienthalplatz 7, 38108 Braunschweig, Germany","2015 IEEE/AIAA 34th Digital Avionics Systems Conference (DASC)","29 Oct 2015","2015","","","3C5-1","3C5-7","In modern cockpits most of the information is provided to the pilot visually. This information is typically presented head-down on multiple displays. The advantage of these glass cockpits tends to be impaired by constantly increasing the amount of information presented. Especially during landing however pilots need to maintain a good spatial awareness from visual references in the vicinity of the aircraft. Therefore, the pilot has to deal with multiple concurrent tasks all with dominant impact on the pilot's visual perception. With the limited human visual perception also the cognitive ability of humans might be reached [1, 2]. This introduces new operational burdens and failure modes to the overall human-machine system [3, 4]. To avoid impacts on safety a remarkable step forward would be firstly to reduce the demand on visual perception that would provide additional spare mental capacity to safely monitor and control the aircraft even under high workload phases or system failure conditions. Secondly, to find a way to maintain or even enhance the information flow at the same time. Today, the auditory component conveys no spatial information and is generally used to draw attention to a visual display [5]. Audio research has been sparse in aviation and only covered spatial audio with a set of speakers around the head of the pilot or simple left-right-volume difference in a stereo headset [6]. Several previous studies have suggested a multitude of applications for useful 3D-audio concepts in the cockpit [5, 7, 8]. Humans are able to localize a 3D-audio presentation via headset adequately enough to direct their attention to a specific point inside the cockpit or out-of-the-window. Tracking the head movement becomes one key feature to reduce frontback confusion and inside-the-head localization. With preselected assistance system 3D-audio has the capability to support pilots during critical flight phases potentially even decreasing the overall workload.","2155-7209","978-1-4799-8940-9","10.1109/DASC.2015.7311401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7311401","","Visualization;Headphones;Ear;Aircraft;Auditory system;Visual perception;Azimuth","air safety;aircraft displays;audio systems","spatial audio;future pilot-assistance systems;multiple displays;glass cockpits;pilot visual perception;human visual perception;human-machine system;aircraft control;aircraft safety;system failure conditions;visual display;auditory component;spatial information;aviation;stereo headset;3D-audio presentation localization;head movement tracking;frontback confusion reduction;inside-the-head localization","","","","34","","29 Oct 2015","","","IEEE","IEEE Conferences"
"Audio-Emotion Recognition System Using Parallel Classifiers and Audio Feature Analyzer","L. W. Chew; K. P. Seng; L. Ang; V. Ramakonar; A. Gnanasegaran","Fac. of Eng., Univ. of Nottingham, Jalan Broga, Malaysia; Sunway Univ., Petaling Jaya, Malaysia; Fac. of Eng., Univ. of Nottingham, Jalan Broga, Malaysia; Alsys MSC Sdn Bhd, Kuala Lumpur, Malaysia; Alsys MSC Sdn Bhd, Kuala Lumpur, Malaysia","2011 Third International Conference on Computational Intelligence, Modelling & Simulation","14 Nov 2011","2011","","","210","215","Emotion recognition based on an audio signal is an area of active research in the domain of human-computer interaction and effective computing. This paper presents an audio-emotion recognition (AER) system using parallel classifiers and an audio feature analyzer. In the proposed system, audio features such as the pitch and fractional cepstral coefficient are first extracted from the audio signal for analysis. These extracted features are then used to train a radial basis function. Lastly, an audio feature analyzer is used to enhance the performance of the recognition rate. The latest simulation results show that the proposed AER system is able to achieve an emotion recognition rate of 81.67%.","2166-8531","978-1-4577-1797-0","10.1109/CIMSim.2011.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6076358","Emotion recognition;linear discriminant analysis;Mel-frequency cepstral coefficients;principal component analysis;radial basis function","Feature extraction;Emotion recognition;Speech;Principal component analysis;Mel frequency cepstral coefficient;Training;Covariance matrix","audio signal processing;emotion recognition;feature extraction;human computer interaction;radial basis function networks","audio-emotion recognition system;parallel classifiers;audio feature analyzer;human-computer interaction;radial basis function","","9","","21","","14 Nov 2011","","","IEEE","IEEE Conferences"
"A Multichannel Audio Denoising Formulation Based on Spectral Sparsity","İ. Bayram","Dept. of Electronics and Communication Engineering, Istanbul Technical University, Istanbul, Turkey","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 May 2017","2015","23","12","2272","2285","We consider the estimation of an audio source from multiple noisy observations, where the correlation between noise in the different observations is low. We propose a two-stage method for this estimation problem. The method does not require any information about noise and assumes that the signal of interest has a sparse time-frequency representation. The first stage uses this assumption to obtain the best linear combination of the observations. The second stage estimates the amount of remaining noise and applies a post-filter to further enhance the reconstruction. We discuss the optimality of this method under a specific model and demonstrate its usefulness on synthetic and real data.","2329-9304","","10.1109/TASLP.2015.2479042","TUBITAK(grant numbers:113E511); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7268852","Beamforming;multichannel audio denoising;post-filter;sparsity;spectrogram;sufficient statistic;uniformly minimum variance unbiased (UMVU) estimator","Time-frequency analysis;Acoustic measurements;Noise measurement;Minimization;Noise reduction;Random variables;Array signal processing;Spectrograms","audio signal processing;correlation theory;filtering theory;signal denoising;signal reconstruction;spectral analysis;time-frequency analysis","sparse time-frequency representation;signal reconstruction;multichannel audio denoising;spectral sparsity;audio source estimation;noise correlation;signal postfilter;noise estimation","","","","39","","15 Sep 2015","","","IEEE","IEEE Journals"
"A 0.96mA quiescent current, 0.0032% THD+N, 1.45W Class-D audio amplifier with area-efficient PWM-residual-aliasing reduction","S. Chien; Y. Chen; T. Kuo","National Cheng Kung University, Tainan, Taiwan; National Cheng Kung University, Tainan, Taiwan; National Cheng Kung University, Tainan, Taiwan","2018 IEEE International Solid - State Circuits Conference - (ISSCC)","12 Mar 2018","2018","","","60","62","Low quiescent current (I<sub>Q</sub>) is critical for Class-D audio amplifiers in mobile devices to extend battery usage time [1], since typical audio signals have a high crest factor of 10 to 20dB. In addition, low distortion is also important for audio fidelity. Distortion sources in closed-loop Class-D amplifiers can be classified into two types. One is attributed to the nonlinearities of PWM modulators and power stages, while the other is due to the aliasing of fed-back PWM high-frequency residuals, the latter of which comprises phase-error and duty-cycle-error distortions [2]. Figure 3.6.1 shows 2<sup>nd</sup>-order closed-loop amplifiers and existing techniques for enhancing an amplifier's linearity. Increasing the loop filter order to obtain a higher in-band loop gain by using more integrators [3] or the single-amplifier-biquad [4] suppresses all aforementioned distortions except for the phase-error distortion, which can be suppressed by adding a phase-error-free PWM modulator [2]. However, these techniques increase I<sub>Q</sub> and/or die area due to the additional active circuits and/or several resistors and capacitors. Since phase-error distortion, as well as duty-cycle-error distortion, is caused by the fed-back PWM high-frequency residuals aliasing with the reference triangular wave V<sub>TRI</sub>, a uniform PWM [5] with a sample-and-hold circuit implemented before the PWM modulation reduces the PWM residuals via an equivalent notch filtering. However, loop stability is affected by the notch filtering unless the PWM switching frequency f<sub>SW</sub> is increased, but doing so increases power consumption [4]. Though the technique in [1] uses a feed-forward path with a replicated loop filter to eliminate the PWM residuals without affecting loop stability, the replicated loop filter increases both I<sub>Q</sub> and area.","2376-8606","978-1-5090-4940-0","10.1109/ISSCC.2018.8310183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8310183","","Pulse width modulation;Phase distortion;Current measurement;Phase modulation;Linearity","audio-frequency amplifiers;biquadratic filters;circuit stability;closed loop systems;distortion;harmonic distortion;notch filters;pulse width modulation;sample and hold circuits","closed-loop class-D amplifiers;active circuits;PWM switching frequency;power consumption;single-amplifier-biquad;PWM high-frequency residuals;amplifier linearity enhancement;resistors;capacitors;sample-and-hold circuit;equivalent notch filtering;feedforward path;audio signals;THD+N class-D audio amplifier;replicated loop filter;loop stability;PWM residuals;PWM modulation;uniform PWM;high-frequency residuals aliasing;duty-cycle-error distortion;phase-error-free PWM modulator;phase-error distortion;in-band loop gain;loop filter order;closed-loop amplifiers;duty-cycle-error distortions;PWM modulators;distortion sources;audio fidelity;low distortion;high crest factor;battery usage time;mobile devices;low quiescent current;area-efficient PWM-residual-aliasing reduction;current 0.96 mA","","","","5","","12 Mar 2018","","","IEEE","IEEE Conferences"
"A Musically Motivated Mid-Level Representation for Pitch Estimation and Musical Audio Source Separation","J. Durrieu; B. David; G. Richard","Signal Processing Laboratories (LTS5), Ecole Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland; Institut Telecom, Telecom ParisTech, CNRS-LTCI, Paris, France; Institut Telecom, Telecom ParisTech, CNRS-LTCI, Paris, France","IEEE Journal of Selected Topics in Signal Processing","15 Sep 2011","2011","5","6","1180","1191","When designing an audio processing system, the target tasks often influence the choice of a data representation or transformation. Low-level time-frequency representations such as the short-time Fourier transform (STFT) are popular, because they offer a meaningful insight on sound properties for a low computational cost. Conversely, when higher level semantics, such as pitch, timbre or phoneme, are sought after, representations usually tend to enhance their discriminative characteristics, at the expense of their invertibility. They become so-called mid-level representations. In this paper, a source/filter signal model which provides a mid-level representation is proposed. This representation makes the pitch content of the signal as well as some timbre information available, hence keeping as much information from the raw data as possible. This model is successfully used within a main melody extraction system and a lead instrument/accompaniment separation system. Both frameworks obtained top results at several international evaluation campaigns.","1941-0484","","10.1109/JSTSP.2011.2158801","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5784290","Audio melody extraction;audio signal representation;musical audio source separation;non-negative matrix factorization (NMF);pitch estimation","Instruments;Estimation;Harmonic analysis;Timbre;Time frequency analysis;Dictionaries;Spectral shape","audio signal processing;Fourier transforms;music;source separation","musically motivated mid-level representation;pitch estimation;musical audio source separation;audio processing system;low-level time-frequency representations;short-time Fourier transform;pitch content;melody extraction system","","86","1","46","","6 Jun 2011","","","IEEE","IEEE Journals"
"A supervised approach to hierarchical metrical cycle tracking from audio music recordings","A. Srinivasamurthy; X. Serra","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","14 Jul 2014","2014","","","5217","5221","A supervised approach to metrical cycle tracking from audio is presented, with a main focus on tracking the tala, the hierarchical cyclic metrical structure in Carnatic music. Given the tala of a piece, we aim to estimate the aksara (lowest metrical pulse), the aksara period, and the sama (first pulse of the tala cycle). Starting with percussion enhanced audio, we estimate the aksara pulse period from a tempogram computed using an onset detection function. A novelty function is computed using a self similarity matrix constructed using frame level audio features. These are then used to estimate possible aksara and sama candidates, followed by a candidate selection based on periodicity constraints, which leads to the final estimates. The algorithm is tested on an annotated collection of 176 pieces spanning four different talas. Though applied to Carnatic music, the framework presented is general and can be extended to other music cultures with cyclical metrical structures.","2379-190X","978-1-4799-2893-4","10.1109/ICASSP.2014.6854598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6854598","Rhythm;Musical meter;Carnatic Music;Metrical Cycles","Decision support systems;Conferences;Acoustics;Speech;Speech processing","audio recording;audio signal processing;feature selection;learning (artificial intelligence);matrix algebra;music","hierarchical metrical cycle tracking;audio music recording;supervised approach;tala tracking;hierarchical cyclic metrical structure;Carnatic music;aksara estimation;sama estimation;percussion enhanced audio;aksara pulse period estimation;tempogram computation;onset detection function;self similarity matrix;frame level audio feature;candidate selection;periodicity constraint;music cultures","","6","","25","","14 Jul 2014","","","IEEE","IEEE Conferences"
"Enhancing quality and accuracy of speech recognition system by using multimodal audio-visual speech signal","E. E. El Maghraby; A. M. Gody; M. H. Farouk","Electrical Engineering, Faculty of Engineering, Fayoum University, Egypt; Electrical Engineering, Faculty of Engineering, Fayoum University, Egypt; Engineering Math. & Physics Dept., Faculty of Engineering, Cairo University, Egypt","2016 12th International Computer Engineering Conference (ICENCO)","16 Feb 2017","2016","","","219","229","Most developments in speech-based automatic recognition have relied on acoustic speech as the sole input signal, disregarding its visual counterpart. However, recognition based on acoustic speech alone can be afflicted with deficiencies that prevent its use in many real-world applications, particularly under adverse conditions. This paper aims to build a connected-words audio visual speech recognition system (AV-ASR) for English language that uses both acoustic and visual speech information to improve the recognition performance. Mel frequency cepstral coefficients (MFCCs) have been used to extract the audio features from the speech-files. For the visual counterpart, the Discrete Cosine Transform (DCT) Coefficients have been used to extract the visual feature from the speaker's mouth region and Principle Component Analysis (PCA) have been used for dimensionality reduction purpose, These features are then concatenated with traditional audio ones, and the resulting features are used for training hidden Markov models (HMMs) parameters using word level acoustic models. The system has been developed using hidden Markov model toolkit (HTK) that uses hidden Markov models (HMMs) for recognition. The potential of the suggested approach is demonstrate by a preliminary experiment on the GRID sentence database one of the largest databases available for audio-visual recognition system, which contains continuous English voice commands for a small vocabulary task. The experimental results show that the proposed Audio Video Speech Recognizer (AV-ASR) system exhibits higher recognition rate in comparison to an audio-only recognizer as well as it indicates robust performance. An increase of success rate by 3.9% for the grammar based word recognition system overall speakers is achieved for speaker independent test and for speaker dependent, it changes from speaker to another between 7% and 1%. Also when test the system under noisy environment it improve the result.","","978-1-5090-2863-4","10.1109/ICENCO.2016.7856472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7856472","AV-ASR;HMM;HTK;MFCC;DCT;PCA;MATLAB;GRID","Visualization;Lips;Face;Speech;Principal component analysis;Hidden Markov models;Keyboards","audio databases;audio-visual systems;cepstral analysis;data reduction;discrete cosine transforms;feature extraction;grammars;hidden Markov models;principal component analysis;speaker recognition","multimodal audiovisual speech signal;speech-based automatie recognition;connected-word audiovisual speech recognition system;AV-ASR accuracy;AV-ASR quality;Mel frequency cepstral coefficients;MFCC;audio feature extraction;speech-files;discrete cosine transform coefficients;DCT coefficients;principle component analysis;speaker mouth region;dimensionality reduction;hidden Markov models parameter training;word level acoustie models;HMM;hidden Markov model toolkit;HTK;GRID sentence database;continuous English voice commands;sm vocabulary task;grammar based word recognition system;speaker independent test;speaker dependent test","","","","38","","16 Feb 2017","","","IEEE","IEEE Conferences"
"Micromachined Piezoelectric Acoustic Sensor with Enhanced SNR for Audio Applications","A. Rahaman; B. Kim","Korea University of Technology and Education,School of Mechatronics Engineering,Cheonan,Rep. of Korea,31253; Korea University of Technology and Education,School of Mechatronics Engineering,Cheonan,Rep. of Korea,31253","2019 IEEE SENSORS","14 Jan 2020","2019","","","1","4","A MEMS piezoelectric acoustic sensor (PAS) with enhanced signal-to-noise ratio (SNR) can be productive in audio applications. Notably, the simultaneous improvements on sensitivity and noise can amplify the SNR; whereas, the traditional works were enclosed either with sensitivity enhancement, or noise optimization. As a consequences, the traditional works had a lack of either high sensitivity, or low noise. In this paper, an analytical and experimental work on the SNR enhancement has been reported. At first, an analytical model was developed using the common influential factors of a PAS, and then, the model was numerically analyzed to achieve the optimized parameters. For the proof-of-concept, the optimized parameters were fabricated for the experimental verification. In experiment, the measured SNR was 67.03 dB which is the highest SNR so far for the circular-shaped PAS, and found 4.02% difference from the analytically achieved SNR. The modal frequency of the PAS was achieved at 10.18 kHz with the bandwidth of 2~10 kHz which can be extended for audio applications with higher SNR.","2168-9229","978-1-7281-1634-1","10.1109/SENSORS43011.2019.8956690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8956690","MEMS;D33;Aluminum Nitride;SNR;Acoustic sensor","","acoustic transducers;micromachining;microsensors;numerical analysis;optimisation;piezoelectric transducers","numerical analysis;micromachined piezoelectric acoustic sensor;circular-shaped PAS;optimized parameters;SNR enhancement;noise optimization;enhanced signal-to-noise ratio;MEMS piezoelectric acoustic sensor;audio applications;noise figure 67.03 dB;frequency 10.18 kHz;frequency 2 kHz to 10 kHz","","2","","23","","14 Jan 2020","","","IEEE","IEEE Conferences"
"Multidimensional QoE Assessment of Multi-view Video and Selectable Audio (MVV-SA) IP Transmission","T. Ishida; T. Nunome","Dept. of Comput. Sci. & Eng., Nagoya Inst. of Technol., Nagoya, Japan; Dept. of Comput. Sci. & Eng., Nagoya Inst. of Technol., Nagoya, Japan","2013 IEEE International Symposium on Multimedia","24 Feb 2014","2013","","","534","535","This paper deals with Multi-View Video and Selectable Audio (MVV-SA) IP transmission, users can switch not only video but also audio according to a viewpoint change request. We evaluate QoE of MVV-SA by a subjective experiment. The evaluation is performed by the Semantic Differential (SD) method with 13 adjective pairs. In the subjective experiment, we ask assessors to evaluate 40 stimuli which consist of two kinds of UDP load traffic, two kinds of fixed additional delay, five kinds of playout buffering time, and selectable or un-selectable audio (i.e., MVV-SA or the previous MVV-A). As a result, MVV-SA gives higher presence to the user than MVV-A and then enhances QoE. We also conduct factor analysis to clarify component factors of QoE.","","978-1-4799-2171-3","10.1109/ISM.2013.109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6746861","multi-view video;selectable audio;audio-video IP transmission;QoE;multidimensional assessment","Streaming media;Cameras;Media;Servers;Delays;Receivers;Psychology","IP networks;multimedia communication;quality of experience;video communication","multidimensional QoE assessment;multiview video-selectable audio IP transmission;MVV-SA IP transmission;viewpoint change request;QoE evaluation;semantic differential method;SD method;UDP load traffic;fixed additional delay;playout buffering time;unselectable audio;selectable audio;QoE enhancement;factor analysis;QoE component factors","","2","","7","","24 Feb 2014","","","IEEE","IEEE Conferences"
"Automatic Gain Control for Enhanced HDR Performance on Audio","D. E. Garcia; J. Hernandez; S. Mann","MannLab Canada,Toronto,Ontario,M5T 1G5; MannLab Canada,Toronto,Ontario,M5T 1G5; MannLab Canada,Toronto,Ontario,M5T 1G5","2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP)","16 Dec 2020","2020","","","1","6","We introduce a method to enhance the performance of the high dynamic range (HDR) technique on audio signals by automatically controlling the gains of the individual signal channels. Automatic gain control (AGC) compensates the receiver's dynamic range by ensuring that the incoming signal is contained within the desired range while the HDR utilizes these multi-channel gains to extend the dynamic range of the composited signal. The results validate that the benefits given by each method are compounded when they are used together. In effect, we produce a dynamic high dynamic range (DHDR) composite signal. The HDR AGC method is simulated to show performance gains under various conditions. The method is then implemented using a custom PCB and a microcontroller to show feasibility in real-world and real-time applications.","2473-3628","978-1-7281-9320-5","10.1109/MMSP48831.2020.9287160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9287160","Signal processing;audio signal;signal reconstruction;high dynamic range (HDR);automatic gain control (AGC)","Performance evaluation;Process control;Dynamic range;Signal processing;Performance gain;Real-time systems;Gain control","audio signal processing;automatic gain control;microcontrollers;performance evaluation;printed circuits;real-time systems","high dynamic range technique;audio signals;individual signal channels;automatic gain control;multichannel gains;dynamic high dynamic range composite signal;HDR AGC method;enhanced HDR performance;DHDR composite signal;performance gains;PCB;microcontroller;real-time applications","","3","","37","","16 Dec 2020","","","IEEE","IEEE Conferences"
"Harmonic model for MDCT based audio coding with LPC envelope","T. Moriya; Y. Kamamoto; N. Harada; T. Bäckström; C. Helmrich; G. Fuchs","Nippon Telegraph and Telephone Corp. (NTT), Japan; Nippon Telegraph and Telephone Corp. (NTT), Japan; Nippon Telegraph and Telephone Corp. (NTT), Japan; International Audio Laboratories Erlangen, Friedrich-Alexander University (FAU), Germany; International Audio Laboratories Erlangen, Friedrich-Alexander University (FAU), Germany; Fraunhofer Institute for Integrate Circuits (IIS), Germany","2015 23rd European Signal Processing Conference (EUSIPCO)","28 Dec 2015","2015","","","789","793","Conventional music coders, based on a modified discrete cosine transform (MDCT) suffer greatly when lowering their bit-rate and delay. In particular, tonal music signals are penalized by short analysis windows and the variable length coding of the quantized MDCT coefficients demands a significant amount of bits for coding the harmonic structure. For solving such an issue, the paper proposes a frequency-domain harmonic model aiming to amend the probability model of the variable length coding of the quantized MDCT coefficients. The new model was combined successfully with an envelope based arithmetic coding at rate lower than 10 kbps, and with a context based arithmetic coding at higher bit rates in the recent 3 GPP EVS (Enhanced Voice Services) codec standard. Objective and subjective quality tests indicate that the proposed harmonic model enhances the quality of music for low-delay audio coding.","2076-1465","978-0-9928-6263-3","10.1109/EUSIPCO.2015.7362491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362491","MDCT;envelope;harmonic interval;arithmetic coding;EVS","Harmonic analysis;Encoding;Power harmonic filters;Context;Frequency-domain analysis;Indexes;Speech","arithmetic codes;audio coding;discrete cosine transforms;frequency-domain analysis;linear predictive coding;music;probability;variable length codes","audio coding;LPC envelope;music coder;modified discrete cosine transform;tonal music signal;short analysis window;variable length coding;quantized MDCT coefficient;frequency-domain harmonic model;probability model;arithmetic coding;3GPP EVS codec standard;enhanced voice service;linear predictive coding","","2","","23","","28 Dec 2015","","","IEEE","IEEE Conferences"
"Low delay LPC and MDCT-based audio coding in the EVS codec","G. Fuchs; C. R. Helmrich; G. Marković; M. Neusinger; E. Ravelli; T. Moriya","Fraunhofer Institut für Integrierte Schaltungen (IIS), Germany; International Audio Laboratories Erlangen, Germany; Fraunhofer Institut für Integrierte Schaltungen (IIS), Germany; Fraunhofer Institut für Integrierte Schaltungen (IIS), Germany; Fraunhofer Institut für Integrierte Schaltungen (IIS), Germany; Nippon Telegraph and Telephone (NTT), Japan","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","6 Aug 2015","2015","","","5723","5727","Speech coders operating in time domain can be extended with a frequency domain mode to improve encoding of music, even though this is challenging at low delay. In such a scenario, the short analysis window limits the benefit of the transform coder, while a delayless switch between the two coders constrains the system further. The paper presents an LPC and MDCT-based audio coder part of the new 3GPP codec for Enhanced Voice Services, which aims to solve the issues. Several advanced coding tools are introduced to alleviate the constraints: transient handling is improved, harmonic structures are better preserved, and the modeling of the zero-quantized frequencies is enhanced. Test results show that the obtained low-delay switched coder brings a clear improvement over a speech coder and is competitive even in comparison to audio coders with higher delay.","2379-190X","978-1-4673-6997-8","10.1109/ICASSP.2015.7179068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7179068","Audio coding;Low delay;LPC;MDCT;EVS","Encoding;Switches;Speech;Quantization (signal);Noise;Decoding;Delays","3G mobile communication;audio coding;codecs;discrete cosine transforms;linear predictive coding;quantisation (signal);vocoders","linear predictive coding;low delay LPC;modified discrete cosine transform;MDCT;audio coding;enhanced voice services;EVS codec;speech coders;time domain mode;frequency domain mode;music encoding;transform coder;3GPP codec;coding tools;zero-quantized frequencies;low-delay switched code","","16","11","18","","6 Aug 2015","","","IEEE","IEEE Conferences"
"Audio Signal Detection and Enhancement Based on Linear CMOS Array and Multi-Channel Data Fusion","C. Dai; C. Liu; Y. Wu; X. Wang; H. Fu; H. Sun","Department of Electronics Engineering, Xiamen University, Xiamen, China; Department of Electronics Engineering, Xiamen University, Xiamen, China; Department of Electronics Engineering, Xiamen University, Xiamen, China; Department of Electronics Engineering, Xiamen University, Xiamen, China; Department of Electronics Engineering, Xiamen University, Xiamen, China; Department of Informatics and Communication Engineering, Xiamen University, Xiamen, China","IEEE Access","28 Jul 2020","2020","8","","133463","133469","An audio signal detection system based on laser speckle and multi-channel data fusion is presented. A linear CMOS array is used as the detector, which owns a fast line rate and suitable sensing size. The signals from the pixels are selected and fused to enhance the reconstructed signal. The reconstructed audio signals are evaluated with a segmental SNR (SegSNR) algorithm. The experimental results of three categories of audio sources (single voice audio, conversation and music) show that data fusion can improve the SegSNR scores. Especially, direct phase-error based filtering (pbf) fusion gives a nearly 3.0 dB increase and obtains another 1.0 dB increase with the combination of single channel process. The experimental results show that the fusion algorithms are not sensitive to audio types and the performance of multi-channel data fusion is not weakened with the increase of measuring distance. This feature has potential applications in remote sensing. The intelligibility of the fused audio signals is evaluated with normalized subband envelope correlation (NSEC) algorithm and the evaluation results shows that fusion can also enhance the intelligibility of the recovered signal.","2169-3536","","10.1109/ACCESS.2020.3010325","Natural National Science Foundation of China (NSFC)(grant numbers:61975167); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9144236","Audio signal detection and enhancement;linear CMOS array;multi-channel data fusion","Data integration;Signal processing algorithms;Speckle;Signal to noise ratio;Laser beams;Vibrations","audio signal processing;CMOS integrated circuits;filtering theory;sensor fusion;signal detection;speckle;speech enhancement","recovered signal;fused audio signals;fusion algorithms;single channel process;single voice audio;audio sources;segmental SNR algorithm;reconstructed audio signals;audio signal detection system;multichannel data fusion;linear CMOS array","","","","24","CCBY","20 Jul 2020","","","IEEE","IEEE Journals"
"System architectures and digital signal processing algorithms for enhancing the output audio quality of stereo FM broadcast receivers","J. Chen; T. Baker; E. McCarthy; J. Thyssen","Broadcom Corporation, Irvine, California, USA; Broadcom Corporation, Irvine, California, USA; Broadcom Corporation, Irvine, California, USA; Broadcom Corporation, Irvine, California, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","6 Aug 2015","2015","","","226","230","This paper presents two FM receiver architectures and three digital signal processing algorithms for enhancing the output audio quality of FM broadcast receivers. The two receiver architectures differ only in the front-end processing for estimating the carrier-to-noise ratio and noise floors of the stereo audio signals. The shared back-end processing consists of three algorithms to suppress the noise in the audio signal, to detect and cancel noise pulses (static), and to conceal the degrading effects of fast fading, respectively. Together these FM enhancement techniques achieve about 20 to 35 dB improvements in SNR and stereo separation over a wide range of RF signal strength spanning nearly 30 dB. Perceptually, the audio quality improvement is large and obvious when the received FM signal is weak.","2379-190X","978-1-4673-6997-8","10.1109/ICASSP.2015.7177965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7177965","FM enhancement;FM noise reduction","Frequency modulation;Receivers;Floors;Signal to noise ratio;Signal processing algorithms;RF signals","radio receivers;signal processing","digital signal processing algorithms;output audio quality;stereo FM broadcast receivers;FM receiver architectures;broadcast receivers;front end processing;carrier-to-noise ratio;noise floors;stereo audio signals;shared backend processing;cancel noise pulses;FM enhancement techniques;stereo separation","","1","","10","","6 Aug 2015","","","IEEE","IEEE Conferences"
"Fast Algorithms for Low-Delay TDAC Filterbanks in MPEG-4 AAC-ELD","R. K. Chivukula; Y. A. Reznik; Y. Hu; V. Devarajan; M. Jayendra-Lakshman","Indian School of Business, Punjab, India; InterDigital, Inc., San Diego, CA, USA; Department of Electrical Engineering, The University of Texas at Arlington, Arlington, TX, USA; Department of Electrical Engineering, The University of Texas at Arlington, Arlington, TX, USA; Qualcomm Inc, San Diega, CA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","28 Aug 2014","2014","22","12","1701","1712","The MPEG committee has completed development of a new audio coding standard called “MPEG-4 advanced audio coding-enhanced low delay” (AAC-ELD). AAC-ELD uses low delay spectral band replication (LD-SBR) technology together with a low delay time domain alias cancellation (LD TDAC) filterbank in the encoder to achieve both high coding efficiency and low algorithmic delay. In this paper, we present fast algorithms for implementing LD-TDAC filterbanks in AAC-ELD. Two types of fast algorithms are presented. In the first, we map LD-TDAC analysis and synthesis filterbanks to modified discrete cosine transform (MDCT) and inverse modified discrete cosine transform (IMDCT), respectively. Since MDCT/IMDCT are already extensively used in AAC and they have many fast algorithms, this mapping not only provides a fast implementation but also allows a common implementation of the filterbanks in AAC Low Complexity (AAC-LC), AAC Low Delay (AAC-LD) and AAC-ELD codecs. In the second algorithm, we provide a mapping to discrete Cosine transform of type II. The mapping to DCT-II allows the merger of the matrix operations with the windowing stage that precedes or follows them. This further reduces the number of multiplications and leads to an algorithm with the lowest known arithmetic complexity. For filterbanks of lengths 1024 and 960, we also present a new fast factorization of 15-point DCT-II that requires only 14 irrational multiplications, 3 dyadic rational multiplications and 67 additions.","2329-9304","","10.1109/TASLP.2014.2346314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6874511","AAC;audio coding;DCT;factorization;fast algorithms;filterbanks;low delay;MDCT;MPEG;speech coding;time domain alias cancellation","Delays;Algorithm design and analysis;Signal processing algorithms;Transform coding;Discrete cosine transforms;Codecs;Speech","audio coding;audio streaming;channel bank filters;codecs;discrete cosine transforms;inverse transforms;time-domain analysis","low-delay TDAC filterbanks;MPEG-4 AAC-ELD;MPEG committee;audio coding standard;MPEG-4 advanced audio coding-enhanced low-delay;low-delay spectral band replication technology;LD-SBR technology;low-delay time-domain alias cancellation filterbank;encoder;coding efficiency;algorithmic delay;LD-TDAC analysis filterbank;LD-TDAC synthesis filterbank;inverse modified discrete cosine transform;MDCT-IMDCT;AAC low-complexity;AAC-LC;AAC-ELD codecs;matrix operations;windowing stage;arithmetic complexity;fast factorization;15-point DCT-II;irrational multiplications;dyadic rational multiplications","","1","1","28","","8 Aug 2014","","","IEEE","IEEE Journals"
"A timbre matching approach to enhance audio quality of psychoacoustic bass enhancement system","H. Mu; W. Gan; E. Tan","Digital Signal Processing Lab, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Digital Signal Processing Lab, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Digital Signal Processing Lab, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","36","40","Small and flat loudspeakers usually result in poor low-frequency (or bass) responses. Conventional gain equalization does not help significantly and may even result in overdriving and distortion. A psychoacoustic approach has been found to be suitable in tricking the human ear to perceive the fundamental frequency from its higher harmonics. Past research efforts have generally focused on weighting the harmonics based on the loudness matching method, but no work on timbre matching has been carried out so far. In this paper, we propose a new timbre matching technique, which can improve the sound quality of the psychoacoustically enhanced bass. This approach adjusts the amplitude of harmonics to produce similar timbre as the original audio content. Objective and subjective tests are carried out to compare the audio quality of the psychoacoustic bass enhanced signal using the equal-loudness weighting and the timbre matching methods.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6637604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637604","music signal processing;timbre matching;phase vocoder;psychoacoustic bass enhancement","Harmonic analysis;Timbre;Power harmonic filters;Psychoacoustics;Loudspeakers;Steady-state;Transient analysis","acoustic signal processing;audio signal processing;frequency response;harmonic analysis;loudspeakers;music","audio quality enhancement;psychoacoustic bass enhancement system;flat loudspeakers;low-frequency response;human ear;fundamental frequency perception;harmonics weighting;loudness matching method;timbre matching technique;sound quality improvement;harmonics amplitude adjustment;audio content;psychoacoustic bass enhanced signal;equal-loudness weighting;music signal processing","","2","","31","","21 Oct 2013","","","IEEE","IEEE Conferences"
"Audio watermarking using bit modification of voiced or unvoiced segments","D. Cai; K. Gopalan","Electrical and Computer Engineering Department Purdue University Calumet Hammond, USA; Electrical and Computer Engineering Department Purdue University Calumet Hammond, USA","IEEE International Conference on Electro/Information Technology","7 Aug 2014","2014","","","491","494","A method of audio watermark embedding by sample bit modification directly in the time domain is proposed. Five maximum consecutive samples in each voiced or unvoiced segments are located and are embedded with a watermark using a key. Voiced segment embedding has better performance in robustness and imperceptibility while the unvoiced segment embedding has higher payload. Both of the proposed methods are better on imperceptibility, as measured by enhanced modified Bark spectrum distortion, and have higher robustness to embedded data under noise-added conditions compared to a similar bit modification method.","2154-0373","978-1-4799-4774-4","10.1109/EIT.2014.6871813","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6871813","","Watermarking;Indexes;Speech;Payloads;Robustness;Bit error rate;Distortion measurement","audio watermarking","audio watermarking;bit modification;unvoiced segment embedding;voiced segment embedding;enhanced modified Bark spectrum distortion;imperceptibility","","1","","8","","7 Aug 2014","","","IEEE","IEEE Conferences"
"Evaluating Salience Representations for Cross-modal Retrieval of Western Classical Music Recordings","F. Zalkow; S. Balke; M. Müller","International Audio Laboratories Erlangen, Germany; International Audio Laboratories Erlangen, Germany; International Audio Laboratories Erlangen, Germany","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","331","335","In this paper, we consider a cross-modal retrieval scenario of Western classical music. Given a short monophonic musical theme in symbolic notation as query, the objective is to find relevant audio recordings in a database. A major challenge of this retrieval task is the possible difference in the degree of polyphony between the monophonic query and the music recordings. Previous studies for popular music addressed this issue by performing the cross-modal comparison based on predominant melodies extracted from the recordings. For Western classical music, however, this approach is problematic since the underlying assumption of a single pre-dominant melody is often violated. Instead of extracting the melody explicitly, another strategy is to perform the cross-modal comparison directly on the basis of melody-enhanced salience representations. As the main contribution of this paper, we evaluate several conceptually different salience representations for our cross-modal retrieval scenario. Our extensive experimental results, which have been made available on a website, comprise more than 2000 musical themes and 100 hours of audio recordings.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8683609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8683609","Music Information Retrieval;Evaluation;Feature Representations","","audio databases;audio signal processing;feature extraction;music;query processing;signal representation","melody-enhanced salience representations;Western classical music recordings;relevant audio recordings;monophonic query;cross-modal retrieval;monophonic musical theme;music information retrieval;feature representation","","","","28","","17 Apr 2019","","","IEEE","IEEE Conferences"
"A Proposal for Enhancing Museum Visiting Experience Implementing Active RFID Technology","S. Sen; S. Roy; S. K. Sarkar","Dept. of E.C.E., Meghnad Saha Inst. of Tech., Kolkata, India; Dept. of E.T.C.E., Jadavpur Univ., Kolkata, India; Dept. of E.T.C.E., Jadavpur Univ., Kolkata, India","2014 Fourth International Conference on Advances in Computing and Communications","29 Sep 2014","2014","","","295","298","Tourism which plays a key factor in the economies of many countries can act as a source of secure employment, investment and regional development. Museums, art galleries etc provide personalized services to meet individual needs to attract more tourists and local visitors. Thus demand for guides at education-oriented leisure centres increases with the increasing number of tourists. Difficulties are always present to provide description of the artifacts to the multilingual visitors of any museum. Emerging Radio Frequency Identification Detection (RFID) technologies are being applied successfully in cultural spaces to help users to get the correct information which is better than traditional guides. Thus a cost effective Active RFID based prototype has been propsed that can describe the audio information of the artifact to the visitors in their preferred language through an audio system, via a headphone. This design proposal is aimed at increasing museum visitors, boosting profits, and greatly reducing maintenance costs for museum.","","978-1-4799-4363-0","10.1109/ICACC.2014.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906045","Active RFID Tag;Active RFID Reader;Collision Resolution;Automation","Program processors;Active RFID tags;Mobile communication;Audio systems;Handheld computers","art;audio systems;headphones;history;linguistics;museums;natural language processing;radiofrequency identification;travel industry","museum visiting experience;active RFID technology;tourism;economies;secure employment;investment;regional development;art galleries;personalized services;education-oriented leisure centres;multilingual visitors;radio frequency identification detection;RFID technologies;cultural spaces;traditional guides;cost effective active RFID based prototype;audio information;visitor language;audio system;headphone","","7","","10","","29 Sep 2014","","","IEEE","IEEE Conferences"
"LDPC based error resilient audio signal processing for wireless communication","J. Baviskar; A. Mulla; M. Gulati; P. Vaswani; A. Baviskar","Department of Electrical Engineering, Veermata Jijabai Technological Institute, Mumbai 400019, India; Department of Electrical Engineering, Veermata Jijabai Technological Institute, Mumbai 400019, India; Department of Image Processing, Intelligent Communication Lab, Mumbai 400089, India; Department of Electrical Engineering, Veermata Jijabai Technological Institute, Mumbai 400019, India; Department of Electronics Engineering, Universal College of Engineering, Vasai 401202, India","2015 International Conference on Pervasive Computing (ICPC)","16 Apr 2015","2015","","","1","5","The increasing growth of communication has fostered research on various methods to obviate noise from affecting the data transmitted over the link. Low Density Parity Check (LDPC) has gained popularity because of its property of exhibiting near Shannon limit performance when interpreted via iterative probabilistic algorithm. This paper presents a high performance and an enhanced LDPC based algorithm, for channel coding audio signals. The algorithm is designed so as to facilitate adaptive feature as a function of input audio size. It generates an encoded stream of data by arranging bits into data vectors and processing it with the designed LDPC matrix. The modulation scheme considered for transmitting the audio signal is Binary Phase Shift Keying (BPSK). The audio signal recovered at various Signal to Noise ratios (SNRs) is illustrated. The performance analysis of the algorithm delineates improved results for AWGN noise. The Bit Error Rate (BER) is estimated at different SNRs and BER Vs SNR graph is plotted to determine the efficacy of the algorithm.","","978-1-4799-6272-3","10.1109/PERVASIVE.2015.7087164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7087164","Channel coding;Audio processing;Low Density Parity Check (LDPC) Code;Binary Phase Shift Keying (BPSK);Modulation scheme;adaptive matrix;probabilistic approach;AWGN noise","Parity check codes;Algorithm design and analysis;Decoding;Channel coding;Signal processing algorithms;Noise","audio coding;audio signal processing;AWGN;error statistics;iterative methods;parity check codes;phase shift keying;probability","LDPC based error resilient audio signal processing;wireless communication;low density parity check;LDPC based algorithm;Shannon limit performance;iterative probabilistic algorithm;channel coding audio signals;encoded data stream;LDPC matrix;binary phase shift keying;BPSK;signal to noise ratios;SNR;AWGN noise;bit error rate","","2","","9","","16 Apr 2015","","","IEEE","IEEE Conferences"
"Mmdenselstm: An Efficient Combination of Convolutional and Recurrent Neural Networks for Audio Source Separation","N. Takahashi; N. Goswami; Y. Mitsufuji","Sony Corporation, Minato-ku, Tokyo, Japan; Sony India Software Centre, Bangalore, India; Sony Corporation, Minato-ku, Tokyo, Japan","2018 16th International Workshop on Acoustic Signal Enhancement (IWAENC)","4 Nov 2018","2018","","","106","110","Deep neural networks have become an indispensable technique for audio source separation (SS). It was recently reported that a variant of CNN architecture called MM-DenseNet was successfully employed to solve the SS problem of estimating source amplitudes, and state-of-the-art results were obtained for DSD 100 dataset. To further enhance MMDenseNet, here we propose a novel architecture that integrates long short-term memory (LSTM) in multiple scales with skip connections to efficiently model long-term structures within an audio context. The experimental results show that the proposed method outperforms MMDenseNet, LSTM and a blend of the two networks. The number of parameters and processing time of the proposed model are significantly less than those for simple blending. Furthermore, the proposed method yields better results than those obtained using ideal binary masks for a singing voice separation task.","","978-1-5386-8151-0","10.1109/IWAENC.2018.8521383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8521383","convolution;recurrent;DenseNet;LSTM;audio source separation","Computer architecture;Convolution;Spectrogram;Conferences;Acoustics;Source separation;Kernel","audio signal processing;feedforward neural nets;recurrent neural nets;source separation;speech processing","audio source separation;source amplitudes;DSD 100 dataset;long-term structures;singing voice separation task;recurrent neural networks;deep neural networks;convolutional neural network;long short-term memory;ideal binary masks;MMDENSELSTM","","33","","27","","4 Nov 2018","","","IEEE","IEEE Conferences"
"Beamspace-Domain Multichannel Nonnegative Matrix Factorization for Audio Source Separation","S. Lee; S. H. Park; K. Sung","INMC, Seoul National University, Seoul, Republic of Korea; INMC, Seoul National University, Seoul, Republic of Korea; INMC, Seoul National University, Seoul, Republic of Korea","IEEE Signal Processing Letters","8 Dec 2011","2012","19","1","43","46","In this letter, we develop a multichannel blind source separation algorithm based on a beamspace transform and the multichannel nonnegative matrix factorization (NMF) method. The conventional multichannel NMF algorithm performs well with multichannel mixing data, but there is still room for enhancement in multichannel real-world recording data. In this letter, we consider a beamspace-time-frequency domain data model for multichannel NMF method, and enhance the conventional method using a beamspace transform. Our decomposition algorithm is applied to 2-channel and 4-channel unsupervised audio source separation, using a dataset from the international Signal Separation Evaluation Campaign 2010 (SiSEC 2010). Our algorithm shows a better performance than the conventional NMF method in an evaluation results.","1558-2361","","10.1109/LSP.2011.2173192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058587","Acoustic signal processing;blind source separation;multichannel audio;nonnegative matrix factorization (NMF)","Signal processing algorithms;Transforms;Source separation;Algorithm design and analysis;Data models;Arrays","audio signal processing;matrix decomposition;source separation;time-frequency analysis","beamspace-domain multichannel nonnegative matrix factorization;audio source separation;multichannel nonnegative matrix factorization method;conventional multichannel NMF algorithm;multichannel mixing data;beamspace transform;decomposition algorithm;international signal separation evaluation campaign 2010;SiSEC 2010","","12","1","16","","21 Oct 2011","","","IEEE","IEEE Journals"
"Enhanced coding of high-frequency tonal components in MPEG-D USAC through joint application of ESBR and sinusoidal modeling","T. Żernicki; M. Bartkowiak; M. Dománski","Telcordia Poland Sp. z o. o., Applied Research Center, Umultowska 85, Poznán, Poland; Poznán University of Technology, Chair of Multimedia Telecommunications and Microelectronics, Polanka 3, Poland; Poznán University of Technology, Chair of Multimedia Telecommunications and Microelectronics, Polanka 3, Poland","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","11 Jul 2011","2011","","","501","504","The new eSBR tool of MPEG-D Universal Speech and Audio Coding offers a great advantage in compression of high frequency content, however it produces audible artifacts for sounds whose pitch frequencies are strongly variable or exceeding the split frequency of eSBR. We propose an extension of the forthcoming standard by adding a high frequency sinusoidal tool. This tool introduces additional parametric information to the data bitstream in order to encode the challenging tonal components which are excluded from eSBR processing. Listening tests demonstrate benefits of the proposed approach for test items of strongly tonal character.","2379-190X","978-1-4577-0539-7","10.1109/ICASSP.2011.5946450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5946450","Bandwidth extension;audio coding;sinusoidal modeling","Encoding;Transform coding;Speech;Hafnium;Trajectory;Harmonic analysis;IEC standards","audio coding;speech coding","MPEG-D;ESBR;sinusoidal modeling;USAC;universal speech and audio coding;audible artifacts;data bitstream","","3","3","18","","11 Jul 2011","","","IEEE","IEEE Conferences"
"Unsupervised Generative Adversarial Alignment Representation for Sheet music, Audio and Lyrics","D. Zeng; Y. Yu; K. Oyama","SOKENDAI,National Institute of Informatics,Tokyo,Japan; SOKENDAI,National Institute of Informatics,Tokyo,Japan; SOKENDAI,National Institute of Informatics,Tokyo,Japan","2020 IEEE Sixth International Conference on Multimedia Big Data (BigMM)","20 Oct 2020","2020","","","162","165","Sheet music, audio, and lyrics are three main modalities during writing a song. In this paper, we propose an unsupervised generative adversarial alignment representation (UGAAR) model to learn deep discriminative representations shared across three major musical modalities: sheet music, lyrics, and audio, where a deep neural network based architecture on three branches is jointly trained. In particular, the proposed model can transfer the strong relationship between audio and sheet music to audio-lyrics and sheet-lyrics pairs by learning the correlation in the latent shared subspace. We apply CCA components of audio and sheet music to establish new ground truth. The generative (G) model learns the correlation of two couples of transferred pairs to generate new audio-sheet pair for a fixed lyrics to challenge the discriminative (D) model. The discriminative model aims at distinguishing the input which is from the generative model or the ground truth. The two models simultaneously train in an adversarial way to enhance the ability of deep alignment representation learning. Our experimental results demonstrate the feasibility of our proposed UGAAR for alignment representation learning among sheet music, audio, and lyrics.","","978-1-7281-9325-0","10.1109/BigMM50055.2020.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9232663","Adversarial learning;representation learning;cross-modal retrieval","Music;Feature extraction;Semantics;Computational modeling;Data models;Predictive models;Correlation","audio signal processing;learning (artificial intelligence);music;neural nets;unsupervised learning","UGAAR;ground truth;CCA components;deep alignment representation learning;audio-sheet pair;sheet-lyrics pairs;audio-lyrics;musical modalities;deep discriminative representations;unsupervised generative adversarial alignment representation model;sheet music","","","","11","","20 Oct 2020","","","IEEE","IEEE Conferences"
"Infrared and Intertial Tracking in the Immersive Audio Environment for Enhanced Military Training","P. Shah; A. Faza; R. Nimmala; S. Grant; W. Chapin; R. Montgomery","Dept. of Electr. & Comput. Eng., Missouri Univ. of Sci. & Technol., Rolla, MO, USA; Dept. of Electr. & Comput. Eng., Missouri Univ. of Sci. & Technol., Rolla, MO, USA; Dept. of Electr. & Comput. Eng., Missouri Univ. of Sci. & Technol., Rolla, MO, USA; Dept. of Electr. & Comput. Eng., Missouri Univ. of Sci. & Technol., Rolla, MO, USA; AuSIM Inc., Mountain View, CA, USA; Dept. of Psychol., Missouri Univ. of Sci. & Technol., Rolla, MO, USA","2012 IEEE International Conference on Multimedia and Expo Workshops","16 Aug 2012","2012","","","181","186","The Immersive Audio Environment (IAE) was designed to provide an effective military training facility. It's efficacy at synthesizing sounds from desired directions and also the ability to synthesize moving sounds has been previously reported. This paper discusses the addition of a tracking system to evaluate subject training performance. Numerous tracking systems have been developed for tracking in immersive environments. Some examples include using head mounted web cams, visible light cameras mounted on the support structure, or even single camera tracking as in commercially available entertainment. Our system uses a combination of an existing infrared tracking system and a specially designed system of inertial tracking. This paper presents tests and results to evaluate the accuracy of the tracking system with respect to our application and verifies the efficacy of using the IAE for training enhancement.","","978-1-4673-2027-6","10.1109/ICMEW.2012.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6266252","training;immersive audio;infrared tracking;inertial tracking","Weapons;Lasers;Cameras;Training;Sensors;Accuracy;Loudspeakers","audio acoustics;helmet mounted displays;infrared imaging;military equipment;optical tracking;video cameras","enhanced military training;infrared tracking;inertial tracking;immersive audio environment;military training facility;moving sound synthesis;head mounted web cams;visible light cameras;mounted support structure;single camera tracking","","1","","7","","16 Aug 2012","","","IEEE","IEEE Conferences"
"A 2.1-Channel Class-D Amplifier Exploited Coupling Virtual-Audio-Image to Enhance Stereo","L. Liu; S. Deng; Z. Zhu; Y. Yang","Key Lab. of Wide Band-Gap Semicond. Mater. & Devices, Xidian Univ., Xi'an, China; Key Lab. of Wide Band-Gap Semicond. Mater. & Devices, Xidian Univ., Xi'an, China; Key Lab. of Wide Band-Gap Semicond. Mater. & Devices, Xidian Univ., Xi'an, China; Key Lab. of Wide Band-Gap Semicond. Mater. & Devices, Xidian Univ., Xi'an, China","IEEE Transactions on Circuits and Systems II: Express Briefs","19 May 2017","2014","61","5","324","328","This brief presents a novel 2.1-channel stereoenhanced class-D audio amplifier to improve the quality of stereophonic sound in portable devices. By a way of cross coupling, the input audio signal from one channel can be amplified at the same time, with different gains in the two channels. The ratio of gains in the two channels can be adjusted by changing the resistance and then making the position of the virtual-audio-image change, which improves the stereo effect. The bass signals from different channels, which frequency range between 30 and 300 Hz, are superimposed and then amplified in the independent bass channel by using the Sallen-Key bandpass filter. A practical implementation in the SMIC 0.18-μm CMOS process has been done to validate the theoretical results. When a 200-mVPP sinusoidal signal at 1 kHz is applied in one channel and a dc signal applied in the other channel, the amplifier demonstrates a gain difference of -9.4 dB. The simulation and experiment results show that the amplifier can deliver 2.5 W × 2 into 4-Ω loads at 3.6-V supply in the left or right channel, with the total harmonic distortion plus noise (THD+N) of less than 0.1%. The gain in the bass channel is up to 30 dB at 3.6-V power supply, with the THD+N of about 0.05%.","1558-3791","","10.1109/TCSII.2014.2312797","National Natural Science Foundation of China(grant numbers:61376033,61234002,61006028); National High-Tech Program of China(grant numbers:2012AA012302,2013AA014103); Ph.D. Programs Foundation of Ministry of Education of China(grant numbers:20120203110017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805576","Bass amplifier;class-D amplifier;cross couple;ratio of gains;stereo enhancement;Bass amplifier;class-D amplifier;cross couple;ratio of gains;stereo enhancement","Gain;Couplings;Pulse width modulation;Resistance;Noise;Solid state circuits;Linearity","amplifiers;audio signals;band-pass filters;CMOS integrated circuits;stereo image processing","channel class-D amplifier;coupling virtual-audio-image to enhance stereo;stereoenhanced class-D audio amplifier;stereophonic sound;portable devices;input audio signal;virtual-audio-image;bass signals;independent bass channel;Sallen-Key bandpass filter;CMOS process;sinusoidal signal;DC signal;total harmonic distortion","","","","16","","25 Apr 2014","","","IEEE","IEEE Journals"
"Improving acoustic modeling using audio-visual speech","A. H. Abdelaziz","International Computer Science Institute, Berkeley, USA","2017 IEEE International Conference on Multimedia and Expo (ICME)","31 Aug 2017","2017","","","1081","1086","Reliable visual features that encode the articulator movements of speakers can dramatically improve the decoding accuracy of automatic speech recognition systems when combined with the corresponding acoustic signals. In this paper, a novel framework is proposed to utilize audio-visual speech not only during decoding but also for training better acoustic models. In this framework, a multi-stream hidden Markov model is iteratively deployed to fuse audio and video likelihoods. The fused likelihoods are used to estimate enhanced frame-state alignments, which are finally used as better training targets. The proposed framework is so flexible that it can be partially used to train acoustic models with the available audio-visual data while a conventional training strategy can be followed with the remaining acoustic data. The experimental results show that the acoustic models trained using the proposed audio-visual framework perform significantly better than those trained conventionally with solely acoustic data in clean and noisy conditions.","1945-788X","978-1-5090-6067-2","10.1109/ICME.2017.8019294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019294","Audio-visual automatic speech recognition;multi-stream HMM;acoustic modeling;audio-visual fusion;noise robustness","Visualization;Acoustics;Hidden Markov models;Training;Feature extraction;Speech;Speech recognition","acoustic signal processing;audio-visual systems;feature extraction;hidden Markov models;iterative methods;learning (artificial intelligence);sensor fusion;speech recognition","audio-visual fusion;iterative deployment;multistream hidden Markov model;acoustic model training;acoustic signals;automatic speech recognition systems;speaker movements;visual features;audio-visual speech","","3","","29","","31 Aug 2017","","","IEEE","IEEE Conferences"
"Optimal Coding of Generalized-Gaussian-Distributed Frequency Spectra for Low-Delay Audio Coder With Powered All-Pole Spectrum Estimation","R. Sugiura; Y. Kamamoto; N. Harada; H. Kameoka; T. Moriya","Nippon Telegraph and Telephone Corporation, Atsugi, Japan; Nippon Telegraph and Telephone Corporation, Atsugi, Japan; Nippon Telegraph and Telephone Corporation, Atsugi, Japan; Nippon Telegraph and Telephone Corporation, Atsugi, Japan; Nippon Telegraph and Telephone Corporation, Atsugi, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","1 Jun 2015","2015","23","8","1309","1321","We present an optimal coding scheme that parameterizes the maximum-likelihood estimate of variance for frequency spectra belonging to the generalized Gaussian distribution, the distribution covering the Laplacian and the Gaussian. By slightly modifying the all-pole model of the conventional linear prediction (LP), we can estimate the variance with the same method as in LP, which has low computational costs. Experimental results show that incorporating the coding scheme in a state-of-the-art wide-band audio coder enhances its objective and subjective quality in a low-bit-rate and low-delay situation by increasing the compression efficiency. Thus, this coding scheme will be useful in applications like mobile communications, which requires highly efficient compression.","2329-9304","","10.1109/TASLP.2015.2431851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7105382","Arithmetic coding;audio compression;generalized Gaussian distribution;linear prediction;low delay;transform coded excitation","Encoding;Speech;Laplace equations;Frequency-domain analysis;Shape;Speech processing;Predictive models","audio coding;data compression;Gaussian distribution;linear predictive coding;maximum likelihood estimation","objective quality enhancement;compression efficiency;wideband audio coder;variance estimation;conventional linear prediction;all-pole model;maximum likelihood estimation;all-pole spectrum estimation;low delay audio coder;optimal coding scheme;generalized Gaussian distributed frequency spectra","","7","4","48","","11 May 2015","","","IEEE","IEEE Journals"
"Proposed approach for improving Bluetooth networks security through SVD audio watermarking","M. A. M. El-Bendary; A. Haggag; F. Shawki; F. E. Abd-El-Samie","Dept. of Electronics Technology, Faculty of Industrial Education, Helwan University., Cairo, Egypt; Dept. of Electronics Technology, Faculty of Industrial Education, Helwan University., Cairo, Egypt; Dept. of Communications Engineering, Faculty of Elec. Engineering, Menofia University., Menof, Egypt; Dept. of Communications Engineering, Faculty of Elec. Engineering, Menofia University., Menof, Egypt","2012 6th International Conference on Sciences of Electronics, Technologies of Information and Telecommunications (SETIT)","21 Mar 2013","2012","","","594","598","Some of applications need multi-level security. The paper proposes a novel approach IEEE 802.15.1 Bluetooth network to provide better secure link for essential applications. It proposes a new approach for audio watermarking using the singular value decomposition (SVD) mathematical technique. This approach is based on embedding the encrypted image in the singular values of the audio signal after transforming it into a 2D format. After watermark embedding, the audio signal is transformed again into a 1-D format. The 1-D audio signal is segmented to Bluetooth packet payload length. That leads to the needs of fragmentation of the image to small segments. In the proposed technique the chaotic encryption is used for encrypt the image. It improves the quality of extracted images as proved experimentally, where it resists the noise and different attacks. The paper uses two type of Bluetooth packets (2DH1 and 2DM1), uncoded Enhanced Data Rate (EDR) and encoded EDR packets respectively. At the receiver the segments are recollected to construct watermark signal. The final step is extracting the image. Experiments of the simulation are carried over fading channel. Experimental results show that the proposed audio watermarking approach maintains the high quality of the audio signal and that the watermark extraction and decryption are possible.","","978-1-4673-1658-3","10.1109/SETIT.2012.6481979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481979","Audio Watermarking;IEEE 802.15.1 (Bluetooth Technology);SVD;Fading channel;Copyright Protection","Watermarking;Bluetooth;Encryption;IEEE 802.15 Standards","audio watermarking;Bluetooth;cellular radio;cryptography;fading channels;radio receivers;singular value decomposition;telecommunication security","Bluetooth network security improvement;SVD audio watermarking;multilevel security;IEEE 802.15.1 Bluetooth network;singular value decomposition mathematical technique;2D format;watermark embedding;1D audio signal;image fragmentation;chaotic encryption;image encryption;image extraction;Bluetooth packet;uncoded enhanced data rate;encoded EDR packet;receiver;watermark signal construction;fading channel;watermark extraction;watermark decryption","","6","","14","","21 Mar 2013","","","IEEE","IEEE Conferences"
"IEEE Standard for Advanced Audio and Video Coding","",,"IEEE Std 1857-2013","30 May 2013","2013","","","1","178","In this standard, a set of tools for efficient video coding is defined, including directional intra prediction, variable block size inter prediction and context adaptive binary arithmetic coding, and the corresponding decoding procedure. The target applications and services include but not limited TV over Internet, user-generated multimedia content, IP-based video conference, IP-based surveillance, and other video/audio enabled services and applications such as digital television broadcasting, digital storage media, and communication.;In this standard, a set of tools for efficient video coding is defined, including directional intra prediction, variable block size inter prediction and context adaptive binary arithmetic coding, and the corresponding decoding procedure. The target applications and services include but not limited TV over Internet, user-generated multimedia content, IP-based video conference, IP-based surveillance, and other video/audio enabled services and applications such as digital television broadcasting, digital storage media, and communication.","","978-0-7381-8283-4","10.1109/IEEESTD.2013.6522104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6522104","block;coefficients;decoding;encoding;entropy coding;field;frame;IEEE 1857;image;Internet Protocol television (IPTV);inter prediction;intra prediction;macroblock;picture;quantization;slice;transform","IEEE standards;VIdeo coding;Audio coding;Decoding;Encoding;IP networks;Image processing;TV;Entropy coding","arithmetic codes;audio coding;binary codes;block codes;decoding;IEEE standards;video coding","IEEE Std 1857-2013;advanced audio coding;advanced video coding;directional intraprediction;variable block size interprediction;context adaptive binary arithmetic coding;decoding procedure;limited TV over Internet;user-generated multimedia content;IP-based video conference;IP-based surveillance;video-audio enabled services","","","","0","","30 May 2013","","","IEEE","IEEE Standards"
